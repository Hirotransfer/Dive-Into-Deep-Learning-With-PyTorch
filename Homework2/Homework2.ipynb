{"cells":[{"metadata":{"id":"ADCE3F2FD42A4636805C736AC67C1F9D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Homework2\n- Task03：过拟合、欠拟合及其解决方案；梯度消失、梯度爆炸；循环神经网络进阶\n- Task04：机器翻译及相关技术；注意力机制与Seq2seq模型；Transformer\n- Task05：卷积神经网络基础；leNet；卷积神经网络进阶"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_3a0i46j","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"44C527C86C2941789DE841DE4A782754","mdEditEnable":false},"source":"# Task03：过拟合、欠拟合及其解决方案；梯度消失、梯度爆炸；循环神经网络进阶\n- Part1 过拟合、欠拟合及其解决方案\n- Part2 梯度消失、梯度爆炸\n- Part3 循环神经网络进阶"},{"metadata":{"id":"8D33F21C2B67409E81A74F09EF5A1B64","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Part1 过拟合、欠拟合及其解决方案\n- 模型选择、过拟合、欠拟合\n- 多项式函数拟合\n- 权重衰减\n- 丢弃法"},{"attachments":{"image.png":{"image/png":"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"}},"cell_type":"markdown","metadata":{"graffitiCellId":"id_2rrhvg5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1C7884059BD2464DB639268E872E99C2","mdEditEnable":false},"source":"# 模型选择、过拟合和欠拟合\n\n## 训练误差和泛化误差\n在解释上述现象之前，我们需要区分训练误差（training error）和泛化误差（generalization error）。通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。\n\n机器学习模型应关注降低泛化误差。\n\n## 模型选择\n### 验证数据集\n从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。\n\n### K折交叉验证  \n由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是K折交叉验证（K-fold cross-validation）。在K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K-1个子数据集来训练模型。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。\n## 过拟合和欠拟合\n接下来，我们将探究模型训练中经常出现的两类典型问题：\n- 一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；\n- 另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。\n在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。\n\n### 模型复杂度\n为了解释模型复杂度，我们以多项式函数拟合为例。给定一个由标量数据特征$x$和对应的标量标签$y$组成的训练数据集，多项式函数拟合的目标是找一个$K$阶多项式函数\n\n\n$$\n \\hat{y} = b + \\sum_{k=1}^K x^k w_k \n$$\n\n\n来近似 $y$。在上式中，$w_k$是模型的权重参数，$b$是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。\n\n给定训练数据集，模型复杂度和误差之间的关系：\n\n![Image Name](https://cdn.kesci.com/upload/image/q5jc27wxoj.png?imageView2/0/w/960/h/960)\n\n### 训练数据集大小\n影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。\n"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_rxqm2hz","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"662A794758DD42EFA08B79C1EF032C65","mdEditEnable":false},"source":"# 多项式函数拟合"},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","graffitiCellId":"id_yjb9esq","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F39C60E42E09441D8602243557564E0A","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"%matplotlib inline\nimport torch\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\nprint(torch.__version__)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_1aakii8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"193FB89ACE2E4D4888FB258A250939AD","mdEditEnable":false},"source":"## 初始化模型参数"},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","graffitiCellId":"id_3xv7jo8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9714C0B5EF3344AB89F6C05008E10951","collapsed":false,"scrolled":false},"outputs":[],"source":"n_train, n_test, true_w, true_b = 100, 100, [1.2, -3.4, 5.6], 5\nfeatures = torch.randn((n_train + n_test, 1))\npoly_features = torch.cat((features, torch.pow(features, 2), torch.pow(features, 3)), 1) \nlabels = (true_w[0] * poly_features[:, 0] + true_w[1] * poly_features[:, 1]\n          + true_w[2] * poly_features[:, 2] + true_b)\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)"},{"cell_type":"code","execution_count":3,"metadata":{"graffitiCellId":"id_m2mu8wo","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FDF06E080C72455296FF40E5A03C8A1D","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(tensor([[-0.3218],\n         [-0.3237]]), tensor([[-0.3218,  0.1035, -0.0333],\n         [-0.3237,  0.1048, -0.0339]]), tensor([4.0676, 4.0544]))"},"transient":{},"execution_count":3}],"source":"features[:2], poly_features[:2], labels[:2]"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_6fb98pt","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"04AB33FA366C42CDBF58FE28874F3F27","mdEditEnable":false},"source":"## 定义、训练和测试模型"},{"cell_type":"code","execution_count":4,"metadata":{"graffitiCellId":"id_tq07955","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DD5CB5E380BE4BF498E4F3D5A9F97F43","collapsed":false,"scrolled":false},"outputs":[],"source":"def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,\n             legend=None, figsize=(3.5, 2.5)):\n    # d2l.set_figsize(figsize)\n    d2l.plt.xlabel(x_label)\n    d2l.plt.ylabel(y_label)\n    d2l.plt.semilogy(x_vals, y_vals)\n    if x2_vals and y2_vals:\n        d2l.plt.semilogy(x2_vals, y2_vals, linestyle=':')\n        d2l.plt.legend(legend)"},{"cell_type":"code","execution_count":5,"metadata":{"graffitiCellId":"id_3lzy7bj","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3DA4335947C44C018CD2C8B215A641ED","collapsed":false,"scrolled":false},"outputs":[],"source":"num_epochs, loss = 100, torch.nn.MSELoss()\n\ndef fit_and_plot(train_features, test_features, train_labels, test_labels):\n    # 初始化网络模型\n    net = torch.nn.Linear(train_features.shape[-1], 1)\n    # 通过Linear文档可知，pytorch已经将参数初始化了，所以我们这里就不手动初始化了\n    \n    # 设置批量大小\n    batch_size = min(10, train_labels.shape[0])    \n    dataset = torch.utils.data.TensorDataset(train_features, train_labels)      # 设置数据集\n    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True) # 设置获取数据方式\n    \n    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)                      # 设置优化函数，使用的是随机梯度下降优化\n    train_ls, test_ls = [], []\n    for _ in range(num_epochs):\n        for X, y in train_iter:                                                 # 取一个批量的数据\n            l = loss(net(X), y.view(-1, 1))                                     # 输入到网络中计算输出，并和标签比较求得损失函数\n            optimizer.zero_grad()                                               # 梯度清零，防止梯度累加干扰优化\n            l.backward()                                                        # 求梯度\n            optimizer.step()                                                    # 迭代优化函数，进行参数优化\n        train_labels = train_labels.view(-1, 1)\n        test_labels = test_labels.view(-1, 1)\n        train_ls.append(loss(net(train_features), train_labels).item())         # 将训练损失保存到train_ls中\n        test_ls.append(loss(net(test_features), test_labels).item())            # 将测试损失保存到test_ls中\n    print('final epoch: train loss', train_ls[-1], 'test loss', test_ls[-1])    \n    semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',\n             range(1, num_epochs + 1), test_ls, ['train', 'test'])\n    print('weight:', net.weight.data,\n          '\\nbias:', net.bias.data)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_tvv0n31","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FEBABF4CE4FC49F8869E557513F6CFD8","mdEditEnable":false},"source":"## 三阶多项式函数拟合（正常）"},{"cell_type":"code","execution_count":6,"metadata":{"graffitiCellId":"id_pc28vr5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CD685B472B744329A1CFC47C9F0B5E89","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"final epoch: train loss 0.00011660061136353761 test loss 0.00017045893764588982\nweight: tensor([[ 1.2108, -3.3995,  5.5956]]) \nbias: tensor([4.9996])\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/CD685B472B744329A1CFC47C9F0B5E89/q5tya8z3tw.png\">"},"transient":{}}],"source":"fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:])"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_pu3gps2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F5C7FE5EF88D44FE85E0B651B3B09959","mdEditEnable":false},"source":"## 线性函数拟合（欠拟合）"},{"cell_type":"code","execution_count":7,"metadata":{"graffitiCellId":"id_07e409i","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"33AD626DA0B94DB7A28D47697312B45D","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"final epoch: train loss 44.76585006713867 test loss 118.83914947509766\nweight: tensor([[12.1562]]) \nbias: tensor([2.6400])\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/33AD626DA0B94DB7A28D47697312B45D/q5tyagn36i.png\">"},"transient":{}}],"source":"fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train], labels[n_train:])"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_hmoo3h2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BC253732133341DC8F028FB6834C68F0","mdEditEnable":false},"source":"## 训练样本不足（过拟合）"},{"cell_type":"code","execution_count":8,"metadata":{"graffitiCellId":"id_h1gobje","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"AB13F65A70A9484788F8004E427EC290","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"final epoch: train loss 0.1520145982503891 test loss 421.29791259765625\nweight: tensor([[-1.0457,  0.8831, -0.5873]]) \nbias: tensor([3.2220])\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/AB13F65A70A9484788F8004E427EC290/q5tyakxprq.png\">"},"transient":{}}],"source":"fit_and_plot(poly_features[0:2, :], poly_features[n_train:, :], labels[0:2], labels[n_train:])"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_4tfoquk","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A1A8C28C0A204B04AD8B5FFF9C2D2F5B","mdEditEnable":false},"source":"# 权重衰减\n## 方法  \n权重衰减等价于 $L_2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。\n\n##  L2 范数正则化（regularization）\n$L_2$范数正则化在模型原损失函数基础上添加$L_2$范数惩罚项，从而得到训练所需要最小化的函数。$L_2$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以线性回归中的线性回归损失函数为例\n\n\n$$\n \\ell(w_1, w_2, b) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right)^2 \n$$\n\n\n其中$w_1, w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)}, x_2^{(i)}$，标签为$y^{(i)}$，样本数为$n$。将权重参数用向量$\\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为\n\n\n$$\n\\ell(w_1, w_2, b) + \\frac{\\lambda}{2n} |\\boldsymbol{w}|^2,\n$$\n\n\n其中超参数$\\lambda > 0$。当权重参数均为0时，惩罚项最小。当$\\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。\n有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将线性回归一节中权重$w_1$和$w_2$的迭代方式更改为\n\n\n$$\n \\begin{aligned} w_1 &\\leftarrow \\left(1- \\frac{\\eta\\lambda}{|\\mathcal{B}|} \\right)w_1 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_1^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right),\\\\ w_2 &\\leftarrow \\left(1- \\frac{\\eta\\lambda}{|\\mathcal{B}|} \\right)w_2 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_2^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right). \\end{aligned} \n$$\n\n\n可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。因此，$L_2$范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。\n"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_6r7el1m","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"86B363E2493A4A2980C5E00729B925F5","mdEditEnable":false},"source":"## 高维线性回归实验从零开始的实现\n下面，我们以高维线性回归为例来引入一个过拟合问题，并使用权重衰减来应对过拟合。设数据样本特征的维度为$p$。对于训练数据集和测试数据集中特征为$x_1, x_2, \\ldots, x_p$的任一样本，我们使用如下的线性函数来生成该样本的标签：\n\n\n$$\n y = 0.05 + \\sum_{i = 1}^p 0.01x_i + \\epsilon \n$$\n\n\n其中噪声项$\\epsilon$服从均值为0、标准差为0.01的正态分布。为了较容易地观察过拟合，我们考虑高维线性回归问题，如设维度$p=200$；同时，我们特意把训练数据集的样本数设低，如20。"},{"cell_type":"code","execution_count":9,"metadata":{"graffitiCellId":"id_m6mthxc","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"51E803AFA42047D48605A9042CD266F8","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"%matplotlib inline\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\n\nprint(torch.__version__)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_d4sywej","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5AC8DFCEDD734B8289B2CE7D06C82B6F","mdEditEnable":false},"source":"## 初始化模型参数\n与前面观察过拟合和欠拟合现象的时候相似，在这里不再解释。"},{"cell_type":"code","execution_count":10,"metadata":{"graffitiCellId":"id_3ymafxv","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D66AE4FB939640948F151F6FF7C1740E","collapsed":false,"scrolled":false},"outputs":[],"source":"n_train, n_test, num_inputs = 20, 100, 200\ntrue_w, true_b = torch.ones(num_inputs, 1) * 0.01, 0.05\n\nfeatures = torch.randn((n_train + n_test, num_inputs))\nlabels = torch.matmul(features, true_w) + true_b\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)\ntrain_features, test_features = features[:n_train, :], features[n_train:, :]\ntrain_labels, test_labels = labels[:n_train], labels[n_train:]"},{"cell_type":"code","execution_count":11,"metadata":{"graffitiCellId":"id_xd0w3ub","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E1707F6923A742F69F84896E84B463FE","collapsed":false,"scrolled":false},"outputs":[],"source":"# 定义参数初始化函数，初始化模型参数并且附上梯度\ndef init_params():\n    w = torch.randn((num_inputs, 1), requires_grad=True)\n    b = torch.zeros(1, requires_grad=True)\n    return [w, b]"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ejz9flm","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"01F0DAF1CD6F48BA92BAAE9F7554EA28","mdEditEnable":false},"source":"## 定义L2范数惩罚项"},{"cell_type":"code","execution_count":12,"metadata":{"graffitiCellId":"id_kkiguji","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4709112E91504EF68681066730402A03","collapsed":false,"scrolled":false},"outputs":[],"source":"def l2_penalty(w):\n    return (w**2).sum() / 2"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_99qaypt","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"06C980C7C2534A728058234C02210A34","mdEditEnable":false},"source":"## 定义训练和测试"},{"cell_type":"code","execution_count":13,"metadata":{"graffitiCellId":"id_6crgl61","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3A369C128AE64990854186DAD85DDD7F","collapsed":false,"scrolled":false},"outputs":[],"source":"batch_size, num_epochs, lr = 1, 100, 0.003\nnet, loss = d2l.linreg, d2l.squared_loss\n\ndataset = torch.utils.data.TensorDataset(train_features, train_labels)\ntrain_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n\ndef fit_and_plot(lambd):\n    w, b = init_params()\n    train_ls, test_ls = [], []\n    for _ in range(num_epochs):\n        for X, y in train_iter:\n            # 添加了L2范数惩罚项\n            l = loss(net(X, w, b), y) + lambd * l2_penalty(w)\n            l = l.sum()\n            \n            if w.grad is not None:\n                w.grad.data.zero_()\n                b.grad.data.zero_()\n            l.backward()\n            d2l.sgd([w, b], lr, batch_size)\n        train_ls.append(loss(net(train_features, w, b), train_labels).mean().item())\n        test_ls.append(loss(net(test_features, w, b), test_labels).mean().item())\n    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',\n                 range(1, num_epochs + 1), test_ls, ['train', 'test'])\n    print('L2 norm of w:', w.norm().item())"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_55gl5e9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BA54CB5FD4E64380AC1357E8537265DC","mdEditEnable":false},"source":"## 观察过拟合"},{"cell_type":"code","execution_count":14,"metadata":{"graffitiCellId":"id_6pmjf6b","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C27406AAA0FD41C6801D55ED4B25D5EA","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"L2 norm of w: 13.82959270477295\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/C27406AAA0FD41C6801D55ED4B25D5EA/q5tybywylj.svg\">"},"transient":{}}],"source":"fit_and_plot(lambd=0)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ejl383l","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F2335EC469AB402C890CE258932583D1","mdEditEnable":false},"source":"## 使用权重衰减"},{"cell_type":"code","execution_count":15,"metadata":{"graffitiCellId":"id_x1tkbn7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0770D8C23B8144C59D13D24390E471F0","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"L2 norm of w: 0.036054737865924835\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/0770D8C23B8144C59D13D24390E471F0/q5tyc4i81f.svg\">"},"transient":{}}],"source":"fit_and_plot(lambd=3)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_g2d3ly2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FD1464A14A6149AB81CF443A10F843A4","mdEditEnable":false},"source":"## 简洁实现"},{"cell_type":"code","execution_count":16,"metadata":{"graffitiCellId":"id_b6kxfkc","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"20CFA1339D054931892561E254150368","collapsed":false,"scrolled":false},"outputs":[],"source":"def fit_and_plot_pytorch(wd):\n    # 对权重参数衰减。权重名称一般是以weight结尾\n    net = nn.Linear(num_inputs, 1)\n    nn.init.normal_(net.weight, mean=0, std=1)\n    nn.init.normal_(net.bias, mean=0, std=1)\n    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) # 对权重参数衰减\n    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  # 不对偏差参数衰减\n    \n    train_ls, test_ls = [], []\n    for _ in range(num_epochs):\n        for X, y in train_iter:\n            l = loss(net(X), y).mean()\n            optimizer_w.zero_grad()\n            optimizer_b.zero_grad()\n            \n            l.backward()\n            \n            # 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差\n            optimizer_w.step()\n            optimizer_b.step()\n        train_ls.append(loss(net(train_features), train_labels).mean().item())\n        test_ls.append(loss(net(test_features), test_labels).mean().item())\n    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',\n                 range(1, num_epochs + 1), test_ls, ['train', 'test'])\n    print('L2 norm of w:', net.weight.data.norm().item())"},{"cell_type":"code","execution_count":17,"metadata":{"graffitiCellId":"id_dxnud8x","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"525D01167F0E40509495588D6B0A0FB9","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"L2 norm of w: 12.972947120666504\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/525D01167F0E40509495588D6B0A0FB9/q5tycjgelf.svg\">"},"transient":{}}],"source":"fit_and_plot_pytorch(0)"},{"cell_type":"code","execution_count":18,"metadata":{"graffitiCellId":"id_qclwxdh","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3FAACA854B9545A8ADADDEB6EE17A680","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"L2 norm of w: 0.04492613300681114\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/3FAACA854B9545A8ADADDEB6EE17A680/q5tycpvtpb.svg\">"},"transient":{}}],"source":"fit_and_plot_pytorch(3)"},{"attachments":{"image.png":{"image/png":"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"}},"cell_type":"markdown","metadata":{"graffitiCellId":"id_c8j5g7h","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3547FD2B0E60465BB11AA6BADF4AC288","mdEditEnable":false},"source":"# 丢弃法\n\n多层感知机中神经网络图描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（$i=1, \\ldots, 5$）的计算表达式为\n\n\n$$\n h_i = \\phi\\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\\right) \n$$\n\n\n这里$\\phi$是激活函数，$x_1, \\ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \\ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i'$\n\n\n$$\n h_i' = \\frac{\\xi_i}{1-p} h_i \n$$\n\n\n由于$E(\\xi_i) = 1-p$，因此\n\n\n$$\n E(h_i') = \\frac{E(\\xi_i)}{1-p}h_i = h_i \n$$\n\n\n即丢弃法不改变其输入的期望值。让我们对之前多层感知机的神经网络中的隐藏层使用丢弃法，一种可能的结果如图所示，其中$h_2$和$h_5$被清零。这时输出值的计算不再依赖$h_2$和$h_5$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, \\ldots, h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, \\ldots, h_5$中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法\n\n![Image Name](https://cdn.kesci.com/upload/image/q5jd69in3m.png?imageView2/0/w/960/h/960)\n"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_698weeu","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"53FBE46C03C040D0A123E957865CEC6A","mdEditEnable":false},"source":"## 丢弃法从零开始的实现"},{"cell_type":"code","execution_count":19,"metadata":{"graffitiCellId":"id_0cw2xsh","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"30FE9592111F415A8C913AC04AD9BF9A","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"%matplotlib inline\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\n\nprint(torch.__version__)"},{"cell_type":"code","execution_count":20,"metadata":{"graffitiCellId":"id_7mt4bcl","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C6790DFD4650485AB8315E04E12D4C74","collapsed":false,"scrolled":false},"outputs":[],"source":"def dropout(X, drop_prob):\n    X = X.float()\n    assert 0 <= drop_prob <= 1\n    keep_prob = 1 - drop_prob\n    # 这种情况下把全部元素都丢弃\n    if keep_prob == 0:\n        return torch.zeros_like(X)\n    mask = (torch.rand(X.shape) < keep_prob).float()\n    \n    return mask * X / keep_prob"},{"cell_type":"code","execution_count":21,"metadata":{"graffitiCellId":"id_ek4ildz","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F961F801B5CB434683F8F82FD79C8F23","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])"},"transient":{},"execution_count":21}],"source":"X = torch.arange(16).view(2, 8)\ndropout(X, 0)"},{"cell_type":"code","execution_count":22,"metadata":{"graffitiCellId":"id_i67qnbo","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A2563ED1062D48728E6AA868A3A8C3F4","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([[ 0.,  2.,  4.,  0.,  8., 10., 12., 14.],\n        [16.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"},"transient":{},"execution_count":22}],"source":"dropout(X, 0.5)"},{"cell_type":"code","execution_count":23,"metadata":{"graffitiCellId":"id_gwsj0hd","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"56DF0D9E4EE84D04A6CCE5180FF9D164","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0.]])"},"transient":{},"execution_count":23}],"source":"dropout(X, 1.0)"},{"cell_type":"code","execution_count":24,"metadata":{"graffitiCellId":"id_w81sasb","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E3ECEB59A5F944898BE2C5ABEB513208","collapsed":false,"scrolled":false},"outputs":[],"source":"# 参数的初始化\nnum_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n\nW1 = torch.tensor(np.random.normal(0, 0.01, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=True)\nb1 = torch.zeros(num_hiddens1, requires_grad=True)\nW2 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=True)\nb2 = torch.zeros(num_hiddens2, requires_grad=True)\nW3 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=True)\nb3 = torch.zeros(num_outputs, requires_grad=True)\n\nparams = [W1, b1, W2, b2, W3, b3]"},{"cell_type":"code","execution_count":25,"metadata":{"graffitiCellId":"id_d24i59y","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C3C7D8DC21FA471AA4AF11DFAC41D7E5","collapsed":false,"scrolled":false},"outputs":[],"source":"drop_prob1, drop_prob2 = 0.2, 0.5\n\ndef net(X, is_training=True):\n    X = X.view(-1, num_inputs)\n    H1 = (torch.matmul(X, W1) + b1).relu()\n    if is_training:  # 只在训练模型时使用丢弃法\n        H1 = dropout(H1, drop_prob1)  # 在第一层全连接后添加丢弃层\n    H2 = (torch.matmul(H1, W2) + b2).relu()\n    if is_training:\n        H2 = dropout(H2, drop_prob2)  # 在第二层全连接后添加丢弃层\n    return torch.matmul(H2, W3) + b3"},{"cell_type":"code","execution_count":26,"metadata":{"graffitiCellId":"id_ticmy78","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"73C1F705FAA14F4390B8D24CCF9E5673","collapsed":false,"scrolled":false},"outputs":[],"source":"def evaluate_accuracy(data_iter, net):\n    acc_sum, n = 0.0, 0\n    for X, y in data_iter:\n        if isinstance(net, torch.nn.Module):\n            net.eval() # 评估模式, 这会关闭dropout\n            acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n            net.train() # 改回训练模式\n        else: # 自定义的模型\n            if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n                # 将is_training设置成False\n                acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n            else:\n                acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n        n += y.shape[0]\n    return acc_sum / n"},{"cell_type":"code","execution_count":27,"metadata":{"graffitiCellId":"id_ryzllq8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B493E72017B54831A80790EE0AA3DB2C","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 1, loss 0.0045, train acc 0.554, test acc 0.735\nepoch 2, loss 0.0023, train acc 0.787, test acc 0.802\nepoch 3, loss 0.0019, train acc 0.824, test acc 0.806\nepoch 4, loss 0.0018, train acc 0.836, test acc 0.852\nepoch 5, loss 0.0016, train acc 0.849, test acc 0.831\n","name":"stdout"}],"source":"num_epochs, lr, batch_size = 5, 100.0, 256  # 这里的学习率设置的很大，原因与之前相同。\nloss = torch.nn.CrossEntropyLoss()\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root='/home/kesci/input/FashionMNIST2065')\nd2l.train_ch3(\n    net,\n    train_iter,\n    test_iter,\n    loss,\n    num_epochs,\n    batch_size,\n    params,\n    lr)\n"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ar5qzrs","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"99EEA36EC08E471E8EC87F0AF6FCEF34","mdEditEnable":false},"source":"## 丢弃法pytorch简洁实现"},{"cell_type":"code","execution_count":28,"metadata":{"graffitiCellId":"id_t8mnig0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"28E345EC41EB48D8876B6A15A3E18FBD","collapsed":false,"scrolled":false},"outputs":[],"source":"net = nn.Sequential(\n        d2l.FlattenLayer(),\n        nn.Linear(num_inputs, num_hiddens1),\n        nn.ReLU(),\n        nn.Dropout(drop_prob1),\n        nn.Linear(num_hiddens1, num_hiddens2), \n        nn.ReLU(),\n        nn.Dropout(drop_prob2),\n        nn.Linear(num_hiddens2, 10)\n        )\n\nfor param in net.parameters():\n    nn.init.normal_(param, mean=0, std=0.01)"},{"cell_type":"code","execution_count":31,"metadata":{"graffitiCellId":"id_iba0hj1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2EDA1430541148BB863DC9071ED92323","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 1, loss 0.0046, train acc 0.553, test acc 0.736\nepoch 2, loss 0.0023, train acc 0.785, test acc 0.803\nepoch 3, loss 0.0019, train acc 0.818, test acc 0.756\nepoch 4, loss 0.0018, train acc 0.835, test acc 0.829\nepoch 5, loss 0.0016, train acc 0.848, test acc 0.851\n","name":"stdout"}],"source":"optimizer = torch.optim.SGD(net.parameters(), lr=0.5)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_m2ujou7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"88258E5A8E1B4FCAA576FB7FB1AAC42C","mdEditEnable":false},"source":"# 总结\n\n- 验证数据集 vs 训练数据集 vs 测试数据集\n验证数据集可以用来调整模型参数，测试数据集不可以用来调整模型参数，如果使用测试数据集调整模型参数，可能在测试数据集上发生一定程度的过拟合，此时将不能用测试误差来近似泛化误差。在数据量不大的情况，`k折交叉验证`（将数据分为k份，每次选择一份用于验证模型，其余的用于训练模型）是一种常用的验证方法\n\n- 欠拟合现象：模型无法达到一个较低的误差\n    \n- 过拟合现象：训练误差较低但是泛化误差依然较高，二者相差较大\n\n- 过拟合 vs 欠拟合\n过拟合是指训练误差达到一个较低的水平，而泛化误差依然较大。\n欠拟合是指训练误差和泛化误差都不能达到一个较低的水平。\n过拟合还可以使用`权重衰减`和`丢弃法`来缓解，即使在一个比较小的数据集上使用了权重衰减和丢弃法之后也能够达到一个比较好的效果。\nL2范数正则化（权重衰减的具体实例）是用来应对过拟合的解决方案。"},{"metadata":{"id":"6A43FF94F1A0476288B07318C5225A80","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 小结练习\n\n\n- 为什么优化器中只对权重参数设置衰减，而不对偏置参数设置衰减呢？\n\n- L2范数惩罚项通过`惩罚绝对值`较大的参数的方法来应对过拟合的。这里面的惩罚绝对值较大的参数是什么意思？\n\n- 我对过拟合和欠拟合的理解（@同济子豪兄）：\nhttps://www.bilibili.com/video/av86713932?p=2\n使用正则化可以防止过拟合，比如Dropout、Batch Normalization等\n我对Dropout的理解：https://www.bilibili.com/video/av86713932?p=8\nDropout为什么能防止过拟合：打破神经网络之间的联合依赖适应性、化整为零、有性繁殖、相当于模型集成和数据增强\n\n- 按照最开始的说法，训练集，测试集（用来测试训练成果），验证集（用来训练超参数或者选择模型），但K折交叉验证为什么只有k-1个训练和1个验证，没有测试集？“。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。”那么这里的k次训练误差是哪里来的呢？\n\n- K折交叉验证中要使用哪一次训练结果来当最终模型呢？\n\n- 过拟合与欠拟合 vs 数据集的大小和模型复杂度"},{"metadata":{"id":"9823E6D397E34ACE97EE6D1AA57CC0C1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Part2 梯度消失、梯度爆炸\n- 梯度消失和梯度爆炸\n- 影响模型效果的其他因素影响\n> **协变量偏移**\n**标签偏移**\n**概念偏移**\n- Kaggle房价预测（实战）"},{"metadata":{"id":"1A7483F4C1AC4424A48F9BFE9A3D33FC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 梯度消失和梯度爆炸\n\n深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。\n\n**当神经网络的层数较多时，模型的数值稳定性容易变差。**\n\n假设一个层数为$L$的多层感知机的第$l$层$\\boldsymbol{H}^{(l)}$的权重参数为$\\boldsymbol{W}^{(l)}$，输出层$\\boldsymbol{H}^{(L)}$的权重参数为$\\boldsymbol{W}^{(L)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\\phi(x) = x$。给定输入$\\boldsymbol{X}$，多层感知机的第$l$层的输出$\\boldsymbol{H}^{(l)} = \\boldsymbol{X} \\boldsymbol{W}^{(1)} \\boldsymbol{W}^{(2)} \\ldots \\boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\\boldsymbol{X}$分别与$0.2^{30} \\approx 1 \\times 10^{-21}$（消失）和$5^{30} \\approx 9 \\times 10^{20}$（爆炸）的乘积。当层数较多时，梯度的计算也容易出现消失或爆炸。\n\n# 随机初始化模型参数\n\n在神经网络中，通常需要随机初始化模型参数。下面我们来解释这样做的原因。\n\n回顾多层感知机一节描述的多层感知机。为了方便解释，假设输出层只保留一个输出单元$o_1$（删去$o_2$和$o_3$以及指向它们的箭头），且隐藏层使用相同的激活函数。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。\n\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5jg76kloy.png?imageView2/0/w/960/h/960)\n\n\n\n###  PyTorch的默认随机初始化\n\n随机初始化模型参数的方法有很多。在线性回归的简洁实现中，我们使用`torch.nn.init.normal_()`使模型`net`的权重参数采用正态分布的随机初始化方式。不过，PyTorch中`nn.Module`的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考[源代码](https://github.com/pytorch/pytorch/tree/master/torch/nn/modules)），因此一般不用我们考虑。\n\n\n### Xavier随机初始化\n\n还有一种比较常用的随机初始化方法叫作Xavier随机初始化。\n假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布\n\n\n$$\nU\\left(-\\sqrt{\\frac{6}{a+b}}, \\sqrt{\\frac{6}{a+b}}\\right).\n$$\n\n\n它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。"},{"metadata":{"id":"3E407618ABCE47AE851259A9BEB65EF8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 考虑环境因素\n\n## 协变量偏移\n\n这里我们假设，虽然输入的分布可能随时间而改变，但是标记函数，即条件分布P（y∣x）不会改变。虽然这个问题容易理解，但在实践中也容易忽视。\n\n想想区分猫和狗的一个例子。我们的训练数据使用的是猫和狗的真实的照片，但是在测试时，我们被要求对猫和狗的卡通图片进行分类。\n\n|cat|cat|dog|dog|\n|:---------------:|:---------------:|:---------------:|:---------------:|\n|![Image Name](https://cdn.kesci.com/upload/image/q5jg8j72fl.jpg?imageView2/0/w/200/h/200)|![Image Name](https://cdn.kesci.com/upload/image/q5jg993za3.jpg?imageView2/0/w/200/h/200)|![Image Name](https://cdn.kesci.com/upload/image/q5jg9tqs4s.jpg?imageView2/0/w/200/h/200)|![Image Name](https://cdn.kesci.com/upload/image/q5jga6mnsk.jpg?imageView2/0/w/200/h/200)|\n\n测试数据：\n\n|cat|cat|dog|dog|\n|:---------------:|:---------------:|:---------------:|:---------------:|\n|![Image Name](https://cdn.kesci.com/upload/image/q5jgat5lsd.png?imageView2/0/w/200/h/200)|![Image Name](https://cdn.kesci.com/upload/image/q5jgbaoij8.png?imageView2/0/w/200/h/200)|![Image Name](https://cdn.kesci.com/upload/image/q5jgbswvbb.png?imageView2/0/w/200/h/200)|![Image Name](https://cdn.kesci.com/upload/image/q5jgc5j7zv.png?imageView2/0/w/200/h/200)\n|\n\n显然，这不太可能奏效。训练集由照片组成，而测试集只包含卡通。在一个看起来与测试集有着本质不同的数据集上进行训练，而不考虑如何适应新的情况，这是不是一个好主意。不幸的是，这是一个非常常见的陷阱。\n\n统计学家称这种协变量变化是因为问题的根源在于特征分布的变化（即协变量的变化）。数学上，我们可以说P（x）改变了，但P（y∣x）保持不变。尽管它的有用性并不局限于此，当我们认为x导致y时，协变量移位通常是正确的假设。\n\n\n## 标签偏移\n\n    \n当我们认为导致偏移的是标签P（y）上的边缘分布的变化，但类条件分布是不变的P（x∣y）时，就会出现相反的问题。当我们认为y导致x时，标签偏移是一个合理的假设。例如，通常我们希望根据其表现来预测诊断结果。在这种情况下，我们认为诊断引起的表现，即疾病引起的症状。有时标签偏移和协变量移位假设可以同时成立。例如，当真正的标签函数是确定的和不变的，那么协变量偏移将始终保持，包括如果标签偏移也保持。有趣的是，当我们期望标签偏移和协变量偏移保持时，使用来自标签偏移假设的方法通常是有利的。这是因为这些方法倾向于操作看起来像标签的对象，这（在深度学习中）与处理看起来像输入的对象（在深度学习中）相比相对容易一些。\n\n病因（要预测的诊断结果）导致 症状（观察到的结果）。  \n\n训练数据集，数据很少只包含流感p(y)的样本。  \n\n而测试数据集有流感p(y)和流感q(y)，其中不变的是流感症状p(x|y)。\n\n\n## 概念偏移\n\n另一个相关的问题出现在概念转换中，即标签本身的定义发生变化的情况。这听起来很奇怪，毕竟猫就是猫。的确，猫的定义可能不会改变，但我们能不能对软饮料也这么说呢？事实证明，如果我们周游美国，按地理位置转移数据来源，我们会发现，即使是如图所示的这个简单术语的定义也会发生相当大的概念转变。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5jgd81pl3.png?imageView2/0/w/640/h/640)\n\n$$\n美国软饮料名称的概念转变 \n$$\n如果我们要建立一个机器翻译系统，分布P（y∣x）可能因我们的位置而异。这个问题很难发现。另一个可取之处是P（y∣x）通常只是逐渐变化。\n"},{"metadata":{"id":"9B6E8CE00C7948AD8CC722C2510633DB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Kaggle 房价预测（实战）\n- 数据的预处理\n- 模型的设计\n- 超参的选择\n\n作为深度学习基础篇章的总结，我们将对本章内容学以致用。下面，让我们动手实战一个Kaggle比赛：房价预测。本节将提供未经调优的数据的预处理、模型的设计和超参数的选择。我们希望读者通过动手操作、仔细观察实验现象、认真分析实验结果并不断调整方法，得到令自己满意的结果。"},{"metadata":{"id":"63C2BB43F24246DB8622D78FDC6609C0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"%matplotlib inline\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\nprint(torch.__version__)\ntorch.set_default_tensor_type(torch.FloatTensor)","execution_count":1},{"metadata":{"id":"1B96768B0E4C41A0B81E726196A10B6A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 获取和读取数据集\n\n比赛数据分为训练数据集和测试数据集。两个数据集都包括每栋房子的特征，如街道类型、建造年份、房顶类型、地下室状况等特征值。这些特征值有连续的数字、离散的标签甚至是缺失值“na”。只有训练数据集包括了每栋房子的价格，也就是标签。我们可以访问比赛网页，点击“Data”标签，并下载这些数据集。\n\n我们将通过`pandas`库读入并处理数据。在导入本节需要的包前请确保已安装`pandas`库。\n假设解压后的数据位于`/home/kesci/input/houseprices2807/`目录，它包括两个csv文件。下面使用`pandas`读取这两个文件。"},{"metadata":{"id":"7AD659030A054480BF743CE81469F204","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"test_data = pd.read_csv(\"/home/kesci/input/houseprices2807/house-prices-advanced-regression-techniques/test.csv\")\ntrain_data = pd.read_csv(\"/home/kesci/input/houseprices2807/house-prices-advanced-regression-techniques/train.csv\")","execution_count":3},{"metadata":{"id":"375EE5FA74034617880215BE01CE2175","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(1460, 81)"},"transient":{},"execution_count":4}],"source":"# 训练数据集包括1460个样本、80个特征和1个标签\ntrain_data.shape","execution_count":4},{"metadata":{"id":"F46C726E454241298F82ACDF3EDEF527","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(1459, 80)"},"transient":{},"execution_count":5}],"source":"# 测试数据集包括1459个样本和80个特征。我们需要将测试数据集中每个样本的标签预测出来\ntest_data.shape","execution_count":5},{"metadata":{"id":"06CD38682A244875BEE66F8D5AEE36CD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n0   1          60       RL         65.0       WD        Normal     208500\n1   2          20       RL         80.0       WD        Normal     181500\n2   3          60       RL         68.0       WD        Normal     223500\n3   4          70       RL         60.0       WD       Abnorml     140000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"transient":{},"execution_count":6}],"source":"# 前4个样本的前4个特征以及后2个特征和标签（SalePrice）\ntrain_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]","execution_count":6},{"metadata":{"id":"9729755B1E6C40159E845AFCA76063CE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 将所有的训练数据和测试数据的79个特征按样本连结\nall_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))","execution_count":7},{"metadata":{"id":"684615134AE94EE0971C9CE689190114","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 预处理数据\n我们对连续数值的特征做标准化（standardization）：设该特征在整个数据集上的均值为$\\mu$，标准差为$\\sigma$。那么，我们可以将该特征的每个值先减去$\\mu$再除以$\\sigma$得到标准化后的每个特征值。对于缺失的特征值，我们将其替换成该特征的均值。"},{"metadata":{"id":"CAF4F7413EE9460B86BA5EB40F4A47EC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\nall_features[numeric_features] = all_features[numeric_features].apply(\n    lambda x: (x - x.mean()) / (x.std()))\n# 标准化后，每个数值特征的均值变为0，所以可以直接用0来替换缺失值\nall_features[numeric_features] = all_features[numeric_features].fillna(0)","execution_count":8},{"metadata":{"id":"8ABF3ABF80364523A428318301443B15","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(2919, 331)"},"transient":{},"execution_count":9}],"source":"# dummy_na=True将缺失值也当作合法的特征值并为其创建指示特征\nall_features = pd.get_dummies(all_features, dummy_na=True)\nall_features.shape","execution_count":9},{"metadata":{"id":"C872D5D8F6D0431789E03577633E04B7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"通过`values`属性得到Numpy格式的数据，并转成`Tensor`方便后面的训练"},{"metadata":{"id":"1F4A9EE2BD2940B7BD6FB644AB54571F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"n_train = train_data.shape[0]\ntrain_features = torch.tensor(all_features[:n_train].values, dtype=torch.float)\ntest_features = torch.tensor(all_features[n_train:].values, dtype=torch.float)\ntrain_labels = torch.tensor(train_data.SalePrice.values, dtype=torch.float).view(-1, 1)","execution_count":10},{"metadata":{"id":"1DF494CB6BEB4FB683CB4F79C68BB808","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 训练模型"},{"metadata":{"id":"D4BEC093A36B436AA1610CD564C9AF17","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"loss = torch.nn.MSELoss() # MSELoss 均方误差\n\ndef get_net(feature_num):\n    net = nn.Linear(feature_num, 1)\n    for param in net.parameters():\n        nn.init.normal_(param, mean=0, std=0.01)\n    return net","execution_count":11},{"metadata":{"id":"D9A07B33EC4E48B1A886C29494936A75","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"定义比赛用来评价模型的对数均方根误差。给定预测值$\\hat y_1, \\ldots, \\hat y_n$和对应的真实标签$y_1,\\ldots, y_n$，它的定义为\n\n$$\n\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(\\log(y_i)-\\log(\\hat y_i)\\right)^2}.\n$$\n"},{"metadata":{"id":"3566439FCAAE418B9BB92321597CABBF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 对数均方根误差的实现\ndef log_rmse(net, features, labels):\n    with torch.no_grad():\n        # 将小于1的值设成1，使得取对数时数值更稳定\n        clipped_preds = torch.max(net(features), torch.tensor(1.0))\n        rmse = torch.sqrt(2 * loss(clipped_preds.log(), labels.log()).mean())\n    return rmse.item()","execution_count":13},{"metadata":{"id":"CF295DD641614447882E819274DACF55","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"```\nAdam vs 小批量随机梯度下降\nAdam优化算法：对学习率不太敏感\n```"},{"metadata":{"id":"C325C89A609C4F368DB9839C35E9A672","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def train(net, train_features, train_labels, test_features, test_labels,\n          num_epochs, learning_rate, weight_decay, batch_size):\n    train_ls, test_ls = [], []\n    dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n    # 这里使用了Adam优化算法\n    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=weight_decay) \n    net = net.float()\n    for epoch in range(num_epochs):\n        for X, y in train_iter:\n            l = loss(net(X.float()), y.float())\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n        train_ls.append(log_rmse(net, train_features, train_labels))\n        if test_labels is not None:\n            test_ls.append(log_rmse(net, test_features, test_labels))\n    return train_ls, test_ls","execution_count":14},{"metadata":{"id":"837154AD13F0465A87F17DA2F655A5DC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## K折交叉验证\n我们在模型选择、欠拟合和过拟合中介绍了$K$折交叉验证。它将被用来选择模型设计并调节超参数。下面实现了一个函数，它返回第`i`折交叉验证时所需要的训练和验证数据。"},{"metadata":{"id":"E269FDB9298A4DD4BBD5F690E03C1688","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def get_k_fold_data(k, i, X, y):\n    # 返回第i折交叉验证时所需要的训练和验证数据\n    assert k > 1\n    fold_size = X.shape[0] // k\n    X_train, y_train = None, None\n    for j in range(k):\n        idx = slice(j * fold_size, (j + 1) * fold_size)\n        X_part, y_part = X[idx, :], y[idx]\n        if j == i:\n            X_valid, y_valid = X_part, y_part\n        elif X_train is None:\n            X_train, y_train = X_part, y_part\n        else:\n            X_train = torch.cat((X_train, X_part), dim=0)\n            y_train = torch.cat((y_train, y_part), dim=0)\n    return X_train, y_train, X_valid, y_valid","execution_count":15},{"metadata":{"id":"F3356A552F804A7B9AAAF3D7625350BD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"在$K$折交叉验证中我们训练$K$次并返回训练和验证的平均误差"},{"metadata":{"id":"D1D8DF93F0C644818B0EA44EC05417A0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def k_fold(k, X_train, y_train, num_epochs,\n           learning_rate, weight_decay, batch_size):\n    train_l_sum, valid_l_sum = 0, 0\n    for i in range(k):\n        data = get_k_fold_data(k, i, X_train, y_train)\n        net = get_net(X_train.shape[1])\n        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n                                   weight_decay, batch_size)\n        train_l_sum += train_ls[-1]\n        valid_l_sum += valid_ls[-1]\n        if i == 0:\n            d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse',\n                         range(1, num_epochs + 1), valid_ls,\n                         ['train', 'valid'])\n        print('fold %d, train rmse %f, valid rmse %f' % (i, train_ls[-1], valid_ls[-1]))\n    return train_l_sum / k, valid_l_sum / k","execution_count":16},{"metadata":{"id":"8435C2836473421988A475CB61828475","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 模型选择\n我们使用一组未经调优的超参数并计算交叉验证误差。可以改动这些超参数来尽可能减小平均测试误差。\n有时候你会发现一组参数的训练误差可以达到很低，但是在$K$折交叉验证上的误差可能反而较高。这种现象很可能是由过拟合造成的。因此，当训练误差降低时，我们要观察$K$折交叉验证上的误差是否也相应降低。"},{"metadata":{"id":"83092C56C6474E85A4F8317BE7E1FF97","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"fold 0, train rmse 0.240275, valid rmse 0.221708\nfold 1, train rmse 0.229394, valid rmse 0.268690\nfold 2, train rmse 0.232215, valid rmse 0.238140\nfold 3, train rmse 0.237794, valid rmse 0.218482\nfold 4, train rmse 0.230983, valid rmse 0.258570\n5-fold validation: avg train rmse 0.234132, avg valid rmse 0.241118\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/83092C56C6474E85A4F8317BE7E1FF97/q5uh2n3jqp.svg\">"},"transient":{}}],"source":"k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64\ntrain_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr, weight_decay, batch_size)\nprint('%d-fold validation: avg train rmse %f, avg valid rmse %f' % (k, train_l, valid_l))","execution_count":17},{"metadata":{"id":"BEFE9251498640C78985EAE7E0AD065B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"定义训练和预测函数以及设计模型和调节参数"},{"metadata":{"id":"9D1A95F796C4481A9D7999E7978B39D7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def train_and_pred(train_features, test_features, train_labels, test_data,\n                   num_epochs, lr, weight_decay, batch_size):\n    net = get_net(train_features.shape[1])\n    train_ls, _ = train(net, train_features, train_labels, None, None,\n                        num_epochs, lr, weight_decay, batch_size)\n    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse')\n    print('train rmse %f' % train_ls[-1])\n    preds = net(test_features).detach().numpy()\n    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n    submission.to_csv('./submission.csv', index=False)\n    # sample_submission_data = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n","execution_count":18},{"metadata":{"id":"9FD090B8AF95450685AB768C35BFD31D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"对测试数据集进行预测"},{"metadata":{"id":"E08A427D329D40DB891EFB6F9CCEC851","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"train rmse 0.229515\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/E08A427D329D40DB891EFB6F9CCEC851/q5uh32hffr.svg\">"},"transient":{}}],"source":"train_and_pred(train_features, test_features, train_labels, test_data, num_epochs, lr, weight_decay, batch_size)","execution_count":19},{"metadata":{"id":"99E9C72458F64103873402EA5A4E4B82","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 小结练习\n（1）梯度消失 vs 梯度爆炸\n在激活函数的选择的地方讲过，在深层网络中尽量避免选择sigmoid和tanh激活函数，原因是这两个激活函数会把元素转换到`[0, 1]`和`[-1, 1]`之间，会加剧梯度消失的现象。\n\n解析：\n\n- 定义：激活函数$h$是一个几乎可微的函数`h : R → R` .\n\n- 常见的`激活函数`有：`sigmoid`；`tanh`；`ReLU`；`LReLU`, `PReLU`, `RReLU`；`ELU（Exponential Linear Units）`；`softplus`；`softsign`,`softmax`等\n\n- [常见的激活函数及其性质比较](https://www.jiqizhixin.com/graph/technologies/1697e627-30e7-48a6-b799-39e2338ffab5)\n\n（2）协变量偏移 vs 标签偏移 vs 概念偏移\n如果数据量足够的情况下，确保训练数据集和测试集中的数据取自同一个数据集，可以防止协变量偏移和标签偏移是正确的。如果数据量很少，少到测试集中存在训练集中未包含的标签，就会发生标签偏移。\n- 梯度消失的理解（@同济子豪兄）：\nhttps://www.bilibili.com/video/av86713932?p=3\ntanh、sigmoid等存在饱和的激活函数会出现梯度消失\n在循环神经网络中使用BPTT反向传播算法，也会出现梯度消失或梯度爆炸\nhttps://www.bilibili.com/video/av86713932?p=9\n\n- BP（反向传播）的解释：（@dw-安晟）\n将权重沿负梯度方向进行小步长的位移，可以逐渐的使loss函数下降，从而让模型具有好的效果。\n反向传播其实就是从终点的loss往回回溯出每一个变量的梯度，从而好进行梯度下降的优化。\n我看到梯度下降知识点的视频已放出，可以去学习一下，应该就对反向传播的意义有比较好的理解了。\n"},{"metadata":{"id":"373688D8201643878CDF395E28A4CFE3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Part3 循环神经网络进阶\n- GRU\n- LSTM\n- 深层RNN\n- 双向RNN"},{"metadata":{"id":"06AB7B28D09D4F2AB8A738DA8403281A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"RNN循环神经网络存在的问题：梯度较容易出现衰减或爆炸（BPTT反向传播），梯度裁剪可以缓解梯度爆炸，但不能解决梯度衰减问题\nGRU门控循环神经网络：捕捉时间序列中时间步距离较大的依赖关系  \n**RNN**:  \n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5jjvcykud.png?imageView2/0/w/320/h/320)\n\n\n$$\nH_{t} = ϕ(X_{t}W_{xh} + H_{t-1}W_{hh} + b_{h})\n$$\n**GRU**:\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5jk0q9suq.png?imageView2/0/w/640/h/640)\n\n\n\n$$\nR_{t} = σ(X_tW_{xr} + H_{t−1}W_{hr} + b_r)\\\\    \nZ_{t} = σ(X_tW_{xz} + H_{t−1}W_{hz} + b_z)\\\\  \n\\widetilde{H}_t = tanh(X_tW_{xh} + (R_t ⊙H_{t−1})W_{hh} + b_h)\\\\\nH_t = Z_t⊙H_{t−1} + (1−Z_t)⊙\\widetilde{H}_t\n$$\n• **重置门**有助于捕捉时间序列里短期的依赖关系；  \n• **更新门**有助于捕捉时间序列里长期的依赖关系。    \n## GRU\n- GRU有重置门和更新门，没有遗忘门。重置门有助于捕捉时间序列里短期的依赖关系，更新门有助于捕捉时间序列⾥长期的依赖关系"},{"metadata":{"id":"240AC73A1DF04C1E85631388FE5C3F09","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import os\nos.listdir('/home/kesci/input')\n\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F","execution_count":1},{"metadata":{"id":"67148EE81E734F898AE5AE8CB332B40E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import sys\nsys.path.append(\"../input/\")\nimport d2l_jay9460 as d2l\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()","execution_count":2},{"metadata":{"id":"1AA8991EA9D64EF28BECE6A9B3CD1053","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 初始化参数"},{"metadata":{"id":"5D1FBBB534364C518E9C76A4448D3680","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"will use cpu\n","name":"stdout"}],"source":"num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\nprint('will use', device)\n\ndef get_params():  \n    def _one(shape):\n        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32) #正态分布\n        return torch.nn.Parameter(ts, requires_grad=True)\n    def _three():\n        return (_one((num_inputs, num_hiddens)),\n                _one((num_hiddens, num_hiddens)),\n                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=True))\n     \n    W_xz, W_hz, b_z = _three()  # 更新门参数\n    W_xr, W_hr, b_r = _three()  # 重置门参数\n    W_xh, W_hh, b_h = _three()  # 候选隐藏状态参数\n    \n    # 输出层参数\n    W_hq = _one((num_hiddens, num_outputs))\n    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=True)\n    return nn.ParameterList([W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q])\n\ndef init_gru_state(batch_size, num_hiddens, device):   #隐藏状态初始化\n    return (torch.zeros((batch_size, num_hiddens), device=device), )","execution_count":3},{"metadata":{"id":"E0DFE1C2D1F5416BAE13454B73FC070E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### GRU模型"},{"metadata":{"id":"4A52FB85515A4C2D993C2D470345F2C0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def gru(inputs, state, params):\n    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n    H, = state\n    outputs = []\n    for X in inputs:\n        Z = torch.sigmoid(torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z)\n        R = torch.sigmoid(torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r)\n        H_tilda = torch.tanh(torch.matmul(X, W_xh) + R * torch.matmul(H, W_hh) + b_h)\n        H = Z * H + (1 - Z) * H_tilda\n        Y = torch.matmul(H, W_hq) + b_q\n        outputs.append(Y)\n    return outputs, (H,)","execution_count":4},{"metadata":{"id":"B438C8C0FE66408F8152496DC8DAED9A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 训练模型"},{"metadata":{"id":"36431C73B56F41788F34350EADF7063B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\npred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']","execution_count":5},{"metadata":{"id":"47C0288088A241228FDBB50267C422BE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 40, perplexity 151.074860, time 1.26 sec\n - 分开 我想你的让我爱爱女人 坏坏的让我爱爱女人女人 坏坏的让我爱爱女人女人 坏坏的让我爱爱女人女人 坏坏\n - 不分开 我想你的让我爱爱女人女人 坏坏的让我爱爱女人女人 坏坏的让我爱爱女人女人 坏坏的让我爱爱女人女人 \nepoch 80, perplexity 32.175123, time 1.28 sec\n - 分开 我想要你的微笑 像像在美不达 让我的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的\n - 不分开 你爱我的爱写 让我的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的\nepoch 120, perplexity 4.896328, time 1.28 sec\n - 分开 我想带你的微笑每天 想著你说 你已一起 再来一碗热粥 你在那里 在小村外的溪边河口默默等著我 娘子\n - 不分开 爱过我 一场两步三步四步望著天 看星星 一颗两颗三颗四颗 连成线背著背默默许下心愿 看远方的星如果\nepoch 160, perplexity 1.478549, time 1.33 sec\n - 分开 一直好酒 他被它一抽粥 静色蜡烛 温暖了空屋 白色蜡烛 温暖了空屋藤蔓植物 爬满了伯爵的坟墓 古堡\n - 不分开 爱过我一只会痛 有你去没有故有 想要和你飞棒球 想这样没担忧 唱着歌 一直走 我想就这样牵着你的手\n","name":"stdout"}],"source":"d2l.train_and_predict_rnn(gru, get_params, init_gru_state, num_hiddens,\n                          vocab_size, device, corpus_indices, idx_to_char,\n                          char_to_idx, False, num_epochs, num_steps, lr,\n                          clipping_theta, batch_size, pred_period, pred_len,\n                          prefixes)","execution_count":6},{"metadata":{"id":"AC2B08C6431347A78081FE0C6E8E1978","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### PyTorch简洁实现"},{"metadata":{"id":"CAB6F76884FF46E58E4C8F5342FD0AD2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 40, perplexity 1.016093, time 0.85 sec\n - 分开的黑色幽默 想通 却又再考倒我 说散 你想很久了吧? 我不想拆穿你 当作 是你开的玩笑 想通 却又再\n - 不分开始 担心今天的你过得好不好 整个画面是你 想你想的睡不著 嘴嘟嘟那可爱的模样 还有在你身上香香的味道\nepoch 80, perplexity 1.010066, time 0.86 sec\n - 分开的黑色幽默 说散 你想很久了吧? 我的认真败给黑色幽默 走过了很多地方 我来到伊斯坦堡 就像是童话故\n - 不分开始打呼 管家是一只会说法语举止优雅的猪 吸血前会念约翰福音做为弥补 拥有一双蓝色眼睛的凯萨琳公主 专\nepoch 120, perplexity 1.008352, time 0.90 sec\n - 分开的黑色幽默 想通 却又再考倒我 说散 你想很久了吧? 我不想拆穿你 当作 是你开的玩笑 想通 却又再\n - 不分开始移动 回到当初爱你的时空 停格内容不忠 所有回忆对着我进攻       所有回忆对着我进攻    \nepoch 160, perplexity 3.082501, time 0.90 sec\n - 分开 我有多是晴天 抛物线进球 穿梭时间的画面 我想想我的让我疯狂的可爱女人 从前 一颗心到 一定实现 \n - 不分开始共渡每一想通 爱可不可以造句如果听的见你手 一颗心到 一定中断熟悉 然后在一片荒芜 我有多是晴天 \n","name":"stdout"}],"source":"num_hiddens=256\nnum_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\npred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n\nlr = 1e-2 # 注意调整学习率\ngru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)\nmodel = d2l.RNNModel(gru_layer, vocab_size).to(device)\nd2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n                                corpus_indices, idx_to_char, char_to_idx,\n                                num_epochs, num_steps, lr, clipping_theta,\n                                batch_size, pred_period, pred_len, prefixes)","execution_count":7},{"metadata":{"id":"BF700D2B0511406C811B3FA23D1050D4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# LSTM（Long Short Term Memory）"},{"metadata":{"id":"5023E09D810B496F997D9EE8083CD41F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"** 长短期记忆（LSTM）**:  \n**遗忘门**：控制上一时间步的记忆细胞 \n**输入门**：控制当前时间步的输入  \n**输出门**：控制从记忆细胞到隐藏状态  \n（每个循环单元中的记忆细胞和循环单元的值为LSTM模型中的隐状态，而非参数，并不需要初始化）\n**记忆细胞**：一种特殊的隐藏状态的信息的流动  \n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5jk2bnnej.png?imageView2/0/w/640/h/640)\n\n$$\nI_t = σ(X_tW_{xi} + H_{t−1}W_{hi} + b_i) \\\\\nF_t = σ(X_tW_{xf} + H_{t−1}W_{hf} + b_f)\\\\\nO_t = σ(X_tW_{xo} + H_{t−1}W_{ho} + b_o)\\\\\n\\widetilde{C}_t = tanh(X_tW_{xc} + H_{t−1}W_{hc} + b_c)\\\\\nC_t = F_t ⊙C_{t−1} + I_t ⊙\\widetilde{C}_t\\\\\nH_t = O_t⊙tanh(C_t)\n$$\n"},{"metadata":{"id":"C5AAF8C631E74D5A8BCFEE159E3DA3D2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 初始化参数"},{"metadata":{"id":"9293D6FA7DED40A089AA087C83C78B8E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"will use cpu\n","name":"stdout"}],"source":"num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\nprint('will use', device)\n\ndef get_params():\n    def _one(shape):\n        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n        return torch.nn.Parameter(ts, requires_grad=True)\n    def _three():\n        return (_one((num_inputs, num_hiddens)),\n                _one((num_hiddens, num_hiddens)),\n                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=True))\n    \n    W_xi, W_hi, b_i = _three()  # 输入门参数\n    W_xf, W_hf, b_f = _three()  # 遗忘门参数\n    W_xo, W_ho, b_o = _three()  # 输出门参数\n    W_xc, W_hc, b_c = _three()  # 候选记忆细胞参数\n    \n    # 输出层参数\n    W_hq = _one((num_hiddens, num_outputs))\n    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=True)\n    return nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])\n\ndef init_lstm_state(batch_size, num_hiddens, device):\n    return (torch.zeros((batch_size, num_hiddens), device=device), \n            torch.zeros((batch_size, num_hiddens), device=device))","execution_count":8},{"metadata":{"id":"8BD7159E502A450F88EBFDB13EF6E7C0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### LSTM模型"},{"metadata":{"id":"6645AB71384A4477BBAB662199C986C9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def lstm(inputs, state, params):\n    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params\n    (H, C) = state\n    outputs = []\n    for X in inputs:\n        I = torch.sigmoid(torch.matmul(X, W_xi) + torch.matmul(H, W_hi) + b_i)\n        F = torch.sigmoid(torch.matmul(X, W_xf) + torch.matmul(H, W_hf) + b_f)\n        O = torch.sigmoid(torch.matmul(X, W_xo) + torch.matmul(H, W_ho) + b_o)\n        C_tilda = torch.tanh(torch.matmul(X, W_xc) + torch.matmul(H, W_hc) + b_c)\n        C = F * C + I * C_tilda\n        H = O * C.tanh()\n        Y = torch.matmul(H, W_hq) + b_q\n        outputs.append(Y)\n    return outputs, (H, C)","execution_count":9},{"metadata":{"id":"AE649336A25B4D83BCA152FD62225AD2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 训练模型"},{"metadata":{"id":"9EFDF45C01B94C0F899C7534945BA8BE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 40, perplexity 210.227055, time 1.50 sec\n - 分开 我不不我 我不不我 我不不我 我不不我 我不不我 我不不我 我不不我 我不不我 我不不我 我不不我\n - 不分开 我不不我 我不不我 我不不我 我不不我 我不不我 我不不我 我不不我 我不不我 我不不我 我不不我\nepoch 80, perplexity 67.084115, time 1.52 sec\n - 分开 我想你你的你 我想想你想想 我不要 我不我 我不要 我不要 我不要 我不要 我不要 我不要 我不要\n - 不分开 你想我 你你我 我想要这我 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要\nepoch 120, perplexity 15.117320, time 1.47 sec\n - 分开 我想你的话笑笑 像通  又给我 说你了没有牵着 我说去很生活 一你了我不着 我该了我 我该了这节奏\n - 不分开 我有你的爱笑笑像龙卷风 不不开我 别我的让 如果 一壶  有一些空 我的上梦 我的风空 你的风空 \nepoch 160, perplexity 3.974053, time 1.45 sec\n - 分开 我想带你的微笑 像少是不不多 除静是乌鸦抢了它的窝 它在灌木丛旁邂逗 一物死了它比谁都难过 印地安\n - 不分开 你在我不不经大怒火 就想开么已你很久 别说躲都说打到听 不要我 说你眼睛看着我 别发抖 快给我抬起\n","name":"stdout"}],"source":"num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\npred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n\nd2l.train_and_predict_rnn(lstm, get_params, init_lstm_state, num_hiddens,\n                          vocab_size, device, corpus_indices, idx_to_char,\n                          char_to_idx, False, num_epochs, num_steps, lr,\n                          clipping_theta, batch_size, pred_period, pred_len,\n                          prefixes)","execution_count":10},{"metadata":{"id":"0B63EE10F5BE4FD3BEF5D101166A3BC7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### PyTorch简洁实现"},{"metadata":{"id":"C306415D82B3473BAB2710E27CB623FD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 40, perplexity 1.020872, time 1.02 sec\n - 分开始 相思寄红豆无能为力的在人海中漂泊心伤透 娘子她人在江南等我 泪不休 语沉默 娘子她人在江南等我 \n - 不分开 你怎么每天祈祷我的心跳你知道  杵在伊斯坦堡 却只想你和汉堡 我想要你的微笑每天都能看到  我知道\nepoch 80, perplexity 1.015200, time 1.02 sec\n - 分开始移动 回到当初爱你的时空 停格内容不忠 所有回忆对着我进攻 我的伤口被你拆封 誓言太沉重泪被纵容 \n - 不分开 你叫我怎么跟你像 不要再这样打我妈妈 我说的话 你甘会听 不要再这样打我妈妈 难道你手不会痛吗 不\nepoch 120, perplexity 1.012637, time 1.00 sec\n - 分开始 家乡的爹娘早已苍老了轮廓 娘子我欠你太多 一壶好酒 再来一碗热粥 配上几斤的牛肉 我说店小二 三\n - 不分开 你怎么面对我 甩开球我满腔的怒火 我想揍你已经很久 别想躲 说你眼睛看着我 别发抖 快给我抬起头 \nepoch 160, perplexity 1.011087, time 1.01 sec\n - 分开始移动 回到当初爱你的时空 停格内容不忠 所有回忆对着我进攻 我的伤口被你拆封 誓言太沉重泪被纵容 \n - 不分开 你不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活 静静悄悄默\n","name":"stdout"}],"source":"num_hiddens=256\nnum_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\npred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n\nlr = 1e-2 # 注意调整学习率\nlstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)\nmodel = d2l.RNNModel(lstm_layer, vocab_size)\nd2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n                                corpus_indices, idx_to_char, char_to_idx,\n                                num_epochs, num_steps, lr, clipping_theta,\n                                batch_size, pred_period, pred_len, prefixes)","execution_count":11},{"metadata":{"id":"E71E3301F782450E81B5001EFCD90C5E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 深度循环神经网络  \n\n![Image Name](https://cdn.kesci.com/upload/image/q5jk3z1hvz.png?imageView2/0/w/320/h/320)\n\n\n$$\n\\boldsymbol{H}_t^{(1)} = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(1)} + \\boldsymbol{H}_{t-1}^{(1)} \\boldsymbol{W}_{hh}^{(1)} + \\boldsymbol{b}_h^{(1)})\\\\\n\\boldsymbol{H}_t^{(\\ell)} = \\phi(\\boldsymbol{H}_t^{(\\ell-1)} \\boldsymbol{W}_{xh}^{(\\ell)} + \\boldsymbol{H}_{t-1}^{(\\ell)} \\boldsymbol{W}_{hh}^{(\\ell)} + \\boldsymbol{b}_h^{(\\ell)})\\\\\n\\boldsymbol{O}_t = \\boldsymbol{H}_t^{(L)} \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q\n$$"},{"metadata":{"id":"4095FAD4F17F41C1B1AEE134F68226D6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 40, perplexity 1.056776, time 1.48 sec\n - 分开 不要了这样打我妈妈 难道你手不会痛吗 不要再这样打我妈妈 难道你手不会痛吗 不要再这样打我妈妈 难\n - 不分开 一场戏 宁愿心碎哭泣 再狠狠忘记 你爱过我的证据 让晶莹的泪滴 闪烁成回忆 伤人的美丽 你的完美主\nepoch 80, perplexity 1.040581, time 1.44 sec\n - 分开 游荡在蓝天边 一片云掉落在我面前 捏成你的形状 随风跟著我 一口一口吃掉忧愁 载著你 彷彿载著阳光\n - 不分开 说完样 生透 我不著 爱情来的太快就像龙卷风 离不开暴风圈来不及逃 我不能再想 我不要再想 我不 \nepoch 120, perplexity 1.010937, time 1.52 sec\n - 分开 所有回忆对着我进攻 我的伤口被你拆封 誓言太沉重泪被纵容 脸上汹涌失控 穿梭时间的画面的钟 从反方\n - 不分开 所有回忆对着我进攻 我的伤口被你拆封 誓言太沉重泪被纵容 脸上汹涌失控 穿梭时间的画面的钟 从反方\nepoch 160, perplexity 1.010987, time 1.56 sec\n - 分开 在色悄剧 我对悔努静烟赏你那张我深爱的脸 祭司 神殿 征战 弓箭 是谁的从前 喜欢在人潮中你只属于\n - 不分开  穿过云层 我试著努力向你奔跑 爱才送到 你却已在别人怀抱 就是开不了口让她知道 我一定会呵护著你\n","name":"stdout"}],"source":"num_hiddens=256\nnum_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\npred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n\nlr = 1e-2 # 注意调整学习率\n\ngru_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens,num_layers=2)\nmodel = d2l.RNNModel(gru_layer, vocab_size).to(device)\nd2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n                                corpus_indices, idx_to_char, char_to_idx,\n                                num_epochs, num_steps, lr, clipping_theta,\n                                batch_size, pred_period, pred_len, prefixes)","execution_count":12},{"metadata":{"id":"0AE1E65EB003471A9FE74B7FFDB9CB49","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### PyTorch简洁实现"},{"metadata":{"id":"5A2E3F480857447E8084F04832D9DB90","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"```\ngru_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens,num_layers=6)\nmodel = d2l.RNNModel(gru_layer, vocab_size).to(device)\nd2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n                                corpus_indices, idx_to_char, char_to_idx,\n                                num_epochs, num_steps, lr, clipping_theta,\n                                batch_size, pred_period, pred_len, prefixes)\n```"},{"metadata":{"id":"476705B5D3954CD6A8151F5711B22DA3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 双向循环神经网络 \n\n![Image Name](https://cdn.kesci.com/upload/image/q5j8hmgyrz.png?imageView2/0/w/320/h/320)\n\n$$ \n\\begin{aligned} \\overrightarrow{\\boldsymbol{H}}_t &= \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(f)} + \\overrightarrow{\\boldsymbol{H}}_{t-1} \\boldsymbol{W}_{hh}^{(f)} + \\boldsymbol{b}_h^{(f)})\\\\\n\\overleftarrow{\\boldsymbol{H}}_t &= \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(b)} + \\overleftarrow{\\boldsymbol{H}}_{t+1} \\boldsymbol{W}_{hh}^{(b)} + \\boldsymbol{b}_h^{(b)}) \\end{aligned} $$\n$$\n\\boldsymbol{H}_t=(\\overrightarrow{\\boldsymbol{H}}_{t}, \\overleftarrow{\\boldsymbol{H}}_t)\n$$\n$$\n\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q\n$$"},{"metadata":{"id":"5023CA1A850046FF9AB15A1241214047","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### PyTorch简洁实现"},{"metadata":{"id":"20EAB28A66AB4FF58ED8A31D48FBCA50","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"```\nnum_hiddens=128\nnum_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e-2, 1e-2\npred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n\nlr = 1e-2 # 注意调整学习率\n\ngru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens,bidirectional=True)\nmodel = d2l.RNNModel(gru_layer, vocab_size).to(device)\nd2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n                                corpus_indices, idx_to_char, char_to_idx,\n                                num_epochs, num_steps, lr, clipping_theta,\n                                batch_size, pred_period, pred_len, prefixes)\n```"},{"metadata":{"id":"216F337BE9AA4FDB9D3B028FB6B330F8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 小结练习\n**GRU vs LSTM vs DRNN vs BiRNN**\n\n- @小罗同学：循环神经网络基础推荐视频[同济子豪兄的B站视频](https://www.bilibili.com/video/av86713932/?p=9)\n推荐学习资料(知乎高赞)：[LSTM知乎高赞文章](https://zhuanlan.zhihu.com/p/32085405）\n- GRU与LSTM非常的相似，也是基于门控的RNN，并且相对于LSTM是一种改进，性能和LSTM差不多，但是参数量比LSTM少，看下面这个链接，可以更好的理解。\n知乎高赞：https://zhuanlan.zhihu.com/p/32481747\n\n**延伸阅读：**\n- [一文搞懂RNN（循环神经网络）基础篇](https://zhuanlan.zhihu.com/p/30844905)\n- [Recurrent Neural Networks cheatsheet]()\n- [Understanding RNN and LSTM](https://towardsdatascience.com/understanding-rnn-and-lstm-f7cdf6dfc14e)\n- [零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458)\n- [循环神经网络 – Recurrent Neural Network | RNN](https://easyai.tech/ai-definition/rnn/)"},{"metadata":{"id":"02CBF61D4D44454591909E3F4934F04A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Task04：机器翻译及相关技术；注意力机制与Seq2seq模型；Transformer\n- 机器翻译及相关技术\n- 注意力机制与Seq2seq模型\n- Transformer"},{"metadata":{"id":"97853D045EB240478C7AA8477D966F80","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Part1 机器翻译及相关技术\n- 机器翻译和数据集\n- Encoder-Decoder\n- Sequence to Sequence模型\n- Beam Search"},{"metadata":{"id":"441A377AB12643059AE36625C4E91BD8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 机器翻译和数据集\n**机器翻译（MT）**\n- 机器翻译（MT）是利用机器的力量「自动将一种自然语言（源语言）的文本翻译成另一种语言（目标语言）」。机器翻译方法通常可分成三大类：基于规则的机器翻译（RBMT）、统计机器翻译（SMT）和神经机器翻译（NMT）\n- 将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。 主要特征：输出是单词序列而不是单个单词。 输出序列的长度可能与源序列的长度不同。"},{"metadata":{"id":"93714BD9190241288EAE2FBEE89B9548","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['d2lzh1981', 'fraeng6506', 'd2l9528', 'FashionMNIST2065']"},"transient":{},"execution_count":1}],"source":"import os\nos.listdir('/home/kesci/input/')","execution_count":1},{"metadata":{"id":"9F7D693FCE934348899A2F951DD6C825","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import sys\nsys.path.append('/home/kesci/input/d2l9528/')\nimport collections\nimport d2l\nimport zipfile\nfrom d2l.data.base import Vocab\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils import data\nfrom torch import optim","execution_count":2},{"metadata":{"id":"BAA4E8C0A2A74A38B11A8EF49F114874","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 数据预处理\n将数据集清洗、转化为神经网络的输入minbatch\n- 分词\n- 建立词典\n- 得到数据生成器\n注意：单词转化为词向量是`模型结构`的一部分，`词向量层`一般作为网络的第一层！"},{"metadata":{"id":"C2EFD35346DF4D3D8DC79726CA7A2A4C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\nHi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\nHi.\tSalut.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)\nRun!\tCours !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)\nRun!\tCourez !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)\nWho?\tQui ?\tCC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #4366796 (gillux)\nWow!\tÇa alors !\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #374631 (zmoo)\nFire!\tAu feu !\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #4627939 (sacredceltic)\nHelp!\tÀ l'aide !\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #128430 (sysko)\nJump.\tSaute.\tCC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #2416938 (Phoenix)\nStop!\tÇa suffit !\tCC-BY 2.0 (France) Attribution: tato\n","name":"stdout"}],"source":"with open('/home/kesci/input/fraeng6506/fra.txt', 'r') as f:\n      raw_text = f.read()\nprint(raw_text[0:1000])","execution_count":3},{"metadata":{"id":"C6F987093659499F8DFCE88C931B65D7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"go .\tva !\tcc-by 2 .0 (france) attribution: tatoeba .org #2877272 (cm) & #1158250 (wittydev)\nhi .\tsalut !\tcc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) & #509819 (aiji)\nhi .\tsalut .\tcc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) & #4320462 (gillux)\nrun !\tcours !\tcc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) & #906331 (sacredceltic)\nrun !\tcourez !\tcc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) & #906332 (sacredceltic)\nwho?\tqui ?\tcc-by 2 .0 (france) attribution: tatoeba .org #2083030 (ck) & #4366796 (gillux)\nwow !\tça alors !\tcc-by 2 .0 (france) attribution: tatoeba .org #52027 (zifre) & #374631 (zmoo)\nfire !\tau feu !\tcc-by 2 .0 (france) attribution: tatoeba .org #1829639 (spamster) & #4627939 (sacredceltic)\nhelp !\tà l'aide !\tcc-by 2 .0 (france) attribution: tatoeba .org #435084 (lukaszpp) & #128430 (sysko)\njump .\tsaute .\tcc-by 2 .0 (france) attribution: tatoeba .org #631038 (shishir) & #2416938 (phoenix)\nstop !\tça suffit !\tcc-b\n","name":"stdout"}],"source":"def preprocess_raw(text):\n    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n    out = ''\n    for i, char in enumerate(text.lower()):\n        if char in (',', '!', '.') and i > 0 and text[i-1] != ' ':\n            out += ' '\n        out += char\n    return out\n\ntext = preprocess_raw(raw_text)\nprint(text[0:1000])","execution_count":4},{"metadata":{"id":"D4173FEA545F43C5844A294C7E9BA3A2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"字符在计算机里是以编码的形式存在，我们通常所用的空格是 \\x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。\n而 \\xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。在数据预处理的过程中，我们首先需要对数据进行清洗。"},{"metadata":{"id":"B49925555B7F478080872C8E2D08107F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 分词\n字符串---单词组成的列表"},{"metadata":{"id":"C4B2DA3718D24ED48706A5273873B85C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"([['go', '.'], ['hi', '.'], ['hi', '.']],\n [['va', '!'], ['salut', '!'], ['salut', '.']])"},"transient":{},"execution_count":5}],"source":"num_examples = 50000\nsource, target = [], []\nfor i, line in enumerate(text.split('\\n')):\n    if i > num_examples:\n        break\n    parts = line.split('\\t')\n    if len(parts) >= 2:\n        source.append(parts[0].split(' '))\n        target.append(parts[1].split(' '))\n        \nsource[0:3], target[0:3]","execution_count":5},{"metadata":{"id":"CB8F25927CFB4B248A5C206B5D132264","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/CB8F25927CFB4B248A5C206B5D132264/q5vwpq2j9j.svg\">"},"transient":{}}],"source":"d2l.set_figsize()\nd2l.plt.hist([[len(l) for l in source], [len(l) for l in target]],label=['source', 'target'])\nd2l.plt.legend(loc='upper right');","execution_count":6},{"metadata":{"id":"D221301ECDBB42628FF085BC0EF03D69","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 建立词典\n单词组成的列表---单词id组成的列表"},{"metadata":{"id":"1FDF2A35A96D4D348CDEB039C4D96D0D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"collapsed":false,"scrolled":false},"cell_type":"code","source":"def build_vocab(tokens):\n    tokens = [token for line in tokens for token in line]\n    return d2l.data.base.Vocab(tokens, min_freq=3, use_special_tokens=True)\n\nsrc_vocab = build_vocab(source)\nlen(src_vocab)","execution_count":7,"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"3789"},"transient":{},"execution_count":7}]},{"metadata":{"id":"A20234B9ED7949A09673697057237BBB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 载入数据集"},{"metadata":{"id":"76EAC6EF453A4900AD23AFD2EF70CAC5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"[38, 4, 0, 0, 0, 0, 0, 0, 0, 0]"},"transient":{},"execution_count":8}],"source":"def pad(line, max_len, padding_token):\n    if len(line) > max_len:\n        return line[:max_len]\n    return line + [padding_token] * (max_len - len(line))\npad(src_vocab[source[0]], 10, src_vocab.pad)","execution_count":8},{"metadata":{"id":"C914B8843700434183751A543E8F41D1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def build_array(lines, vocab, max_len, is_source):\n    lines = [vocab[line] for line in lines]\n    if not is_source:\n        lines = [[vocab.bos] + line + [vocab.eos] for line in lines]\n    array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])\n    valid_len = (array != vocab.pad).sum(1) #第一个维度\n    return array, valid_len","execution_count":9},{"metadata":{"id":"90E2EEE976E34AA5BC0ECB62AA194F74","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def load_data_nmt(batch_size, max_len): # This function is saved in d2l.\n    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)\n    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False)\n    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)\n    train_iter = data.DataLoader(train_data, batch_size, shuffle=True)\n    return src_vocab, tgt_vocab, train_iter","execution_count":10},{"metadata":{"id":"BC41C9B2BEE24CA38B61BF24E1EDAA82","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"X = tensor([[   5,   78,   20,  498,    4,    0,    0,    0],\n        [1077,   26,  299,    4,    0,    0,    0,    0]], dtype=torch.int32) \nValid lengths for X = tensor([5, 4]) \nY = tensor([[   1,    5,  100,   22,   25, 1811,    4,    2],\n        [   1,    3,  190,  294,    4,    2,    0,    0]], dtype=torch.int32) \nValid lengths for Y = tensor([8, 6])\n","name":"stdout"}],"source":"src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size=2, max_len=8)\nfor X, X_valid_len, Y, Y_valid_len, in train_iter:\n    print('X =', X.type(torch.int32), '\\nValid lengths for X =', X_valid_len,\n        '\\nY =', Y.type(torch.int32), '\\nValid lengths for Y =', Y_valid_len)\n    break","execution_count":11},{"metadata":{"id":"E7ECDE3EC31849DF88B5339549834F20","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### Encoder-Decoder\n encoder：输入到隐藏状态  \n decoder：隐藏状态到输出\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5jcat3c8m.png?imageView2/0/w/640/h/640)\n\n"},{"metadata":{"id":"857ECC8BDB9146D195E019D59DB5179A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class Encoder(nn.Module):\n    def __init__(self, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n\n    def forward(self, X, *args):\n        raise NotImplementedError","execution_count":12},{"metadata":{"id":"B6B3683610B84E42834DE9768F07F70D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class Decoder(nn.Module):\n    def __init__(self, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n\n    def init_state(self, enc_outputs, *args):\n        raise NotImplementedError\n\n    def forward(self, X, state):\n        raise NotImplementedError","execution_count":13},{"metadata":{"id":"0B1A93C2E7DC47808B0649FE4839D506","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class EncoderDecoder(nn.Module):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(EncoderDecoder, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, enc_X, dec_X, *args):\n        enc_outputs = self.encoder(enc_X, *args)\n        dec_state = self.decoder.init_state(enc_outputs, *args)\n        return self.decoder(dec_X, dec_state)","execution_count":14},{"metadata":{"id":"52484DA5F05C4261BABF1C0EABC2A404","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Sequence to Sequence模型\n\n### 模型：\n训练  \n![Image Name](https://cdn.kesci.com/upload/image/q5jc7a53pt.png?imageView2/0/w/640/h/640)\n预测\n\n![Image Name](https://cdn.kesci.com/upload/image/q5jcecxcba.png?imageView2/0/w/640/h/640)\n\n\n\n### 具体结构：\n![Image Name](https://cdn.kesci.com/upload/image/q5jccjhkii.png?imageView2/0/w/500/h/500)\n### Encoder"},{"metadata":{"id":"D5DE435644D047DE9E6972C81424770C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class Seq2SeqEncoder(d2l.Encoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqEncoder, self).__init__(**kwargs)\n        self.num_hiddens=num_hiddens\n        self.num_layers=num_layers\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)\n   \n    def begin_state(self, batch_size, device):\n        return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),\n                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]\n    def forward(self, X, *args):\n        X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size)\n        X = X.transpose(0, 1)  # RNN needs first axes to be time\n        # state = self.begin_state(X.shape[1], device=X.device)\n        out, state = self.rnn(X)\n        # The shape of out is (seq_len, batch_size, num_hiddens).\n        # state contains the hidden state and the memory cell\n        # of the last time step, the shape is (num_layers, batch_size, num_hiddens)\n        return out, state","execution_count":15},{"metadata":{"id":"E3D4F3A4FD3F4301B5448FEC45544C9A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(torch.Size([7, 4, 16]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))"},"transient":{},"execution_count":16}],"source":"encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)\nX = torch.zeros((4, 7),dtype=torch.long)\noutput, state = encoder(X)\noutput.shape, len(state), state[0].shape, state[1].shape","execution_count":16},{"metadata":{"id":"2B6B893D54864EEB895C6ABC3270791B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Decoder"},{"metadata":{"id":"3C88F1672F014558A38E7C88C2BF43FE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class Seq2SeqDecoder(d2l.Decoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqDecoder, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)\n        self.dense = nn.Linear(num_hiddens,vocab_size)\n\n    def init_state(self, enc_outputs, *args):\n        return enc_outputs[1]\n\n    def forward(self, X, state):\n        X = self.embedding(X).transpose(0, 1)\n        out, state = self.rnn(X, state)\n        # Make the batch to be the first dimension to simplify loss computation.\n        out = self.dense(out).transpose(0, 1)\n        return out, state","execution_count":17},{"metadata":{"id":"7F25718E0FFF4042B9265202A7783AE6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(torch.Size([4, 7, 10]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))"},"transient":{},"execution_count":18}],"source":"decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)\nstate = decoder.init_state(encoder(X))\nout, state = decoder(X, state)\nout.shape, len(state), state[0].shape, state[1].shape","execution_count":18},{"metadata":{"id":"C6AD02B9AACB40C688298B5872D7F37A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 损失函数"},{"metadata":{"id":"D3C268F9DF144A7D8C67AC0405FA58F3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def SequenceMask(X, X_len,value=0):\n    maxlen = X.size(1)\n    mask = torch.arange(maxlen)[None, :].to(X_len.device) < X_len[:, None]   \n    X[~mask]=value\n    return X","execution_count":19},{"metadata":{"id":"B4ED0B9C9EE94D6EB2660003C3CC225D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([[1, 0, 0],\n        [4, 5, 0]])"},"transient":{},"execution_count":20}],"source":"X = torch.tensor([[1,2,3], [4,5,6]])\nSequenceMask(X,torch.tensor([1,2]))","execution_count":20},{"metadata":{"id":"1D2B4A33ACD241DB8105F6969EBC2925","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([[[ 1.,  1.,  1.,  1.],\n         [-1., -1., -1., -1.],\n         [-1., -1., -1., -1.]],\n\n        [[ 1.,  1.,  1.,  1.],\n         [ 1.,  1.,  1.,  1.],\n         [-1., -1., -1., -1.]]])"},"transient":{},"execution_count":21}],"source":"X = torch.ones((2,3, 4))\nSequenceMask(X, torch.tensor([1,2]),value=-1)","execution_count":21},{"metadata":{"id":"766649CF5882481F98BEAB1563357FDF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n    # pred shape: (batch_size, seq_len, vocab_size)\n    # label shape: (batch_size, seq_len)\n    # valid_length shape: (batch_size, )\n    def forward(self, pred, label, valid_length):\n        # the sample weights shape should be (batch_size, seq_len)\n        weights = torch.ones_like(label)\n        weights = SequenceMask(weights, valid_length).float()\n        self.reduction='none'\n        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)\n        return (output*weights).mean(dim=1)","execution_count":22},{"metadata":{"id":"74BF786DE2564F439E2D47B3675BF394","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([2.3026, 1.7269, 0.0000])"},"transient":{},"execution_count":23}],"source":"loss = MaskedSoftmaxCELoss()\nloss(torch.ones((3, 4, 10)), torch.ones((3,4),dtype=torch.long), torch.tensor([4,3,0]))","execution_count":23},{"metadata":{"id":"7CA28CAD99C4409A93D90821563F9FE2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 训练"},{"metadata":{"id":"109C3E357092407B836ADCC2509A01C7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def train_ch7(model, data_iter, lr, num_epochs, device):  # Saved in d2l\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    loss = MaskedSoftmaxCELoss()\n    tic = time.time()\n    for epoch in range(1, num_epochs+1):\n        l_sum, num_tokens_sum = 0.0, 0.0\n        for batch in data_iter:\n            optimizer.zero_grad()\n            X, X_vlen, Y, Y_vlen = [x.to(device) for x in batch]\n            Y_input, Y_label, Y_vlen = Y[:,:-1], Y[:,1:], Y_vlen-1\n            \n            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)\n            l = loss(Y_hat, Y_label, Y_vlen).sum()\n            l.backward()\n\n            with torch.no_grad():\n                d2l.grad_clipping_nn(model, 5, device)\n            num_tokens = Y_vlen.sum().item()\n            optimizer.step()\n            l_sum += l.sum().item()\n            num_tokens_sum += num_tokens\n        if epoch % 50 == 0:\n            print(\"epoch {0:4d},loss {1:.3f}, time {2:.1f} sec\".format( \n                  epoch, (l_sum/num_tokens_sum), time.time()-tic))\n            tic = time.time()","execution_count":24},{"metadata":{"id":"9EE4421D197940A093D03FEECD7279DD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch   50,loss 0.111, time 27.6 sec\nepoch  100,loss 0.053, time 27.4 sec\nepoch  150,loss 0.033, time 27.2 sec\nepoch  200,loss 0.028, time 27.7 sec\nepoch  250,loss 0.025, time 28.1 sec\nepoch  300,loss 0.028, time 27.7 sec\n","name":"stdout"}],"source":"embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0\nbatch_size, num_examples, max_len = 64, 1e3, 10\nlr, num_epochs, ctx = 0.005, 300, d2l.try_gpu()\nsrc_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(\n    batch_size, max_len,num_examples)\nencoder = Seq2SeqEncoder(\n    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\ndecoder = Seq2SeqDecoder(\n    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\nmodel = d2l.EncoderDecoder(encoder, decoder)\ntrain_ch7(model, train_iter, lr, num_epochs, ctx)","execution_count":25},{"metadata":{"id":"7A3BFF590132407C8F7D50C2CD824A1A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 测试"},{"metadata":{"id":"8D81B7416B374EBE80A650F856CB77FF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def translate_ch7(model, src_sentence, src_vocab, tgt_vocab, max_len, device):\n    src_tokens = src_vocab[src_sentence.lower().split(' ')]\n    src_len = len(src_tokens)\n    if src_len < max_len:\n        src_tokens += [src_vocab.pad] * (max_len - src_len)\n    enc_X = torch.tensor(src_tokens, device=device)\n    enc_valid_length = torch.tensor([src_len], device=device)\n    # use expand_dim to add the batch_size dimension.\n    enc_outputs = model.encoder(enc_X.unsqueeze(dim=0), enc_valid_length)\n    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length)\n    dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=0)\n    predict_tokens = []\n    for _ in range(max_len):\n        Y, dec_state = model.decoder(dec_X, dec_state)\n        # The token with highest score is used as the next time step input.\n        dec_X = Y.argmax(dim=2)\n        py = dec_X.squeeze(dim=0).int().item()\n        if py == tgt_vocab.eos:\n            break\n        predict_tokens.append(py)\n    return ' '.join(tgt_vocab.to_tokens(predict_tokens))","execution_count":26},{"metadata":{"id":"D3E1D188581B4475A8243193D76DE382","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Go . => va !\nWow ! => <unk> !\nI'm OK . => je vais bien .\nI won ! => j'ai gagné !\n","name":"stdout"}],"source":"for sentence in ['Go .', 'Wow !', \"I'm OK .\", 'I won !']:\n    print(sentence + ' => ' + translate_ch7(\n        model, sentence, src_vocab, tgt_vocab, max_len, ctx))","execution_count":27},{"metadata":{"id":"2AEAA2440AD34A3F8D3454889FF58D99","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Beam Search\n简单贪心搜索（greedy search）：\n\n![Image Name](https://cdn.kesci.com/upload/image/q5jchqoppn.png?imageView2/0/w/440/h/440)\n\n维特比算法：选择整体分数最高的句子（搜索空间太大）\n集束搜索：\n\n![Image Name](https://cdn.kesci.com/upload/image/q5jcia86z1.png?imageView2/0/w/640/h/640)\n"},{"metadata":{"id":"36DD2F37C09E439AA6EA0F9A0B4168AE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 小结练习\n\n**延伸阅读**\n- [一文看懂 NLP 里的模型框架 Encoder-Decoder 和 Seq2Seq](https://segmentfault.com/a/1190000020843265)\n- [神奇的神经机器翻译：从发展脉络到未来前景（附论文资源）](https://www.jiqizhixin.com/articles/2017-08-22-6)\n\n\n"},{"metadata":{"id":"CA6AA084AD7742D7BBEBBFBF045593D9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Part2 注意力机制与Seq2seq模型\n- 注意力机制的概念和框架\n- 点积注意力机制和多层感知机注意力机制的实现\n- 使用注意力机制的seq2seq模型"},{"metadata":{"id":"58100352F2C7484181D3387A38455C84","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 注意力机制\n我们可以粗略地把神经注意机制类比成一个可以专注于输入内容的某一子集（或特征）的神经网络. 注意力机制最早是由 DeepMind 为图像分类提出的，这让「神经网络在执行预测任务时可以更多关注输入中的相关部分，更少关注不相关的部分」。当解码器生成一个用于构成目标句子的词时，源句子中仅有少部分是相关的；因此，可以应用一个基于内容的注意力机制来根据源句子动态地生成一个（加权的）语境向量（context vector）, 然后网络会根据这个语境向量而不是某个固定长度的向量来预测词。\n在“编码器—解码器（seq2seq）”一节中，解码器在各个时间步依赖相同的背景变量（context vector）来获取输出序列信息。当编码器为循环神经网络时，背景变量来表示它最终时间步的隐藏状态。将源序列输入信息以循环单位状态编码，然后将其传递给解码器以生成目标序列。然而这种结构存在着问题，尤其是RNN机制实际中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。\n与此同时，解码的目标词语可能只与原输入的部分词语有关，而并不是与所有的输入有关。例如，当把“Hello world”翻译成“Bonjour le monde”时，“Hello”映射成“Bonjour”，“world”映射成“monde”。在seq2seq模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。\n\n![Image Name](https://cdn.kesci.com/upload/image/q5km4dwgf9.PNG?imageView2/0/w/960/h/960)\n\n## 注意力机制框架\n\nAttention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。$𝐤_𝑖∈ℝ^{𝑑_𝑘}, 𝐯_𝑖∈ℝ^{𝑑_𝑣}$. Query  $𝐪∈ℝ^{𝑑_𝑞}$ , attention layer得到输出与value的维度一致 $𝐨∈ℝ^{𝑑_𝑣}$. 对于一个query来说，attention layer 会与每一个key计算注意力分数并进行权重的归一化，输出的向量$o$则是value的加权求和，而每个key计算的权重与value一一对应。\n\n为了计算输出，我们首先假设有一个函数$\\alpha$ 用于计算query和key的相似性，然后可以计算所有的 attention scores $a_1, \\ldots, a_n$ by\n\n\n$$\na_i = \\alpha(\\mathbf q, \\mathbf k_i).\n$$\n\n\n我们使用 softmax函数 获得注意力权重：\n\n\n$$\nb_1, \\ldots, b_n = \\textrm{softmax}(a_1, \\ldots, a_n).\n$$\n\n\n最终的输出就是value的加权求和：\n\n\n$$\n\\mathbf o = \\sum_{i=1}^n b_i \\mathbf v_i.\n$$\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5km4ooyu2.PNG?imageView2/0/w/960/h/960)\n\n不同的attetion layer的区别在于score函数的选择，在本节的其余部分，我们将讨论两个常用的注意层 Dot-product Attention 和 Multilayer Perceptron Attention；随后我们将实现一个引入attention的seq2seq模型并在英法翻译语料上进行训练与测试。"},{"metadata":{"id":"024FBE52A2A14673A925570886862503","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import math\nimport torch \nimport torch.nn as nn","execution_count":null},{"metadata":{"id":"655BBD4F7AFC470089842C9492F11266","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import os\ndef file_name_walk(file_dir):\n    for root, dirs, files in os.walk(file_dir):\n#         print(\"root\", root)  # 当前目录路径\n         print(\"dirs\", dirs)  # 当前路径下所有子目录\n         print(\"files\", files)  # 当前路径下所有非目录子文件\n\nfile_name_walk(\"/home/kesci/input/fraeng6506\")","execution_count":null},{"metadata":{"id":"FA60566EDBFB4F4A801ADC6EDC3AE054","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### Softmax屏蔽\n在深入研究实现之前，我们首先介绍softmax操作符的一个屏蔽操作。"},{"metadata":{"id":"C972471F626E486EA080F3FA4142436D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def SequenceMask(X, X_len,value=-1e6):\n    maxlen = X.size(1)\n    #print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\\n',X_len[:, None] )\n    mask = torch.arange((maxlen),dtype=torch.float)[None, :] >= X_len[:, None]   \n    #print(mask)\n    X[mask]=value\n    return X","execution_count":null},{"metadata":{"id":"C6C2743D79174BFD80D83E70ABF28D75","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def masked_softmax(X, valid_length):\n    # X: 3-D tensor, valid_length: 1-D or 2-D tensor\n    softmax = nn.Softmax(dim=-1)\n    if valid_length is None:\n        return softmax(X)\n    else:\n        shape = X.shape\n        if valid_length.dim() == 1:\n            try:\n                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n            except:\n                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n        else:\n            valid_length = valid_length.reshape((-1,))\n        # fill masked elements with a large negative, whose exp is 0\n        X = SequenceMask(X.reshape((-1, shape[-1])), valid_length)\n \n        return softmax(X).reshape(shape)","execution_count":null},{"metadata":{"id":"05AA0A7F45F54A48846837AD8429B5DC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"masked_softmax(torch.rand((2,2,4),dtype=torch.float), torch.FloatTensor([2,3]))","execution_count":null},{"metadata":{"id":"45C3C61E5577423C8226E1F4CEC7539A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"**超出2维矩阵的乘法** \n\n$X$ 和 $Y$ 是维度分别为$(b,n,m)$ 和$(b, m, k)$的张量，进行 $b$ 次二维矩阵乘法后得到 $Z$, 维度为 $(b, n, k)$。\n\n\n$$\n Z[i,:,:] = dot(X[i,:,:], Y[i,:,:])\\qquad for\\ i= 1,…,n\\ .\n$$\n"},{"metadata":{"id":"AE4AA1364C95449087677B997474804A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"torch.bmm(torch.ones((2,1,3), dtype = torch.float), torch.ones((2,3,2), dtype = torch.float))","execution_count":null},{"metadata":{"id":"37F57CAE8DAB419096912427E40611D0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 点积注意力\nThe dot product 假设query和keys有相同的维度, 即 $\\forall i, 𝐪,𝐤_𝑖 ∈ ℝ_𝑑 $. 通过计算query和key转置的乘积来计算attention score,通常还会除去 $\\sqrt{d}$ 减少计算出来的score对维度𝑑的依赖性，如下\n\n\n$$\n𝛼(𝐪,𝐤)=⟨𝐪,𝐤⟩/ \\sqrt{d} \n$$\n\n假设 $ 𝐐∈ℝ^{𝑚×𝑑}$ 有 $m$ 个query，$𝐊∈ℝ^{𝑛×𝑑}$ 有 $n$ 个keys. 我们可以通过矩阵运算的方式计算所有 $mn$ 个score：\n\n\n$$\n𝛼(𝐐,𝐊)=𝐐𝐊^𝑇/\\sqrt{d}\n$$\n \n现在让我们实现这个层，它支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重."},{"metadata":{"id":"AC7DE74ED3A34EE9852F0F2A822B9AC9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# Save to the d2l package.\nclass DotProductAttention(nn.Module): \n    def __init__(self, dropout, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    # query: (batch_size, #queries, d)\n    # key: (batch_size, #kv_pairs, d)\n    # value: (batch_size, #kv_pairs, dim_v)\n    # valid_length: either (batch_size, ) or (batch_size, xx)\n    def forward(self, query, key, value, valid_length=None):\n        d = query.shape[-1]\n        # set transpose_b=True to swap the last two dimensions of key\n        \n        scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)\n        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n        print(\"attention_weight\\n\",attention_weights)\n        return torch.bmm(attention_weights, value)","execution_count":null},{"metadata":{"id":"3D7502E0501A4DF4BDD819C99BC6DFA7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 测试\n现在我们创建了两个批，每个批有一个query和10个key-values对。我们通过valid_length指定，对于第一批，我们只关注前2个键-值对，而对于第二批，我们将检查前6个键-值对。因此，尽管这两个批处理具有相同的查询和键值对，但我们获得的输出是不同的。"},{"metadata":{"id":"F2CF11EBEA7B4BDF9749D300CF09E4D7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"atten = DotProductAttention(dropout=0)\n\nkeys = torch.ones((2,10,2),dtype=torch.float)\nvalues = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)\natten(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))","execution_count":null},{"metadata":{"id":"E49E329628704A4AB3F317F6C13C4DA9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 多层感知机注意力\n在多层感知器中，我们首先将 query and keys 投影到  $ℝ^ℎ$ .为了更具体，我们将可以学习的参数做如下映射 \n$𝐖_𝑘∈ℝ^{ℎ×𝑑_𝑘}$ ,  $𝐖_𝑞∈ℝ^{ℎ×𝑑_𝑞}$ , and  $𝐯∈ℝ^h$ . 将score函数定义\n$$\n𝛼(𝐤,𝐪)=𝐯^𝑇tanh(𝐖_𝑘𝐤+𝐖_𝑞𝐪)\n$$\n. \n然后将key 和 value 在特征的维度上合并（concatenate），然后送至 a single hidden layer perceptron 这层中 hidden layer 为  ℎ  and 输出的size为 1 .隐层激活函数为tanh，无偏置."},{"metadata":{"id":"43F4479315E549CC87556266EF72087F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# Save to the d2l package.\nclass MLPAttention(nn.Module):  \n    def __init__(self, units,ipt_dim,dropout, **kwargs):\n        super(MLPAttention, self).__init__(**kwargs)\n        # Use flatten=True to keep query's and key's 3-D shapes.\n        self.W_k = nn.Linear(ipt_dim, units, bias=False)\n        self.W_q = nn.Linear(ipt_dim, units, bias=False)\n        self.v = nn.Linear(units, 1, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query, key, value, valid_length):\n        query, key = self.W_k(query), self.W_q(key)\n        #print(\"size\",query.size(),key.size())\n        # expand query to (batch_size, #querys, 1, units), and key to\n        # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.\n        features = query.unsqueeze(2) + key.unsqueeze(1)\n        #print(\"features:\",features.size())  #--------------开启\n        scores = self.v(features).squeeze(-1) \n        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n        return torch.bmm(attention_weights, value)","execution_count":null},{"metadata":{"id":"F5C11698B18C40BC882E4A17E6922DBA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 测试\n尽管MLPAttention包含一个额外的MLP模型，但如果给定相同的输入和相同的键，我们将获得与DotProductAttention相同的输出"},{"metadata":{"id":"F3C3CC8579C34B12832FDAF50198BA0F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"atten = MLPAttention(ipt_dim=2,units = 8, dropout=0)\natten(torch.ones((2,1,2), dtype = torch.float), keys, values, torch.FloatTensor([2, 6]))","execution_count":null},{"metadata":{"id":"3678D7D5AF62495B8BDDA02582DE431D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 总结\n\n- 注意力层显式地选择相关的信息。\n- 注意层的内存由键-值对组成，因此它的输出接近于键类似于查询的值。"},{"metadata":{"id":"E7C6298E39154EF2BF8C09DCAC80DB48","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 引入注意力机制的Seq2seq模型\n\n本节中将注意机制添加到sequence to sequence 模型中，以显式地使用权重聚合states。下图展示encoding 和decoding的模型结构，在时间步为t的时候。此刻attention layer保存着encodering看到的所有信息——即encoding的每一步输出。在decoding阶段，解码器的$t$时刻的隐藏状态被当作query，encoder的每个时间步的hidden states作为key和value进行attention聚合. Attetion model的输出当作成上下文信息context vector，并与解码器输入$D_t$拼接起来一起送到解码器：\n\n![Image Name](https://cdn.kesci.com/upload/image/q5km7o8z93.PNG?imageView2/0/w/800/h/800)\n\n$$\nFig1具有注意机制的seq-to-seq模型解码的第二步\n$$\n\n\n下图展示了seq2seq机制的所有层的关系，下面展示了encoder和decoder的layer结构\n\n![Image Name](https://cdn.kesci.com/upload/image/q5km8dihlr.PNG?imageView2/0/w/800/h/800)\n\n$$\nFig2具有注意机制的seq-to-seq模型中层结构\n$$\n"},{"metadata":{"id":"E10D049CD6714B5B8F1E24B2FA27A2D4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import sys\nsys.path.append('/home/kesci/input/d2len9900')\nimport d2l","execution_count":null},{"metadata":{"id":"0BC045384C444451BEEB4752E1E883D9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 解码器\n   由于带有注意机制的seq2seq的编码器与之前章节中的Seq2SeqEncoder相同，所以在此处我们只关注解码器。我们添加了一个MLP注意层(MLPAttention)，它的隐藏大小与解码器中的LSTM层相同。然后我们通过从编码器传递三个参数来初始化解码器的状态:\n   \n- the encoder outputs of all timesteps：encoder输出的各个状态，被用于attetion layer的memory部分，有相同的key和values\n\n\n- the hidden state of the encoder’s final timestep：编码器最后一个时间步的隐藏状态，被用于初始化decoder 的hidden state\n\n\n- the encoder valid length: 编码器的有效长度，借此，注意层不会考虑编码器输出中的填充标记（Paddings）\n\n\n   在解码的每个时间步，我们使用解码器的最后一个RNN层的输出作为注意层的query。然后，将注意力模型的输出与输入嵌入向量连接起来，输入到RNN层。虽然RNN层隐藏状态也包含来自解码器的历史信息，但是attention model的输出显式地选择了enc_valid_len以内的编码器输出，这样attention机制就会尽可能排除其他不相关的信息。"},{"metadata":{"id":"2A24A7DFF27949E384ED60B2E1D1D04E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class Seq2SeqAttentionDecoder(d2l.Decoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n        self.attention_cell = MLPAttention(num_hiddens,num_hiddens, dropout)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.LSTM(embed_size+ num_hiddens,num_hiddens, num_layers, dropout=dropout)\n        self.dense = nn.Linear(num_hiddens,vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_len, *args):\n        outputs, hidden_state = enc_outputs\n#         print(\"first:\",outputs.size(),hidden_state[0].size(),hidden_state[1].size())\n        # Transpose outputs to (batch_size, seq_len, hidden_size)\n        return (outputs.permute(1,0,-1), hidden_state, enc_valid_len)\n        #outputs.swapaxes(0, 1)\n        \n    def forward(self, X, state):\n        enc_outputs, hidden_state, enc_valid_len = state\n        #(\"X.size\",X.size())\n        X = self.embedding(X).transpose(0,1)\n#         print(\"Xembeding.size2\",X.size())\n        outputs = []\n        for l, x in enumerate(X):\n#             print(f\"\\n{l}-th token\")\n#             print(\"x.first.size()\",x.size())\n            # query shape: (batch_size, 1, hidden_size)\n            # select hidden state of the last rnn layer as query\n            query = hidden_state[0][-1].unsqueeze(1) # np.expand_dims(hidden_state[0][-1], axis=1)\n            # context has same shape as query\n#             print(\"query enc_outputs, enc_outputs:\\n\",query.size(), enc_outputs.size(), enc_outputs.size())\n            context = self.attention_cell(query, enc_outputs, enc_outputs, enc_valid_len)\n            # Concatenate on the feature dimension\n#             print(\"context.size:\",context.size())\n            x = torch.cat((context, x.unsqueeze(1)), dim=-1)\n            # Reshape x to (1, batch_size, embed_size+hidden_size)\n#             print(\"rnn\",x.size(), len(hidden_state))\n            out, hidden_state = self.rnn(x.transpose(0,1), hidden_state)\n            outputs.append(out)\n        outputs = self.dense(torch.cat(outputs, dim=0))\n        return outputs.transpose(0, 1), [enc_outputs, hidden_state,\n                                        enc_valid_len]","execution_count":null},{"metadata":{"id":"6FFBBCA4F3C746A59FC173D4062DB252","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"现在我们可以用注意力模型来测试seq2seq。为了与第9.7节中的模型保持一致，我们对vocab_size、embed_size、num_hiddens和num_layers使用相同的超参数。结果，我们得到了相同的解码器输出形状，但是状态结构改变了。"},{"metadata":{"id":"8C10E844DA504D949187DA01CD32CC38","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8,\n                            num_hiddens=16, num_layers=2)\n# encoder.initialize()\ndecoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8,\n                                  num_hiddens=16, num_layers=2)\nX = torch.zeros((4, 7),dtype=torch.long)\nprint(\"batch size=4\\nseq_length=7\\nhidden dim=16\\nnum_layers=2\\n\")\nprint('encoder output size:', encoder(X)[0].size())\nprint('encoder hidden size:', encoder(X)[1][0].size())\nprint('encoder memory size:', encoder(X)[1][1].size())\nstate = decoder.init_state(encoder(X), None)\nout, state = decoder(X, state)\nout.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape","execution_count":null},{"metadata":{"id":"F368CEB72739432AA70CB9EC56C9E4F0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 训练\n\n  与第9.7.4节相似，通过应用相同的训练超参数和相同的训练损失来尝试一个简单的娱乐模型。从结果中我们可以看出，由于训练数据集中的序列相对较短，额外的注意层并没有带来显著的改进。由于编码器和解码器的注意层的计算开销，该模型比没有注意的seq2seq模型慢得多。"},{"metadata":{"id":"2F5D84C5B7384987A1FFDA5E75852A81","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import zipfile\nimport torch\nimport requests\nfrom io import BytesIO\nfrom torch.utils import data\nimport sys\nimport collections\n\nclass Vocab(object): # This class is saved in d2l.\n  def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n    # sort by frequency and token\n    counter = collections.Counter(tokens)\n    token_freqs = sorted(counter.items(), key=lambda x: x[0])\n    token_freqs.sort(key=lambda x: x[1], reverse=True)\n    if use_special_tokens:\n      # padding, begin of sentence, end of sentence, unknown\n      self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n      tokens = ['', '', '', '']\n    else:\n      self.unk = 0\n      tokens = ['']\n    tokens += [token for token, freq in token_freqs if freq >= min_freq]\n    self.idx_to_token = []\n    self.token_to_idx = dict()\n    for token in tokens:\n      self.idx_to_token.append(token)\n      self.token_to_idx[token] = len(self.idx_to_token) - 1\n      \n  def __len__(self):\n    return len(self.idx_to_token)\n  \n  def __getitem__(self, tokens):\n    if not isinstance(tokens, (list, tuple)):\n      return self.token_to_idx.get(tokens, self.unk)\n    else:\n      return [self.__getitem__(token) for token in tokens]\n    \n  def to_tokens(self, indices):\n    if not isinstance(indices, (list, tuple)):\n      return self.idx_to_token[indices]\n    else:\n      return [self.idx_to_token[index] for index in indices]\n\ndef load_data_nmt(batch_size, max_len, num_examples=1000):\n    \"\"\"Download an NMT dataset, return its vocabulary and data iterator.\"\"\"\n    # Download and preprocess\n    def preprocess_raw(text):\n        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n        out = ''\n        for i, char in enumerate(text.lower()):\n            if char in (',', '!', '.') and text[i-1] != ' ':\n                out += ' '\n            out += char\n        return out \n\n\n    with open('/home/kesci/input/fraeng6506/fra.txt', 'r') as f:\n      raw_text = f.read()\n\n\n    text = preprocess_raw(raw_text)\n\n    # Tokenize\n    source, target = [], []\n    for i, line in enumerate(text.split('\\n')):\n        if i >= num_examples:\n            break\n        parts = line.split('\\t')\n        if len(parts) >= 2:\n            source.append(parts[0].split(' '))\n            target.append(parts[1].split(' '))\n\n    # Build vocab\n    def build_vocab(tokens):\n        tokens = [token for line in tokens for token in line]\n        return Vocab(tokens, min_freq=3, use_special_tokens=True)\n    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n\n    # Convert to index arrays\n    def pad(line, max_len, padding_token):\n        if len(line) > max_len:\n            return line[:max_len]\n        return line + [padding_token] * (max_len - len(line))\n\n    def build_array(lines, vocab, max_len, is_source):\n        lines = [vocab[line] for line in lines]\n        if not is_source:\n            lines = [[vocab.bos] + line + [vocab.eos] for line in lines]\n        array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])\n        valid_len = (array != vocab.pad).sum(1)\n        return array, valid_len\n\n    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)\n    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False)\n    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)\n    train_iter = data.DataLoader(train_data, batch_size, shuffle=True)\n    return src_vocab, tgt_vocab, train_iter","execution_count":null},{"metadata":{"id":"D8919DBF4F4147B98DAB65C49765E2EB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0\nbatch_size, num_steps = 64, 10\nlr, num_epochs, ctx = 0.005, 500, d2l.try_gpu()\n\nsrc_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size, num_steps)\nencoder = d2l.Seq2SeqEncoder(\n    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\ndecoder = Seq2SeqAttentionDecoder(\n    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\nmodel = d2l.EncoderDecoder(encoder, decoder)","execution_count":null},{"metadata":{"id":"F787324C075F4D9E8FB306872A574CF7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 训练\nd2l.train_s2s_ch9(model, train_iter, lr, num_epochs, ctx)","execution_count":null},{"metadata":{"id":"D06D5FD431DF4A0EAA79E4EF5C39A8C3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 预测\nfor sentence in ['Go .', 'Good Night !', \"I'm OK .\", 'I won !']:\n    print(sentence + ' => ' + d2l.predict_s2s_ch9(\n        model, sentence, src_vocab, tgt_vocab, num_steps, ctx))","execution_count":null},{"metadata":{"id":"4D16ADDEA3ED417B9B2E8547994744F4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 小结练习\n**注意力机制**\n- 注意力机制本身有高效的并行性，但引入注意力并不能改变seq2seq内部RNN的迭代机制，也无法加速模型的训练。\n\n**延伸阅读**\n- [注意力机制](https://www.jiqizhixin.com/graph/technologies/60bee267-89dc-4606-ba24-6b3f7d1f6189)\n- [Deep Reading | 从0到1再读注意力机制，此文必收藏！](https://www.tinymind.cn/articles/4065)\n"},{"metadata":{"id":"A67CFF791FC54E81A6DE0EAFB567F26A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Part3 Transformer\n- Transformer的概念以及相对于与传统CNN、RNN的优势 \n- Transformer的实现\n- Multi-head Attention、FFN、AddNorm\n- Encoder、Decoder的代码实现"},{"metadata":{"id":"5CEA715CF0FC49268A344657C0959A4C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Transformer\n\n在之前的章节中，我们已经介绍了主流的神经网络架构如卷积神经网络（CNNs）和循环神经网络（RNNs）。让我们进行一些回顾：\n\n- CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。\n- RNNs 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。\n\n为了整合CNN和RNN的优势，[\\[Vaswani et al., 2017\\]](https://d2l.ai/chapter_references/zreferences.html#vaswani-shazeer-parmar-ea-2017) 创新性地使用注意力机制设计了Transformer模型。该模型利用attention机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的tokens，上述优势使得Transformer模型在性能优异的同时大大减少了训练时间。\n\n图10.3.1展示了Transformer模型的架构，与9.7节的seq2seq模型相似，Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：\n1. Transformer blocks：将seq2seq模型中的`循环网络`替换为了`Transformer Blocks`，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。\n2. Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含`残差结构`以及`层归一化`。\n3. Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。\n\n![Fig. 10.3.1 The Transformer architecture.](https://cdn.kesci.com/upload/image/q5kpbj2cj5.png?imageView2/0/w/960/h/960)\n\n$$\nFig.10.3.1\\ Transformer 架构.\n$$\n\n\n在接下来的部分，我们将会带领大家实现Transformer里全新的子结构，并且构建一个神经机器翻译模型用以训练和测试。"},{"metadata":{"id":"6FD5D6A5DAEB4DFEAB6FF0DB51E0A28D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import os\nimport math\nimport numpy as np\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\nsys.path.append('/home/kesci/input/d2len9900')\nimport d2l","execution_count":null},{"metadata":{"id":"49A469F06D004BF18B5A3DC351D86389","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# masked softmax 实现\ndef SequenceMask(X, X_len,value=-1e6):\n    maxlen = X.size(1)\n    X_len = X_len.to(X.device)\n    #print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\\n',X_len[:, None] )\n    mask = torch.arange((maxlen), dtype=torch.float, device=X.device)\n    mask = mask[None, :] < X_len[:, None]\n    #print(mask)\n    X[~mask]=value\n    return X\n\ndef masked_softmax(X, valid_length):\n    # X: 3-D tensor, valid_length: 1-D or 2-D tensor\n    softmax = nn.Softmax(dim=-1)\n    if valid_length is None:\n        return softmax(X)\n    else:\n        shape = X.shape\n        if valid_length.dim() == 1:\n            try:\n                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n            except:\n                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n        else:\n            valid_length = valid_length.reshape((-1,))\n        # fill masked elements with a large negative, whose exp is 0\n        X = SequenceMask(X.reshape((-1, shape[-1])), valid_length)\n \n        return softmax(X).reshape(shape)\n\n# Save to the d2l package.\nclass DotProductAttention(nn.Module): \n    def __init__(self, dropout, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    # query: (batch_size, #queries, d)\n    # key: (batch_size, #kv_pairs, d)\n    # value: (batch_size, #kv_pairs, dim_v)\n    # valid_length: either (batch_size, ) or (batch_size, xx)\n    def forward(self, query, key, value, valid_length=None):\n        d = query.shape[-1]\n        # set transpose_b=True to swap the last two dimensions of key\n        scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)\n        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n        return torch.bmm(attention_weights, value)","execution_count":null},{"metadata":{"id":"9DAED5B741B246138CED140667948E95","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 多头注意力层\n\n在我们讨论多头注意力层之前，先来迅速理解一下自注意力（self-attention）的结构。自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。如图10.3.2 自注意力输出了一个与输入长度相同的表征序列，与循环神经网络相比，自注意力对每个元素输出的计算是并行的，所以我们可以高效的实现这个模块。\n\n![Fig. 10.3.2 自注意力结构](https://cdn.kesci.com/upload/image/q5kpckv38q.png?imageView2/0/w/320/h/320)\n\n$$\nFig.10.3.2\\ 自注意力结构\n$$\n\n\n多头注意力层包含$h$个并行的自注意力层，每一个这种层被成为一个head。对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个现行层进行映射，这$h$个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。\n\n![Image Name](https://cdn.kesci.com/upload/image/q5kpcsozid.png?imageView2/0/w/640/h/640)\n\n$$\nFig.10.3.3\\ 多头注意力\n$$\n\n\n假设query，key和value的维度分别是$d_q$、$d_k$和$d_v$。那么对于每一个头$i=1,\\ldots,h$，我们可以训练相应的模型权重$W_q^{(i)} \\in \\mathbb{R}^{p_q\\times d_q}$、$W_k^{(i)} \\in \\mathbb{R}^{p_k\\times d_k}$和$W_v^{(i)} \\in \\mathbb{R}^{p_v\\times d_v}$，以得到每个头的输出：\n\n\n$$\no^{(i)} = attention(W_q^{(i)}q, W_k^{(i)}k, W_v^{(i)}v)\n$$\n\n\n这里的attention可以是任意的attention function，比如前一节介绍的dot-product attention以及MLP attention。之后我们将所有head对应的输出拼接起来，送入最后一个线性层进行整合，这个层的权重可以表示为$W_o\\in \\mathbb{R}^{d_0 \\times hp_v}$\n\n\n$$\no = W_o[o^{(1)}, \\ldots, o^{(h)}]\n$$\n\n\n接下来我们就可以来实现多头注意力了，假设我们有$h$个头，隐藏层权重 $hidden\\_size = p_q = p_k = p_v$ 与query，key，value的维度一致。除此之外，因为多头注意力层保持输入与输出张量的维度不变，所以输出feature的维度也设置为 $d_0 = hidden\\_size$。"},{"metadata":{"id":"5CA48F5D5CB74A1D9D03A14C74F729E5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, input_size, hidden_size, num_heads, dropout, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.attention = DotProductAttention(dropout)\n        self.W_q = nn.Linear(input_size, hidden_size, bias=False)\n        self.W_k = nn.Linear(input_size, hidden_size, bias=False)\n        self.W_v = nn.Linear(input_size, hidden_size, bias=False)\n        self.W_o = nn.Linear(hidden_size, hidden_size, bias=False)\n    \n    def forward(self, query, key, value, valid_length):\n        # query, key, and value shape: (batch_size, seq_len, dim),\n        # where seq_len is the length of input sequence\n        # valid_length shape is either (batch_size, )\n        # or (batch_size, seq_len).\n\n        # Project and transpose query, key, and value from\n        # (batch_size, seq_len, hidden_size * num_heads) to\n        # (batch_size * num_heads, seq_len, hidden_size).\n        \n        query = transpose_qkv(self.W_q(query), self.num_heads)\n        key = transpose_qkv(self.W_k(key), self.num_heads)\n        value = transpose_qkv(self.W_v(value), self.num_heads)\n        \n        if valid_length is not None:\n            # Copy valid_length by num_heads times\n            device = valid_length.device\n            valid_length = valid_length.cpu().numpy() if valid_length.is_cuda else valid_length.numpy()\n            if valid_length.ndim == 1:\n                valid_length = torch.FloatTensor(np.tile(valid_length, self.num_heads))\n            else:\n                valid_length = torch.FloatTensor(np.tile(valid_length, (self.num_heads,1)))\n\n            valid_length = valid_length.to(device)\n            \n        output = self.attention(query, key, value, valid_length)\n        output_concat = transpose_output(output, self.num_heads)\n        return self.W_o(output_concat)","execution_count":null},{"metadata":{"id":"2154E20132404F3FBBC0D3EE7EF1F8DC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def transpose_qkv(X, num_heads):\n    # Original X shape: (batch_size, seq_len, hidden_size * num_heads),\n    # -1 means inferring its value, after first reshape, X shape:\n    # (batch_size, seq_len, num_heads, hidden_size)\n    X = X.view(X.shape[0], X.shape[1], num_heads, -1)\n    \n    # After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size)\n    X = X.transpose(2, 1).contiguous()\n\n    # Merge the first two dimensions. Use reverse=True to infer shape from\n    # right to left.\n    # output shape: (batch_size * num_heads, seq_len, hidden_size)\n    output = X.view(-1, X.shape[2], X.shape[3])\n    return output\n\n\n# Saved in the d2l package for later use\ndef transpose_output(X, num_heads):\n    # A reversed version of transpose_qkv\n    X = X.view(-1, num_heads, X.shape[1], X.shape[2])\n    X = X.transpose(2, 1).contiguous()\n    return X.view(X.shape[0], X.shape[1], -1)","execution_count":null},{"metadata":{"id":"8C23380A05C74BEBA7456A1B5978D999","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"cell = MultiHeadAttention(5, 9, 3, 0.5)\nX = torch.ones((2, 4, 5))\nvalid_length = torch.FloatTensor([2, 3])\ncell(X, X, X, valid_length).shape","execution_count":null},{"metadata":{"id":"D06951E457AB482B80B4B01A6FA7EAAB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 基于位置的前馈网络\n\nTransformer 模块另一个非常重要的部分就是基于位置的前馈网络（FFN），它接受一个形状为（batch_size，seq_length, feature_size）的三维张量。Position-wise FFN由两个全连接层组成，他们作用在最后一维上。因为序列的每个位置的状态都会被单独地更新，所以我们称他为position-wise，这等效于一个1x1的卷积。\n\n下面我们来实现PositionWiseFFN："},{"metadata":{"id":"F3E429944375425081DA508162B11BCD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# Save to the d2l package.\nclass PositionWiseFFN(nn.Module):\n    def __init__(self, input_size, ffn_hidden_size, hidden_size_out, **kwargs):\n        super(PositionWiseFFN, self).__init__(**kwargs)\n        self.ffn_1 = nn.Linear(input_size, ffn_hidden_size)\n        self.ffn_2 = nn.Linear(ffn_hidden_size, hidden_size_out)\n        \n        \n    def forward(self, X):\n        return self.ffn_2(F.relu(self.ffn_1(X)))","execution_count":null},{"metadata":{"id":"D3360A7C1F524E9F8A6058DBBD7288B7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"与多头注意力层相似，FFN层同样只会对最后一维的大小进行改变；除此之外，对于两个完全相同的输入，FFN层的输出也将相等。"},{"metadata":{"id":"330C568E73014F15B6D2B0C4D3538980","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"ffn = PositionWiseFFN(4, 4, 8)\nout = ffn(torch.ones((2,3,4)))\n\nprint(out, out.shape)","execution_count":null},{"metadata":{"id":"5E91B9A9FEE8488D839C94744BC83353","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Add and Norm\n\n除了上面两个模块之外，Transformer还有一个重要的相加归一化层，它可以平滑地整合输入和其他层的输出，因此我们在每个多头注意力层和FFN层后面都添加一个含残差连接的Layer Norm层。这里 Layer Norm 与7.5小节的Batch Norm很相似，唯一的区别在于Batch Norm是对于batch size这个维度进行计算均值和方差的，而Layer Norm则是对最后一维进行计算。层归一化可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能。 [(ref)](https://zhuanlan.zhihu.com/p/54530247)"},{"metadata":{"id":"2D06DE2D70414FAF9D557C7370924946","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"layernorm = nn.LayerNorm(normalized_shape=2, elementwise_affine=True)\nbatchnorm = nn.BatchNorm1d(num_features=2, affine=True)\nX = torch.FloatTensor([[1,2], [3,4]])\nprint('layer norm:', layernorm(X))\nprint('batch norm:', batchnorm(X))","execution_count":null},{"metadata":{"id":"991799544B98495F927829B116F980A4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# Save to the d2l package.\nclass AddNorm(nn.Module):\n    def __init__(self, hidden_size, dropout, **kwargs):\n        super(AddNorm, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.norm = nn.LayerNorm(hidden_size)\n    \n    def forward(self, X, Y):\n        return self.norm(self.dropout(Y) + X)","execution_count":null},{"metadata":{"id":"55F4D97458164FDC8AAF9C41C2FF5ADF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 由于残差连接，X和Y需要有相同的维度\nadd_norm = AddNorm(4, 0.5)\nadd_norm(torch.ones((2,3,4)), torch.ones((2,3,4))).shape","execution_count":null},{"metadata":{"id":"7E385CEF9D1D4B218C3572444DE9E509","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 位置编码\n\n与循环神经网络不同，无论是多头注意力网络还是前馈神经网络都是独立地对每个位置的元素进行更新，这种特性帮助我们实现了高效的并行，却丢失了重要的序列顺序的信息。为了更好的捕捉序列信息，Transformer模型引入了位置编码去保持输入序列元素的位置。\n\n假设输入序列的嵌入表示 $X\\in \\mathbb{R}^{l\\times d}$, 序列长度为$l$嵌入向量维度为$d$，则其位置编码为$P \\in \\mathbb{R}^{l\\times d}$ ，输出的向量就是二者相加 $X + P$。\n\n位置编码是一个二维的矩阵，i对应着序列中的顺序，j对应其embedding vector内部的维度索引。我们可以通过以下等式计算位置编码：\n\n$$\nP_{i,2j} = sin(i/10000^{2j/d})\n$$\n\n\n$$\nP_{i,2j+1} = cos(i/10000^{2j/d})\n$$\n\n$$\nfor\\ i=0,\\ldots, l-1\\ and\\ j=0,\\ldots,\\lfloor (d-1)/2 \\rfloor\n$$\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5kpe0lu38.png?imageView2/0/w/640/h/640)\n\n$$\nFig. 10.3.4\\ 位置编码\n$$\n"},{"metadata":{"id":"0C6F63C02BBF453ABF76551B16B18E94","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class PositionalEncoding(nn.Module):\n    def __init__(self, embedding_size, dropout, max_len=1000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.P = np.zeros((1, max_len, embedding_size))\n        X = np.arange(0, max_len).reshape(-1, 1) / np.power(\n            10000, np.arange(0, embedding_size, 2)/embedding_size)\n        self.P[:, :, 0::2] = np.sin(X)\n        self.P[:, :, 1::2] = np.cos(X)\n        self.P = torch.FloatTensor(self.P)\n    \n    def forward(self, X):\n        if X.is_cuda and not self.P.is_cuda:\n            self.P = self.P.cuda()\n        X = X + self.P[:, :X.shape[1], :]\n        return self.dropout(X)","execution_count":null},{"metadata":{"id":"0AD12645870E453DA29C10639A32B213","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 测试\n\n下面我们用PositionalEncoding这个类进行一个小测试，取其中的四个维度进行可视化。 我们可以看到，第4维和第5维有相同的频率但偏置不同。第6维和第7维具有更低的频率；因此positional encoding对于不同维度具有可区分性。"},{"metadata":{"id":"49F1C07AAFB34AAFB682065D2CE681C1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import numpy as np\npe = PositionalEncoding(20, 0)\nY = pe(torch.zeros((1, 100, 20))).numpy()\nd2l.plot(np.arange(100), Y[0, :, 4:8].T, figsize=(6, 2.5),\n         legend=[\"dim %d\" % p for p in [4, 5, 6, 7]])","execution_count":null},{"metadata":{"id":"9B733CC1557C4BB881CFEEA667E1E342","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 编码器\n\n我们已经有了组成Transformer的各个模块，现在我们可以开始搭建了！编码器包含一个多头注意力层，一个position-wise FFN，和两个 Add and Norm层。对于attention模型以及FFN模型，我们的输出维度都是与embedding维度一致的，这也是由于残差连接天生的特性导致的，因为我们要将前一层的输出与原始输入相加并归一化。"},{"metadata":{"id":"5E44DC286C4B4B8088405F9481DC4038","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class EncoderBlock(nn.Module):\n    def __init__(self, embedding_size, ffn_hidden_size, num_heads,\n                 dropout, **kwargs):\n        super(EncoderBlock, self).__init__(**kwargs)\n        self.attention = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)\n        self.addnorm_1 = AddNorm(embedding_size, dropout)\n        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)\n        self.addnorm_2 = AddNorm(embedding_size, dropout)\n\n    def forward(self, X, valid_length):\n        Y = self.addnorm_1(X, self.attention(X, X, X, valid_length))\n        return self.addnorm_2(Y, self.ffn(Y))","execution_count":null},{"metadata":{"id":"AD614278F61C4BFF8C2CBB4A25766211","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# batch_size = 2, seq_len = 100, embedding_size = 24\n# ffn_hidden_size = 48, num_head = 8, dropout = 0.5\n\nX = torch.ones((2, 100, 24))\nencoder_blk = EncoderBlock(24, 48, 8, 0.5)\nencoder_blk(X, valid_length).shape","execution_count":null},{"metadata":{"id":"2A2FC63F0C8A4C308D2B961E04CD6D31","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"现在我们来实现整个Transformer 编码器模型，整个编码器由n个刚刚定义的Encoder Block堆叠而成，因为残差连接的缘故，中间状态的维度始终与嵌入向量的维度d一致；同时注意到我们把嵌入向量乘以 $\\sqrt{d}$ 以防止其值过小。"},{"metadata":{"id":"F058485339544958B71705376FED438A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class TransformerEncoder(d2l.Encoder):\n    def __init__(self, vocab_size, embedding_size, ffn_hidden_size,\n                 num_heads, num_layers, dropout, **kwargs):\n        super(TransformerEncoder, self).__init__(**kwargs)\n        self.embedding_size = embedding_size\n        self.embed = nn.Embedding(vocab_size, embedding_size)\n        self.pos_encoding = PositionalEncoding(embedding_size, dropout)\n        self.blks = nn.ModuleList()\n        for i in range(num_layers):\n            self.blks.append(\n                EncoderBlock(embedding_size, ffn_hidden_size,\n                             num_heads, dropout))\n\n    def forward(self, X, valid_length, *args):\n        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))\n        for blk in self.blks:\n            X = blk(X, valid_length)\n        return X","execution_count":null},{"metadata":{"id":"AB6B2EFD8BBA4BEE99996C4742D9E3D9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# test encoder\nencoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)\nencoder(torch.ones((2, 100)).long(), valid_length).shape","execution_count":null},{"metadata":{"id":"A8D2D71FFCDD46F68662BA6570002FE3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 解码器\nTransformer 模型的解码器与编码器结构类似，然而，除了之前介绍的几个模块之外，编码器部分有另一个子模块。该模块也是多头注意力层，接受编码器的输出作为key和value，decoder的状态作为query。与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和层归一化将各个子层的输出相连。\n\n仔细来讲，在第t个时间步，当前输入$x_t$是query，那么self attention接受了第t步以及前t-1步的所有输入$x_1,\\ldots, x_{t-1}$。在训练时，由于第t位置的输入可以观测到全部的序列，这与预测阶段的情形项矛盾，所以我们要通过将第t个时间步所对应的可观测长度设置为t，以消除不需要看到的未来的信息。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5kpefhcyg.png?imageView2/0/w/800/h/800)\n"},{"metadata":{"id":"38955721D15A41B6B4F3FB47FE5484CE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class DecoderBlock(nn.Module):\n    def __init__(self, embedding_size, ffn_hidden_size, num_heads,dropout,i,**kwargs):\n        super(DecoderBlock, self).__init__(**kwargs)\n        self.i = i\n        self.attention_1 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)\n        self.addnorm_1 = AddNorm(embedding_size, dropout)\n        self.attention_2 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)\n        self.addnorm_2 = AddNorm(embedding_size, dropout)\n        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)\n        self.addnorm_3 = AddNorm(embedding_size, dropout)\n    \n    def forward(self, X, state):\n        enc_outputs, enc_valid_length = state[0], state[1]\n        \n        # state[2][self.i] stores all the previous t-1 query state of layer-i\n        # len(state[2]) = num_layers\n        \n        # If training:\n        #     state[2] is useless.\n        # If predicting:\n        #     In the t-th timestep:\n        #         state[2][self.i].shape = (batch_size, t-1, hidden_size)\n        # Demo:\n        # love dogs ! [EOS]\n        #  |    |   |   |\n        #   Transformer \n        #    Decoder\n        #  |   |   |   |\n        #  I love dogs !\n        \n        if state[2][self.i] is None:\n            key_values = X\n        else:\n            # shape of key_values = (batch_size, t, hidden_size)\n            key_values = torch.cat((state[2][self.i], X), dim=1) \n        state[2][self.i] = key_values\n        \n        if self.training:\n            batch_size, seq_len, _ = X.shape\n            # Shape: (batch_size, seq_len), the values in the j-th column are j+1\n            valid_length = torch.FloatTensor(np.tile(np.arange(1, seq_len+1), (batch_size, 1))) \n            valid_length = valid_length.to(X.device)\n        else:\n            valid_length = None\n\n        X2 = self.attention_1(X, key_values, key_values, valid_length)\n        Y = self.addnorm_1(X, X2)\n        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_length)\n        Z = self.addnorm_2(Y, Y2)\n        return self.addnorm_3(Z, self.ffn(Z)), state","execution_count":null},{"metadata":{"id":"961F101D1A134B0787FCEE5B63BC1E85","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"decoder_blk = DecoderBlock(24, 48, 8, 0.5, 0)\nX = torch.ones((2, 100, 24))\nstate = [encoder_blk(X, valid_length), valid_length, [None]]\ndecoder_blk(X, state)[0].shape","execution_count":null},{"metadata":{"id":"40880B6C907D45808BA801FB2F5CD88C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"对于Transformer解码器来说，构造方式与编码器一样，除了最后一层添加一个dense layer以获得输出的置信度分数。下面让我们来实现一下Transformer Decoder，除了常规的超参数例如vocab_size embedding_size 之外，解码器还需要编码器的输出 enc_outputs 和句子有效长度 enc_valid_length。"},{"metadata":{"id":"88427C4C73C5414081D1CC3E86ED9B44","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class TransformerDecoder(d2l.Decoder):\n    def __init__(self, vocab_size, embedding_size, ffn_hidden_size,\n                 num_heads, num_layers, dropout, **kwargs):\n        super(TransformerDecoder, self).__init__(**kwargs)\n        self.embedding_size = embedding_size\n        self.num_layers = num_layers\n        self.embed = nn.Embedding(vocab_size, embedding_size)\n        self.pos_encoding = PositionalEncoding(embedding_size, dropout)\n        self.blks = nn.ModuleList()\n        for i in range(num_layers):\n            self.blks.append(\n                DecoderBlock(embedding_size, ffn_hidden_size, num_heads,\n                             dropout, i))\n        self.dense = nn.Linear(embedding_size, vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_length, *args):\n        return [enc_outputs, enc_valid_length, [None]*self.num_layers]\n\n    def forward(self, X, state):\n        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))\n        for blk in self.blks:\n            X, state = blk(X, state)\n        return self.dense(X), state","execution_count":null},{"metadata":{"id":"F1AA87AF64094997AE38043BC7D9D143","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 训练"},{"metadata":{"id":"027EBF16A9C94CA08557FF6D71B26290","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import zipfile\nimport torch\nimport requests\nfrom io import BytesIO\nfrom torch.utils import data\nimport sys\nimport collections\n\nclass Vocab(object): # This class is saved in d2l.\n  def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n    # sort by frequency and token\n    counter = collections.Counter(tokens)\n    token_freqs = sorted(counter.items(), key=lambda x: x[0])\n    token_freqs.sort(key=lambda x: x[1], reverse=True)\n    if use_special_tokens:\n      # padding, begin of sentence, end of sentence, unknown\n      self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n      tokens = ['', '', '', '']\n    else:\n      self.unk = 0\n      tokens = ['']\n    tokens += [token for token, freq in token_freqs if freq >= min_freq]\n    self.idx_to_token = []\n    self.token_to_idx = dict()\n    for token in tokens:\n      self.idx_to_token.append(token)\n      self.token_to_idx[token] = len(self.idx_to_token) - 1\n      \n  def __len__(self):\n    return len(self.idx_to_token)\n  \n  def __getitem__(self, tokens):\n    if not isinstance(tokens, (list, tuple)):\n      return self.token_to_idx.get(tokens, self.unk)\n    else:\n      return [self.__getitem__(token) for token in tokens]\n    \n  def to_tokens(self, indices):\n    if not isinstance(indices, (list, tuple)):\n      return self.idx_to_token[indices]\n    else:\n      return [self.idx_to_token[index] for index in indices]\n\ndef load_data_nmt(batch_size, max_len, num_examples=1000):\n    \"\"\"Download an NMT dataset, return its vocabulary and data iterator.\"\"\"\n    # Download and preprocess\n    def preprocess_raw(text):\n        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n        out = ''\n        for i, char in enumerate(text.lower()):\n            if char in (',', '!', '.') and text[i-1] != ' ':\n                out += ' '\n            out += char\n        return out \n\n\n    with open('/home/kesci/input/fraeng6506/fra.txt', 'r') as f:\n      raw_text = f.read()\n\n\n    text = preprocess_raw(raw_text)\n\n    # Tokenize\n    source, target = [], []\n    for i, line in enumerate(text.split('\\n')):\n        if i >= num_examples:\n            break\n        parts = line.split('\\t')\n        if len(parts) >= 2:\n            source.append(parts[0].split(' '))\n            target.append(parts[1].split(' '))\n\n    # Build vocab\n    def build_vocab(tokens):\n        tokens = [token for line in tokens for token in line]\n        return Vocab(tokens, min_freq=3, use_special_tokens=True)\n    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n\n    # Convert to index arrays\n    def pad(line, max_len, padding_token):\n        if len(line) > max_len:\n            return line[:max_len]\n        return line + [padding_token] * (max_len - len(line))\n\n    def build_array(lines, vocab, max_len, is_source):\n        lines = [vocab[line] for line in lines]\n        if not is_source:\n            lines = [[vocab.bos] + line + [vocab.eos] for line in lines]\n        array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])\n        valid_len = (array != vocab.pad).sum(1)\n        return array, valid_len\n\n    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)\n    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False)\n    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)\n    train_iter = data.DataLoader(train_data, batch_size, shuffle=True)\n    return src_vocab, tgt_vocab, train_iter","execution_count":null},{"metadata":{"id":"B0D03F3E62094F438FA3832EA279D543","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import os\n\nimport d2l\n\n# 平台暂时不支持gpu，现在会自动使用cpu训练，gpu可以用了之后会使用gpu来训练\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nembed_size, embedding_size, num_layers, dropout = 32, 32, 2, 0.05\nbatch_size, num_steps = 64, 10\nlr, num_epochs, ctx = 0.005, 250, d2l.try_gpu()\nprint(ctx)\nnum_hiddens, num_heads = 64, 4\n\nsrc_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size, num_steps)\n\nencoder = TransformerEncoder(\n    len(src_vocab), embedding_size, num_hiddens, num_heads, num_layers,\n    dropout)\ndecoder = TransformerDecoder(\n    len(src_vocab), embedding_size, num_hiddens, num_heads, num_layers,\n    dropout)\nmodel = d2l.EncoderDecoder(encoder, decoder)\nd2l.train_s2s_ch9(model, train_iter, lr, num_epochs, ctx)","execution_count":null},{"metadata":{"id":"2B5AA8F0C05B4732A44D078F7C17FEBC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"model.eval()\nfor sentence in ['Go .', 'Wow !', \"I'm OK .\", 'I won !']:\n    print(sentence + ' => ' + d2l.predict_s2s_ch9(\n        model, sentence, src_vocab, tgt_vocab, num_steps, ctx))","execution_count":null},{"metadata":{"id":"095B4BBE66334F42976CD249C92E9E85","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 小结练习\n**Transformer**\n- 自注意力会计算句子内任意两个位置的注意力权重\n- 在Transformer模型中，注意力头数为h，嵌入向量和隐藏状态维度均为d，那么一个多头注意力层所含的参数量是\n\n**层归一化 vs 批归一化**\n- 批归一化（Batch Normalization）是对每个神经元的输入数据以mini-batch为单位进行汇总\n\n**延伸阅读**\n- [How Transformers Work](https://towardsdatascience.com/transformers-141e32e69591)\n- [harvardnlp: The Annotated Transformer](https://zhuanlan.zhihu.com/p/106867810)\n- [【NLP】Transformer详解](https://zhuanlan.zhihu.com/p/44121378)"},{"metadata":{"id":"6F10E7C1A6FC4203BB5EAB78319CE579","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Task05：卷积神经网络基础；leNet；卷积神经网络进阶\n- 卷积神经网络基础\n- leNet\n- 卷积神经网络进阶"},{"metadata":{"id":"E6EAAF79B7D7493C80F8F0A70C5014E2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Part1 卷积神经网络基础\n卷积神经网路（Convolutional Neural Network, CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。卷积神经网路由一个或多个卷积层和顶端的全连通层（对应经典的神经网路）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网路能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网路在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网路，卷积神经网路需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。\n- 卷积神经网络中的一些基础知识以及基本概念\n> **卷积层和池化层**\n**解释`填充`、`步幅`、`输入通道`和`输出通道`的含义**"},{"metadata":{"id":"D9FA75C73F894E08BAE6C1454A363BE9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 二维卷积层\n\n本节介绍的是最常见的二维卷积层，常用于处理图像数据。"},{"metadata":{"id":"2484D952F7EA4DF69DA2C882D1C69398","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 二维互相关运算\n\n二维互相关（cross-correlation）运算的输入是一个二维输入数组和一个二维核（kernel）数组，输出也是一个二维数组，其中核数组通常称为卷积核或过滤器（filter）。卷积核的尺寸通常小于输入数组，卷积核在输入数组上滑动，在每个位置上，卷积核与该位置处的输入子数组按元素相乘并求和，得到输出数组中相应位置的元素。图1展示了一个互相关运算的例子，阴影部分分别是输入的第一个计算区域、核数组以及对应的输出。\n\n![Image Name](https://cdn.kesci.com/upload/image/q5nfdbhcw5.png?imageView2/0/w/640/h/640)\n图1 二维互相关运算\n\n下面我们用`corr2d`函数实现二维互相关运算，它接受输入数组`X`与核数组`K`，并输出数组`Y`。"},{"metadata":{"id":"508D6973251A43678C23F2E3F2C81CA3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import torch \nimport torch.nn as nn\n\ndef corr2d(X, K):\n    H, W = X.shape\n    h, w = K.shape\n    Y = torch.zeros(H - h + 1, W - w + 1)\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n    return Y","execution_count":1},{"metadata":{"id":"88A7AF4025A64828800490F253D41112","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"构造上图中的输入数组`X`、核数组`K`来验证二维互相关运算的输出。"},{"metadata":{"id":"6A4F591E2F294D948A527551330398CF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([[19., 25.],\n        [37., 43.]])\n","name":"stdout"}],"source":"X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\nK = torch.tensor([[0, 1], [2, 3]])\nY = corr2d(X, K)\nprint(Y)","execution_count":2},{"metadata":{"id":"D2DC3A41ABAF4FE0834653AF4E0235CA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 二维卷积层\n\n二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏置来得到输出。卷积层的模型参数包括卷积核和标量偏置。"},{"metadata":{"id":"84318D67FA9D448381040979FFA1CDDA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class Conv2D(nn.Module):\n    def __init__(self, kernel_size):\n        super(Conv2D, self).__init__()\n        self.weight = nn.Parameter(torch.randn(kernel_size))\n        self.bias = nn.Parameter(torch.randn(1))\n\n    def forward(self, x):\n        return corr2d(x, self.weight) + self.bias","execution_count":3},{"metadata":{"id":"CFAE839B946A433E8F8A091833718FE6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"构造一张$6 \\times 8$的图像，中间4列为黑（0），其余为白（1），希望检测到颜色边缘。我们的标签是一个$6 \\times 7$的二维数组，第2列是1（从1到0的边缘），第6列是-1（从0到1的边缘）。"},{"metadata":{"id":"865A5104BE3A464BAF22DEDC9D88584E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.]])\ntensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])\n","name":"stdout"}],"source":"X = torch.ones(6, 8)\nY = torch.zeros(6, 7)\nX[:, 2: 6] = 0\nY[:, 1] = 1\nY[:, 5] = -1\nprint(X)\nprint(Y)","execution_count":4},{"metadata":{"id":"1C01523198BA48DBAE6DDA1B88B35B54","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Step 5, loss 9.061\nStep 10, loss 2.052\nStep 15, loss 0.519\nStep 20, loss 0.139\nStep 25, loss 0.038\nStep 30, loss 0.010\ntensor([[ 0.9730, -0.9750]])\ntensor([0.0011])\n","name":"stdout"}],"source":"# 学习一个 1 x 2 的卷积层，通过卷积层来检测颜色边缘。\nconv2d = Conv2D(kernel_size=(1, 2))\nstep = 30\nlr = 0.01\nfor i in range(step):\n    Y_hat = conv2d(X)\n    l = ((Y_hat - Y) ** 2).sum()\n    l.backward()\n    # 梯度下降\n    conv2d.weight.data -= lr * conv2d.weight.grad\n    conv2d.bias.data -= lr * conv2d.bias.grad\n    '''\n    如果删掉.data会发生什么？如何解决此问题？\n    \n    参考解答：\n    @dw-安晟\n    with torch.no_grad(﻿)﻿:\n        conv2d.weight -= lr * conv2d.weight.grad\n        conv2d.bias -= lr * conv2d.bias.grad\n    \n    @Yiran：从pytorch的autograd包中关于梯度的反向传播机制去理解。\n①对于修改tensor的数值，但是又不希望被autograd记录（即不会影响反向传播），那么我么可以对tensor.data进行操作。\n②对于被torch.no_grad():包裹的tensor，在反向传播的过程中，其梯度是不会回传的\n详见：《动手学深度学习》(PyTorch版) 2.3.3 梯度，最后部分\nhttps://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.3_autograd?id=_233-%e6%a2%af%e5%ba%a6    \n    '''\n    \n    # 梯度清零\n    conv2d.weight.grad.zero_()\n    conv2d.bias.grad.zero_()\n    if (i + 1) % 5 == 0:\n        print('Step %d, loss %.3f' % (i + 1, l.item()))\n        \nprint(conv2d.weight.data)\nprint(conv2d.bias.data)","execution_count":5},{"metadata":{"id":"5B40413E139F4AE2891DA96AE45755DD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 互相关运算与卷积运算\n\n卷积层得名于卷积运算，但卷积层中用到的并非卷积运算而是互相关运算。我们将核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，所以使用互相关运算与使用卷积运算并无本质区别。"},{"metadata":{"id":"55B21339E9D342E1ACA4EC0CD38A8EA2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 特征图与感受野\n\n二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素$x$的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做$x$的感受野（receptive field）。\n\n以图1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为$2 \\times 2$的输出记为$Y$，将$Y$与另一个形状为$2 \\times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。"},{"metadata":{"id":"3E7B694D5E8F4C3898522A92E04FD9A7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 填充和步幅\n\n我们介绍卷积层的两个超参数，即填充和步幅，它们可以对给定形状的输入和卷积核改变输出形状。"},{"metadata":{"id":"FC0F4573E0C842D9AD622EE6B9D46937","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 填充\n\n填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素），图2里我们在原输入高和宽的两侧分别添加了值为0的元素。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5nfl6ejy4.png?imageView2/0/w/640/h/640)\n\n图2 在输入的高和宽两侧分别填充了0元素的二维互相关计算\n\n如果原输入的高和宽是$n_h$和$n_w$，卷积核的高和宽是$k_h$和$k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，则输出形状为：\n\n\n$$\n\n(n_h+p_h-k_h+1)\\times(n_w+p_w-k_w+1)\n\n$$\n\n\n我们在卷积神经网络中使用奇数高宽的核，比如$3 \\times 3$，$5 \\times 5$的卷积核，对于高度（或宽度）为大小为$2 k + 1$的核，令步幅为1，在高（或宽）两侧选择大小为$k$的填充，便可保持输入与输出尺寸相同。"},{"metadata":{"id":"11966F283FA44546A0C62B2A20F4FA27","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 步幅\n\n在互相关运算中，卷积核在输入数组上滑动，每次滑动的行数与列数即是步幅（stride）。此前我们使用的步幅都是1，图3展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5nflohnqg.png?imageView2/0/w/640/h/640)\n\n图3 高和宽上步幅分别为3和2的二维互相关运算\n\n一般来说，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为：\n\n\n$$\n\n\\lfloor(n_h+p_h-k_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w+p_w-k_w+s_w)/s_w\\rfloor\n\n$$\n\n\n如果$p_h=k_h-1$，$p_w=k_w-1$，那么输出形状将简化为$\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是$(n_h / s_h) \\times (n_w/s_w)$。\n\n当$p_h = p_w = p$时，我们称填充为$p$；当$s_h = s_w = s$时，我们称步幅为$s$。"},{"metadata":{"id":"5BB34FDCC587481F857E004401D405FD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 多输入通道和多输出通道\n\n之前的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3 \\times h \\times w$的多维数组，我们将大小为3的这一维称为通道（channel）维。"},{"metadata":{"id":"60500BB5623947A58A6FFB71A69EEA9A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 多输入通道\n\n卷积层的输入可以包含多个通道，图4展示了一个含2个输入通道的二维互相关计算的例子。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5nfmdnwbq.png?imageView2/0/w/640/h/640)\n\n图4 含2个输入通道的互相关计算\n\n假设输入数据的通道数为$c_i$，卷积核形状为$k_h\\times k_w$，我们为每个输入通道各分配一个形状为$k_h\\times k_w$的核数组，将$c_i$个互相关运算的二维输出按通道相加，得到一个二维数组作为输出。我们把$c_i$个核数组在通道维上连结，即得到一个形状为$c_i\\times k_h\\times k_w$的卷积核。"},{"metadata":{"id":"8BA73D7C989A43988A3165702B933601","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 多输出通道\n\n卷积层的输出也可以包含多个通道，设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\\times k_h\\times k_w$的核数组，将它们在输出通道维上连结，卷积核的形状即$c_o\\times c_i\\times k_h\\times k_w$。\n\n对于输出通道的卷积核，我们提供这样一种理解，一个$c_i \\times k_h \\times k_w$的核数组可以提取某种局部特征，但是输入可能具有相当丰富的特征，我们需要有多个这样的$c_i \\times k_h \\times k_w$的核数组，不同的核数组提取的是不同的特征。"},{"metadata":{"id":"4DCC57B1B8DA44038035D561AE6920C9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 1x1卷积层\n\n最后讨论形状为$1 \\times 1$的卷积核，我们通常称这样的卷积运算为$1 \\times 1$卷积，称包含这种卷积核的卷积层为$1 \\times 1$卷积层。图5展示了使用输入通道数为3、输出通道数为2的$1\\times 1$卷积核的互相关计算。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5nfmq980r.png?imageView2/0/w/640/h/640)\n\n图5 1x1卷积核的互相关计算。输入和输出具有相同的高和宽\n\n$1 \\times 1$卷积核可在不改变高宽的情况下，调整通道数。$1 \\times 1$卷积核不识别高和宽维度上相邻元素构成的模式，其主要计算发生在通道维上。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么$1\\times 1$卷积层的作用与全连接层等价。"},{"metadata":{"id":"CA0B1F47F1A14607A5FC4C11D869FA25","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 卷积层与全连接层的对比\n\n二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：\n\n一是全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。\n\n二是卷积层的参数量更少。不考虑偏置的情况下，一个形状为$(c_i, c_o, h, w)$的卷积核的参数量是$c_i \\times c_o \\times h \\times w$，与输入图像的宽高无关。假如一个卷积层的输入和输出形状分别是$(c_1, h_1, w_1)$和$(c_2, h_2, w_2)$，如果要用全连接层进行连接，参数数量就是$c_1 \\times c_2 \\times h_1 \\times w_1 \\times h_2 \\times w_2$。使用卷积层可以以较少的参数数量来处理更大的图像。"},{"metadata":{"id":"5EF91BD6998944FA87B2106ADF290791","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 卷积层的简洁实现\n\n我们使用Pytorch中的`nn.Conv2d`类来实现二维卷积层，主要关注以下几个构造函数参数：\n\n* `in_channels` (python:int) – Number of channels in the input imag\n* `out_channels` (python:int) – Number of channels produced by the convolution\n* `kernel_size` (python:int or tuple) – Size of the convolving kernel\n* `stride` (python:int or tuple, optional) – Stride of the convolution. Default: 1\n* `padding` (python:int or tuple, optional) – Zero-padding added to both sides of the input. Default: 0\n* `bias` (bool, optional) – If True, adds a learnable bias to the output. Default: True\n\n`forward`函数的参数为一个四维张量，形状为$(N, C_{in}, H_{in}, W_{in})$，返回值也是一个四维张量，形状为$(N, C_{out}, H_{out}, W_{out})$，其中$N$是批量大小，$C, H, W$分别表示通道数、高度、宽度。\n\n代码讲解"},{"metadata":{"id":"40F5E84BC4164E91835B6B8EA81D2B4C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"torch.Size([4, 2, 3, 5])\nY.shape:  torch.Size([4, 3, 3, 5])\nweight.shape:  torch.Size([3, 2, 3, 5])\nbias.shape:  torch.Size([3])\n","name":"stdout"}],"source":"X = torch.rand(4, 2, 3, 5)\nprint(X.shape)\n\nconv2d = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=(3, 5), stride=1, padding=(1, 2))\nY = conv2d(X)\nprint('Y.shape: ', Y.shape)\nprint('weight.shape: ', conv2d.weight.shape)\nprint('bias.shape: ', conv2d.bias.shape)","execution_count":6},{"metadata":{"id":"74090C1B10C346EB81C9C092EEE8A41D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 池化\n\n## 二维池化层\n\n池化层主要用于缓解卷积层对位置的过度敏感性。同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做最大池化或平均池化。图6展示了池化窗口形状为$2\\times 2$的最大池化。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5nfob3odo.png?imageView2/0/w/640/h/640)\n\n图6 池化窗口形状为 2 x 2 的最大池化\n\n二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为$p \\times q$的池化层称为$p \\times q$池化层，其中的池化运算叫作$p \\times q$池化。\n\n池化层也可以在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。\n\n在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着池化层的输出通道数与输入通道数相等。"},{"metadata":{"id":"1E8970DBC97F4D85B6EDFFBCBECC9E3C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 池化层的简洁实现\n\n我们使用Pytorch中的`nn.MaxPool2d`实现最大池化层，关注以下构造函数参数：\n\n* `kernel_size` – the size of the window to take a max over\n* `stride` – the stride of the window. Default value is kernel_size\n* `padding` – implicit zero padding to be added on both sides\n\n`forward`函数的参数为一个四维张量，形状为$(N, C, H_{in}, W_{in})$，返回值也是一个四维张量，形状为$(N, C, H_{out}, W_{out})$，其中$N$是批量大小，$C, H, W$分别表示通道数、高度、宽度。\n\n代码讲解"},{"metadata":{"id":"74033EBA229B42B082F3F21A84B7FF01","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]],\n\n         [[16., 17., 18., 19.],\n          [20., 21., 22., 23.],\n          [24., 25., 26., 27.],\n          [28., 29., 30., 31.]]]])\ntensor([[[[ 5.,  6.,  7.,  7.],\n          [13., 14., 15., 15.]],\n\n         [[21., 22., 23., 23.],\n          [29., 30., 31., 31.]]]])\n","name":"stdout"}],"source":"# 平均池化层使用的是nn.AvgPool2d，等价于nn.MaxPool2d\nX = torch.arange(32, dtype=torch.float32).view(1, 2, 4, 4)\npool2d = nn.MaxPool2d(kernel_size=3, padding=1, stride=(2, 1))\nY = pool2d(X)\nprint(X)\nprint(Y)","execution_count":7},{"metadata":{"id":"EB733E2315D84E8D86318D03936CFA63","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 小结练习\n（1）假如你用全连接层处理一张256 \\times 256的彩色（RGB）图像，输出包含1000个神经元，在使用偏置的情况下，参数数量是：\n解析：图像展平后长度为3 \\times 256 \\times 256，权重参数和偏置参数的数量是3 \\times 256 \\times 256 \\times 1000 + 1000 = 196609000。\n（2）假如你用全连接层处理一张256 \\times 256的彩色（RGB）图像，卷积核的高宽是3 \\times 3，输出包含10个通道，在使用偏置的情况下，这个卷积层共有多少个参数：\n解析：输入通道数是3，输出通道数是10，所以参数数量是10 \\times 3 \\times 3 \\times 3 + 10 = 280。\n（3）`conv2d = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=2)`，输入一张形状为3 \\times 100 \\times 100的图像，输出的形状为：\n解析：输出通道数是4，上下两侧总共填充4行，卷积核高度是3，所以输出的高度是104 - 3 + 1=102\n（4）池化层有参与模型的正向计算，同样也会参与反向传播。\n\n**问题锦集**\n\n@宋益东：\n- 通过观察互相关运算可知，输入数据（这里以二维数组为例）的边缘部分相较于中间部分来说，使用的次数较少，对于一些存储重要信息的边缘数据来说，这无疑是重大的损失，这里可以通过填充来解决这一问题吗？？也就是在输入数据的边缘填充0，使得原来的边缘数据中心化？？？\n- 卷积神经网络相较于全连接神经网络来说，其数据规模缩小，但它对数据信息的继承和提取相较于全连接神经网络怎样呢？？ 以文字识别为例，我们可以先通过卷积将数据的维度降下来，之后再将数据拉长，采用全连接神经网络；那么这两种网络是否有更好的组合方式呢？？比如说交叉使用等？？？  \n\n**延伸阅读：**\n- [卷积神经网络 – CNN](https://easyai.tech/ai-definition/cnn/)\n- [卷积神经网络](https://www.jiqizhixin.com/graph/technologies/85c4b79b-6428-4184-b9bc-5beb6e2b1f3f)"},{"metadata":{"id":"0513F00DFB20438387D72E7266F93D6C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Part2 LeNet\nLeNet 诞生于 1994 年，是最早的卷积神经网络之一，并且推动了深度学习领域的发展。自从 1988 年开始，在许多次成功的迭代后，这项由 Yann LeCun 完成的开拓性成果被命名为 LeNet5。LeNet5 的架构基于这样的观点：（尤其是）图像的特征分布在整张图像上，以及带有可学习参数的卷积是一种用少量参数在多个位置上提取相似特征的有效方式。在那时候，没有 GPU 帮助训练，甚至 CPU 的速度也很慢。因此，能够保存参数以及计算过程是一个关键进展。这和将每个像素用作一个大型多层神经网络的单独输入相反。LeNet5 阐述了那些像素不应该被使用在第一层，因为图像具有很强的空间相关性，而使用图像中独立的像素作为不同的输入特征则利用不到这些相关性。\n- lenet 模型介绍\n- lenet 网络搭建\n- 运用lenet进行图像识别-fashion-mnist数据集"},{"metadata":{"id":"FA68349BCB014F888901F90C28910A18","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"#  Convolutional Neural Networks\n\n使用全连接层的局限性：\n\n- 图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。\n- 对于大尺寸的输入图像，使用全连接层容易导致模型过大。\n\n使用卷积层的优势：\n\n- 卷积层保留输入形状。\n- 卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。"},{"metadata":{"id":"F984340659CF4C8B82A4E099FC1DB6DC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# LeNet 模型\n\nLeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5ndwsmsao.png?imageView2/0/w/960/h/960)\n\n\n卷积层块里的基本单位是卷积层后接平均池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的平均池化层则用来降低卷积层对位置的敏感性。\n\n卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5 \\times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。\n\n全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。\n\n下面我们通过Sequential类来实现LeNet模型。"},{"metadata":{"id":"65F32EA898BE4AB68BD2FF7808BEA5AD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport time","execution_count":null},{"metadata":{"id":"4E3DF9FC0ADC4E6696F1D29D32261466","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"#net\nclass Flatten(torch.nn.Module):  #展平操作\n    def forward(self, x):\n        return x.view(x.shape[0], -1)\n\nclass Reshape(torch.nn.Module): #将图像大小重定型\n    def forward(self, x):\n        return x.view(-1,1,28,28)      #(B x C x H x W)\n    \nnet = torch.nn.Sequential(     #Lelet                                                  \n    Reshape(),\n    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2), #b*1*28*28  =>b*6*28*28\n    nn.Sigmoid(),                                                       \n    nn.AvgPool2d(kernel_size=2, stride=2),                              #b*6*28*28  =>b*6*14*14\n    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),           #b*6*14*14  =>b*16*10*10\n    nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),                              #b*16*10*10  => b*16*5*5\n    Flatten(),                                                          #b*16*5*5   => b*400\n    nn.Linear(in_features=16*5*5, out_features=120),\n    nn.Sigmoid(),\n    nn.Linear(120, 84),\n    nn.Sigmoid(),\n    nn.Linear(84, 10)\n)","execution_count":null},{"metadata":{"id":"FCAA28EF48EE431583B928B384943F2B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 构造一个高和宽均为28的单通道数据样本，并逐层进行前向计算来查看每个层的输出形状。\nX = torch.randn(size=(1,1,28,28), dtype = torch.float32)\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape: \\t',X.shape)","execution_count":null},{"metadata":{"id":"4EFCB82F24D744858D95D68B592F250A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"可以看到，在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层则将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5ndxi6jl5.png?imageView2/0/w/640/h/640)\n"},{"metadata":{"id":"53FD0BD3D2404390A66A7E692FD7BE0E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 获取数据和训练模型\n\n下面我们来实现LeNet模型。我们仍然使用Fashion-MNIST作为训练数据集。"},{"metadata":{"id":"18528DAE249A4C50BAFE9CF24BEDCF79","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 数据\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(\n    batch_size=batch_size, root='/home/kesci/input/FashionMNIST2065')\nprint(len(train_iter))\n","execution_count":null},{"metadata":{"id":"9ED27A7ABC3D44F289B69E1B7F80C9C1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"#数据展示\nimport matplotlib.pyplot as plt\ndef show_fashion_mnist(images, labels):\n    d2l.use_svg_display()\n    # 这里的_表示我们忽略（不使用）的变量\n    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n    for f, img, lbl in zip(figs, images, labels):\n        f.imshow(img.view((28, 28)).numpy())\n        f.set_title(lbl)\n        f.axes.get_xaxis().set_visible(False)\n        f.axes.get_yaxis().set_visible(False)\n    plt.show()\n\nfor Xdata,ylabel in train_iter:\n    break\nX, y = [], []\nfor i in range(10):\n    print(Xdata[i].shape,ylabel[i].numpy())\n    X.append(Xdata[i]) # 将第i个feature加到X中\n    y.append(ylabel[i].numpy()) # 将第i个label加到y中\nshow_fashion_mnist(X, y)","execution_count":null},{"metadata":{"id":"4D0113AB3DCA4CC681B63057DC0B155C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"因为卷积神经网络计算比多层感知机要复杂，建议使用GPU来加速计算。我们查看看是否可以用GPU，如果成功则使用`cuda:0`，否则仍然使用`cpu`。"},{"metadata":{"id":"D9F109622399472A80B0DE0FDE9E07D7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# This function has been saved in the d2l package for future use\n#use GPU\ndef try_gpu():\n    \"\"\"If GPU is available, return torch.device as cuda:0; else return torch.device as cpu.\"\"\"\n    if torch.cuda.is_available():\n        device = torch.device('cuda:0')\n    else:\n        device = torch.device('cpu')\n    return device\n\ndevice = try_gpu()\ndevice","execution_count":null},{"metadata":{"id":"0D11F74615F94630B5CC036571F785D5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"接下来实现`evaluate_accuracy`函数，该函数用于计算模型`net`在数据集`data_iter`上的准确率。"},{"metadata":{"id":"10DEEF1575A344BA89DF7D1DCA22331C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"\n#计算准确率\n'''\n(1). net.train()\n  启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为True\n(2). net.eval()\n不启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为False\n'''\n\ndef evaluate_accuracy(data_iter, net,device=torch.device('cpu')):\n    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n    acc_sum,n = torch.tensor([0],dtype=torch.float32,device=device),0\n    for X,y in data_iter:\n        # If device is the GPU, copy the data to the GPU.\n        X,y = X.to(device),y.to(device)\n        net.eval()\n        with torch.no_grad():\n            y = y.long()\n            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))  #[[0.2 ,0.4 ,0.5 ,0.6 ,0.8] ,[ 0.1,0.2 ,0.4 ,0.3 ,0.1]] => [ 4 , 2 ]\n            n += y.shape[0]\n    return acc_sum.item()/n","execution_count":null},{"metadata":{"id":"4A092465DE6F4A7BB0FD08E669ACF25B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 定义训练函数\ndef train_ch5(net, train_iter, test_iter,criterion, num_epochs, batch_size, device,lr=None):\n    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n    print('training on', device)\n    net.to(device)\n    optimizer = optim.SGD(net.parameters(), lr=lr)\n    for epoch in range(num_epochs):\n        train_l_sum = torch.tensor([0.0],dtype=torch.float32,device=device)\n        train_acc_sum = torch.tensor([0.0],dtype=torch.float32,device=device)\n        n, start = 0, time.time()\n        for X, y in train_iter:\n            net.train()\n            \n            optimizer.zero_grad()\n            X,y = X.to(device),y.to(device) \n            y_hat = net(X)\n            loss = criterion(y_hat, y)\n            loss.backward()\n            optimizer.step()\n            \n            with torch.no_grad():\n                y = y.long()\n                train_l_sum += loss.float()\n                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=1) == y))).float()\n                n += y.shape[0]\n        test_acc = evaluate_accuracy(test_iter, net,device)\n        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n              'time %.1f sec'\n              % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc,\n                 time.time() - start))","execution_count":null},{"metadata":{"id":"E6D8492A3B424A81B39A33CF9493D1D2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"我们重新将模型参数初始化到对应的设备`device`(`cpu` or `cuda:0`)之上，并使用Xavier随机初始化。损失函数和训练算法则依然使用交叉熵损失函数和小批量随机梯度下降。"},{"metadata":{"id":"01CC83B20753457E82BAD8307F362D17","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 训练\nlr, num_epochs = 0.9, 10\n\ndef init_weights(m):\n    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n        torch.nn.init.xavier_uniform_(m.weight)\n\nnet.apply(init_weights)\nnet = net.to(device)\n\ncriterion = nn.CrossEntropyLoss()   #交叉熵描述了两个概率分布之间的距离，交叉熵越小说明两者之间越接近\ntrain_ch5(net, train_iter, test_iter, criterion,num_epochs, batch_size,device, lr)","execution_count":null},{"metadata":{"id":"A31554988D794C0A8C8D1959E2463CAB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# test\nfor testdata,testlabe in test_iter:\n    testdata,testlabe = testdata.to(device),testlabe.to(device)\n    break\nprint(testdata.shape,testlabe.shape)\nnet.eval()\ny_pre = net(testdata)\nprint(torch.argmax(y_pre,dim=1)[:10])\nprint(testlabe[:10])","execution_count":null},{"metadata":{"id":"2C532D5E6F9545CDAE316D2F22272DFE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 总结\n\n卷积神经网络就是含卷积层的网络。\nLeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。"},{"metadata":{"id":"1485A9EDFCAC435ABE1FBEB1C4706B45","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 小结练习\n\n**延伸阅读**\n- [LeNet](https://www.jiqizhixin.com/graph/technologies/6c9baf12-1a32-4c53-8217-8c9f69bd011b)\n- [A Comprehensive Guide to Convolutional Neural Networks](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)"},{"metadata":{"id":"67F9AACEA2154D04905BE9DA6278B987","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Part3 卷积神经网络进阶\n- AlexNet\n- VGG\n- NiN\n- GoogLeNet"},{"metadata":{"id":"3E2A8A0DD6B34EE2B865075FB01980F5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":" # 深度卷积神经网络（AlexNet） "},{"metadata":{"id":"6E9BDF3BF59F4F8386118F2C79C542C1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"LeNet:  在大的真实数据集上的表现并不尽如人意。     \n1.神经网络计算复杂。  \n2.还没有大量深入研究参数初始化和凸优化算法等诸多领域。  \n  \n机器学习的特征提取:手工定义的特征提取函数  \n神经网络的特征提取：通过学习得到数据的多级表征，并逐级表现越来越抽象的概念或模式。  \n  \n> 神经网络发展的限制:数据、硬件"},{"metadata":{"id":"FBFCA2C1628349FF86B7C003B349629E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### AlexNet\n首次证明了学习到的特征可以超越人工设计的特征，从而一举打破计算机视觉研究的前状。   \n**特征：**\n1. 8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。\n2. 将sigmoid激活函数改成了更加简单的ReLU激活函数。\n3. 用Dropout来控制全连接层的模型复杂度。\n4. 引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。\n\n![Image Name](https://cdn.kesci.com/upload/image/q5kv4gpx88.png?imageView2/0/w/640/h/640)"},{"metadata":{"id":"BEA8FA2E48294A0D92AD964765BCBC4F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"#考虑到本代码中的模型过大，CPU训练较慢，\n#我们还将代码上传了一份到 https://www.kaggle.com/boyuai/boyu-d2l-modernconvolutionalnetwork\n#如希望提前使用gpu运行请至kaggle。\n\nimport time\nimport torch\nfrom torch import nn, optim\nimport torchvision\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input/\") \nimport d2lzh1981 as d2l\nimport os\nimport torch.nn.functional as F\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass AlexNet(nn.Module):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2), # kernel_size, stride\n            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n            nn.Conv2d(96, 256, 5, 1, 2),\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2),\n            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。\n            # 前两个卷积层后不使用池化层来减小输入的高和宽\n            nn.Conv2d(256, 384, 3, 1, 1),\n            nn.ReLU(),\n            nn.Conv2d(384, 384, 3, 1, 1),\n            nn.ReLU(),\n            nn.Conv2d(384, 256, 3, 1, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2)\n        )\n         # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合\n        self.fc = nn.Sequential(\n            nn.Linear(256*5*5, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            #由于使用CPU镜像，精简网络，若为GPU镜像可添加该层\n            #nn.Linear(4096, 4096),\n            #nn.ReLU(),\n            #nn.Dropout(0.5),\n\n            # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n            nn.Linear(4096, 10),\n        )\n\n    def forward(self, img):\n\n        feature = self.conv(img)\n        output = self.fc(feature.view(img.shape[0], -1))\n        return output","execution_count":null},{"metadata":{"id":"3DC58A836C76478F800C46DE75CDC1C9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"net = AlexNet()\nprint(net)","execution_count":null},{"metadata":{"id":"1DD3EF51D118460D8D55386D2C04E748","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 载入数据集"},{"metadata":{"id":"CF45CEA47C73437185DE839A3FAD5F11","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 本函数已保存在d2lzh_pytorch包中方便以后使用\ndef load_data_fashion_mnist(batch_size, resize=None, root='/home/kesci/input/FashionMNIST2065'):\n    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n    trans = []\n    if resize:\n        trans.append(torchvision.transforms.Resize(size=resize))\n    trans.append(torchvision.transforms.ToTensor())\n    \n    transform = torchvision.transforms.Compose(trans)\n    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n\n    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=2)\n    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return train_iter, test_iter\n\n#batchsize=128\nbatch_size = 16\n# 如出现“out of memory”的报错信息，可减小batch_size或resize\ntrain_iter, test_iter = load_data_fashion_mnist(batch_size,224)\nfor X, Y in train_iter:\n    print('X =', X.shape,\n        '\\nY =', Y.type(torch.int32))\n    break\n    ","execution_count":null},{"metadata":{"id":"F84BF7AADC00493F8F9694DCAC72F3F3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 训练"},{"metadata":{"id":"61E9BE7D1B2848CB94B630731065DF07","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"lr, num_epochs = 0.001, 3\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nd2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)","execution_count":null},{"metadata":{"id":"DC56940C416F462386D0B1AE204C91C8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"#  使用重复元素的网络（VGG）\nVGG：通过重复使⽤简单的基础块来构建深度模型。  \nBlock:数个相同的填充为1、窗口形状为$3\\times 3$的卷积层,接上一个步幅为2、窗口形状为$2\\times 2$的最大池化层。  \n卷积层保持输入的高和宽不变，而池化层则对其减半。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5l6vut7h1.png?imageView2/0/w/640/h/640)"},{"metadata":{"id":"A2A810E4F05044B891184F97CC8B3C39","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### VGG11的简单实现"},{"metadata":{"id":"AA1CD91283D84139ACD3A158DA4DEC4B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def vgg_block(num_convs, in_channels, out_channels): #卷积层个数，输入通道数，输出通道数\n    blk = []\n    for i in range(num_convs):\n        if i == 0:\n            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n        else:\n            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n        blk.append(nn.ReLU())\n    blk.append(nn.MaxPool2d(kernel_size=2, stride=2)) # 这里会使宽高减半\n    return nn.Sequential(*blk)","execution_count":null},{"metadata":{"id":"1F68E4A34212493A8783CF3BBF3D59F7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"conv_arch = ((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512))\n# 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7\nfc_features = 512 * 7 * 7 # c * w * h\nfc_hidden_units = 4096 # 任意","execution_count":null},{"metadata":{"id":"8449CF01A0FF48638771201A7D18CD5D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def vgg(conv_arch, fc_features, fc_hidden_units=4096):\n    net = nn.Sequential()\n    # 卷积层部分\n    for i, (num_convs, in_channels, out_channels) in enumerate(conv_arch):\n        # 每经过一个vgg_block都会使宽高减半\n        net.add_module(\"vgg_block_\" + str(i+1), vgg_block(num_convs, in_channels, out_channels))\n    # 全连接层部分\n    net.add_module(\"fc\", nn.Sequential(d2l.FlattenLayer(),\n                                 nn.Linear(fc_features, fc_hidden_units),\n                                 nn.ReLU(),\n                                 nn.Dropout(0.5),\n                                 nn.Linear(fc_hidden_units, fc_hidden_units),\n                                 nn.ReLU(),\n                                 nn.Dropout(0.5),\n                                 nn.Linear(fc_hidden_units, 10)\n                                ))\n    return net","execution_count":null},{"metadata":{"id":"D182160C368344478C4E5710A5B2C5D9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"net = vgg(conv_arch, fc_features, fc_hidden_units)\nX = torch.rand(1, 1, 224, 224)\n\n# named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)\nfor name, blk in net.named_children(): \n    X = blk(X)\n    print(name, 'output shape: ', X.shape)","execution_count":null},{"metadata":{"id":"3EA86A589ECC45CD814B2998F2037E54","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"ratio = 8\nsmall_conv_arch = [(1, 1, 64//ratio), (1, 64//ratio, 128//ratio), (2, 128//ratio, 256//ratio), \n                   (2, 256//ratio, 512//ratio), (2, 512//ratio, 512//ratio)]\nnet = vgg(small_conv_arch, fc_features // ratio, fc_hidden_units // ratio)\nprint(net)","execution_count":null},{"metadata":{"id":"AD45F0300F094D3388C17DD2ACBC3E4B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"batchsize=16\n#batch_size = 64\n# 如出现“out of memory”的报错信息，可减小batch_size或resize\n# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n\nlr, num_epochs = 0.001, 5\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nd2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)","execution_count":null},{"metadata":{"id":"4F9C4DB3A6144DE89C62EF9CC2CC6E1E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"#  网络中的网络（NiN） \nLeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。  \nNiN：串联多个由卷积层和“全连接”层构成的小网络来构建多个深层网络。  \n输出通道数等于标签类别数的NiN块，然后使用全局平均池化层对每个通道中所有元素求平均并直接用于分类。  \n\n![Image Name](https://cdn.kesci.com/upload/image/q5l6u1p5vy.png?imageView2/0/w/960/h/960)\n\n1×1卷积核作用   \n1.放缩通道数：通过控制卷积核的数量达到通道数的放缩。  \n2.增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。  \n3.计算参数少   "},{"metadata":{"id":"8464B9E1AF3B470086FE013A643852F9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def nin_block(in_channels, out_channels, kernel_size, stride, padding):\n    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n                        nn.ReLU(),\n                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n                        nn.ReLU(),\n                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n                        nn.ReLU())\n    return blk","execution_count":null},{"metadata":{"id":"A04514EA50684D13B0FFADB10E79C4C1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 已保存在d2lzh_pytorch\nclass GlobalAvgPool2d(nn.Module):\n    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\n    def __init__(self):\n        super(GlobalAvgPool2d, self).__init__()\n    def forward(self, x):\n        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n\nnet = nn.Sequential(\n    nin_block(1, 96, kernel_size=11, stride=4, padding=0),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    nin_block(96, 256, kernel_size=5, stride=1, padding=2),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    nin_block(256, 384, kernel_size=3, stride=1, padding=1),\n    nn.MaxPool2d(kernel_size=3, stride=2), \n    nn.Dropout(0.5),\n    # 标签类别数是10\n    nin_block(384, 10, kernel_size=3, stride=1, padding=1),\n    GlobalAvgPool2d(), \n    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n    d2l.FlattenLayer())","execution_count":null},{"metadata":{"id":"5B54194AF9B1474DB1AC79B595BAEFEA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"X = torch.rand(1, 1, 224, 224)\nfor name, blk in net.named_children(): \n    X = blk(X)\n    print(name, 'output shape: ', X.shape)","execution_count":null},{"metadata":{"id":"D83FFA1DE80C4F46851F9FE2DEE2594C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"batch_size = 128\n# 如出现“out of memory”的报错信息，可减小batch_size或resize\n#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n\nlr, num_epochs = 0.002, 5\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nd2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)","execution_count":null},{"metadata":{"id":"45E61268F628413CAB428734856CF904","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"NiN重复使用由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层网络。  \nNiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。   \nNiN的以上设计思想影响了后面一系列卷积神经网络的设计。  "},{"metadata":{"id":"EE8D5A954D1F4A6BAE00C972C0D2B081","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# GoogLeNet\n1. 由Inception基础块组成。  \n2. Inception块相当于多个有4条线路的网络。它通过不同窗口形状的卷积层和最大池化层来并且抽取信息，并使用1×1卷积层减少通道数从而降低模型复杂度。   \n3. 可以定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 \n\n![Image Name](https://cdn.kesci.com/upload/image/q5l6uortw.png?imageView2/0/w/640/h/640)"},{"metadata":{"id":"F4ADF1EBE71A4045830B7C94F05E9829","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class Inception(nn.Module):\n    # c1 - c4为每条线路里的层的输出通道数\n    def __init__(self, in_c, c1, c2, c3, c4):\n        super(Inception, self).__init__()\n        # 线路1，单1 x 1卷积层\n        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=1)\n        # 线路2，1 x 1卷积层后接3 x 3卷积层\n        self.p2_1 = nn.Conv2d(in_c, c2[0], kernel_size=1)\n        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n        # 线路3，1 x 1卷积层后接5 x 5卷积层\n        self.p3_1 = nn.Conv2d(in_c, c3[0], kernel_size=1)\n        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n        # 线路4，3 x 3最大池化层后接1 x 1卷积层\n        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=1)\n\n    def forward(self, x):\n        p1 = F.relu(self.p1_1(x))\n        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n        p4 = F.relu(self.p4_2(self.p4_1(x)))\n        return torch.cat((p1, p2, p3, p4), dim=1)  # 在通道维上连结输出","execution_count":null},{"metadata":{"id":"275F1F50A60B42EA8D8E03A00E6C66DD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### GoogLeNet模型\n完整模型结构  \n\n![Image Name](https://cdn.kesci.com/upload/image/q5l6x0fyyn.png?imageView2/0/w/640/h/640)\n"},{"metadata":{"id":"38B7F273DCBF4034AA4E981DE477E030","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n                   nn.ReLU(),\n                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\nb2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\nb3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n                   Inception(256, 128, (128, 192), (32, 96), 64),\n                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\nb4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n                   Inception(512, 160, (112, 224), (24, 64), 64),\n                   Inception(512, 128, (128, 256), (24, 64), 64),\n                   Inception(512, 112, (144, 288), (32, 64), 64),\n                   Inception(528, 256, (160, 320), (32, 128), 128),\n                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\nb5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n                   Inception(832, 384, (192, 384), (48, 128), 128),\n                   d2l.GlobalAvgPool2d())\n\nnet = nn.Sequential(b1, b2, b3, b4, b5, \n                    d2l.FlattenLayer(), nn.Linear(1024, 10))\n\nnet = nn.Sequential(b1, b2, b3, b4, b5, d2l.FlattenLayer(), nn.Linear(1024, 10))\n\nX = torch.rand(1, 1, 96, 96)\n\nfor blk in net.children(): \n    X = blk(X)\n    print('output shape: ', X.shape)\n\n#batchsize=128\nbatch_size = 16\n# 如出现“out of memory”的报错信息，可减小batch_size或resize\n#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n\nlr, num_epochs = 0.001, 5\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nd2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)","execution_count":null},{"metadata":{"id":"A324B88D0A5343DE83F3E066D08B7D4C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 小结练习\n\n**延伸阅读**\n- [经典CNN结构简析：AlexNet、VGG、NIN、GoogLeNet、ResNet etc.](https://zhuanlan.zhihu.com/p/47391705)\n- [（深度学习）CNN经典模型：Lenet5，Alexnet，VGG，Network-in-netwoork，Googlenet , ZFnet](https://blog.csdn.net/ling00007/article/details/79114442)\n- [Pytorch之CNN：基于Pytorch框架实现经典卷积神经网络的算法(LeNet、AlexNet、VGG、NIN、GoogleNet、ResNet)](https://www.okcode.net/article/57478)"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}