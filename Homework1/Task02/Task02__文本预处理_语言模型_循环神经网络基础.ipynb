{"cells":[{"metadata":{"id":"B07420424D4C437184F1D92ED31AEFC4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Task02：文本预处理；语言模型；循环神经网络基础\n- **Part1：文本预处理**\n\n- **Part2：语言模型**\n\n- **Part3：循环神经网络基础**"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_635azvh","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B4BA5B0FCB884B6DBA6FFE071318505E","mdEditEnable":false},"source":"# 文本预处理\n\n\n文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：\n\n1. 读入文本\n2. 分词\n3. 建立字典，将每个词映射到一个唯一的索引（index）\n4. 将文本从词的序列转换为索引的序列，方便输入模型"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_da72pg7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C3B3705CB33847A895AD10619F23E97B","mdEditEnable":false},"source":"## 读入文本\n\n我们用一部英文小说，即H. G. Well的[Time Machine](http://www.gutenberg.org/ebooks/35)，作为示例，展示文本预处理的具体过程。"},{"cell_type":"code","execution_count":10,"metadata":{"graffitiCellId":"id_ytfpat1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7FA4C53DED4F42279EA3AB3229B88DB7","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"# sentences 3221\n","name":"stdout"}],"source":"import collections\nimport re\n\ndef read_time_machine():\n    with open('/home/kesci/input/timemachine7163/timemachine.txt', 'r') as f:\n        lines = [re.sub('[^a-z]+', ' ', line.strip().lower()) for line in f]\n    return lines\n\n\nlines = read_time_machine()\nprint('# sentences %d' % len(lines))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_gy3tram","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EABE813C62FC4E1B8DFFD7B819C31829","mdEditEnable":false},"source":"## 分词\n\n我们对每个句子进行分词，也就是将一个句子划分成若干个词（token），转换为一个词的序列。"},{"cell_type":"code","execution_count":11,"metadata":{"graffitiCellId":"id_z5grfxp","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F80F8AFC1C0A48BDB66D52A18DC3A940","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"[['the', 'time', 'machine', 'by', 'h', 'g', 'wells', ''], ['']]"},"transient":{},"execution_count":11}],"source":"def tokenize(sentences, token='word'):\n    \"\"\"Split sentences into word or char tokens\"\"\"\n    if token == 'word':\n        return [sentence.split(' ') for sentence in sentences]\n    elif token == 'char':\n        return [list(sentence) for sentence in sentences]\n    else:\n        print('ERROR: unkown token type '+token)\n\ntokens = tokenize(lines)\ntokens[0:2]"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_rap2ka4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"01CE759264D84FAA8C60CE9156B86157","mdEditEnable":false},"source":"## 建立字典\n\n为了方便模型处理，我们需要将字符串转换为数字。因此我们需要先构建一个字典（vocabulary），将每个词映射到一个唯一的索引编号。"},{"cell_type":"code","execution_count":12,"metadata":{"attributes":{"classes":[],"id":"","n":"9"},"graffitiCellId":"id_wapwqkb","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"37532FBF89C242A1805534BBE05C343A","collapsed":false,"scrolled":false},"outputs":[],"source":"class Vocab(object):\n    def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n        counter = count_corpus(tokens)  # : \n        self.token_freqs = list(counter.items())\n        self.idx_to_token = []\n        if use_special_tokens:\n            # padding, begin of sentence, end of sentence, unknown\n            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n            self.idx_to_token += ['', '', '', '']\n        else:\n            self.unk = 0\n            self.idx_to_token += ['']\n        self.idx_to_token += [token for token, freq in self.token_freqs\n                        if freq >= min_freq and token not in self.idx_to_token]\n        self.token_to_idx = dict()\n        for idx, token in enumerate(self.idx_to_token):\n            self.token_to_idx[token] = idx\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n\ndef count_corpus(sentences):\n    tokens = [tk for st in sentences for tk in st]\n    return collections.Counter(tokens)  # 返回一个字典，记录每个词的出现次数"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_k17qg7x","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DB6949BC67FF4C7481DFFD00FE64BE56","mdEditEnable":false},"source":"我们看一个例子，这里我们尝试用Time Machine作为语料构建字典"},{"cell_type":"code","execution_count":13,"metadata":{"attributes":{"classes":[],"id":"","n":"23"},"graffitiCellId":"id_hm9y6bm","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1BE94FF518DB4C4A8CDFB95C0262B47E","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"[('', 0), ('the', 1), ('time', 2), ('machine', 3), ('by', 4), ('h', 5), ('g', 6), ('wells', 7), ('i', 8), ('traveller', 9)]\n","name":"stdout"}],"source":"vocab = Vocab(tokens)\nprint(list(vocab.token_to_idx.items())[0:10])"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_l6pjfl7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"73D0F629056F41B59D4A53F15BFAB552","mdEditEnable":false},"source":"## 将词转为索引\n\n使用字典，我们可以将原文本中的句子从单词序列转换为索引序列"},{"cell_type":"code","execution_count":14,"metadata":{"graffitiCellId":"id_k48bsl2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9FBB71C21B5C4F5283C70CFE3BB07112","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"words: ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him', '']\nindices: [1, 2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0]\nwords: ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\nindices: [20, 21, 22, 23, 24, 16, 25, 26, 27, 28, 29, 30]\n","name":"stdout"}],"source":"for i in range(8, 10):\n    print('words:', tokens[i])\n    print('indices:', vocab[tokens[i]])"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_q6fupul","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EA20BC8762A74B188D3FDD761BFEDE98","mdEditEnable":false},"source":"## 用现有工具进行分词\n\n我们前面介绍的分词方式非常简单，它至少有以下几个缺点:\n\n1. 标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了\n2. 类似“shouldn't\", \"doesn't\"这样的词会被错误地处理\n3. 类似\"Mr.\", \"Dr.\"这样的词会被错误地处理\n\n我们可以通过引入更复杂的规则来解决这些问题，但是事实上，有一些现有的工具可以很好地进行分词，我们在这里简单介绍其中的两个：[spaCy](https://spacy.io/)和[NLTK](https://www.nltk.org/)。\n\n下面是一个简单的例子："},{"cell_type":"code","execution_count":15,"metadata":{"graffitiCellId":"id_7u3knll","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"46F5F57611E248ECB51F04BD0104E278","collapsed":false,"scrolled":false},"outputs":[],"source":"text = \"Mr. Chen doesn't agree with my suggestion.\""},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ae3i5g2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7D5831E3D5AD4FF48155334F73065451","mdEditEnable":false},"source":"spaCy:"},{"cell_type":"code","execution_count":16,"metadata":{"graffitiCellId":"id_uz6civu","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"30D69C6B1BE44362BA556E2E5EEF493A","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"['Mr.', 'Chen', 'does', \"n't\", 'agree', 'with', 'my', 'suggestion', '.']\n","name":"stdout"}],"source":"import spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nprint([token.text for token in doc])"},{"metadata":{"id":"765CC9B5A1C348A58A2B340ADC532FD1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"NLTK:"},{"cell_type":"code","execution_count":17,"metadata":{"graffitiCellId":"id_r13iwga","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B83D30D3670B44A38527B4943BE4DBE0","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"['Mr.', 'Chen', 'does', \"n't\", 'agree', 'with', 'my', 'suggestion', '.']\n","name":"stdout"}],"source":"from nltk.tokenize import word_tokenize\nfrom nltk import data\ndata.path.append('/home/kesci/input/nltk_data3784/nltk_data')\nprint(word_tokenize(text))"},{"metadata":{"id":"B1A149F7A381409B8633D22F46316781","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 小结作业练习"},{"metadata":{"id":"E5A5F2D56B534F218A23836124848E07","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"阅读`Vocab`类，回答以下问题：\n\n```\nclass Vocab(object):\n    def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n        counter = count_corpus(tokens)  # : \n        self.token_freqs = list(counter.items())\n        self.idx_to_token = []\n        if use_special_tokens:\n            # padding, begin of sentence, end of sentence, unknown\n            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n            self.idx_to_token += ['', '', '', '']\n        else:\n            self.unk = 0\n            self.idx_to_token += ['']\n        self.idx_to_token += [token for token, freq in self.token_freqs\n                        if freq >= min_freq and token not in self.idx_to_token]\n        self.token_to_idx = dict()\n        for idx, token in enumerate(self.idx_to_token):\n            self.token_to_idx[token] = idx\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n\ndef count_corpus(sentences):\n    tokens = [tk for st in sentences for tk in st]\n    return collections.Counter(tokens)  # 返回一个字典，记录每个词的出现次数\n```\n\n（1）. 哪一项不是构建`Vocab`类所必须的步骤：\n\nA. 词频统计，清洗低频词\nB. 统计句子长度\nC. 构建索引到token的映射\nD. 构建token到索引的映射\n\n（2）. 无论`use_special_token`参数是否为真，都会使用的特殊`token`是（`<unk>`），表示未登录词。"},{"metadata":{"id":"A6DA604D8CCD43F1807CFCF2249F2D15","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 语言模型\n\n- n元语法（n-gram）\n\n- 语言模型数据集的两种采样方法\n\n  > 1. 随机采样\n\t> 2. 相邻采样"},{"metadata":{"id":"903C82F80E044978BF5DDA44509488A3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"一段自然语言文本可以看作是一个离散时间序列，给定一个长度为$T$的词的序列$w_1, w_2, \\ldots, w_T$，语言模型的目标就是评估该序列是否合理，即计算该序列的概率：\n\n\n$$\nP(w_1, w_2, \\ldots, w_T).\n$$\n\n\n本节我们介绍基于统计的语言模型，主要是$n$元语法（$n$-gram）。在后续内容中，我们将会介绍基于神经网络的语言模型。\n\n假设序列$w_1, w_2, \\ldots, w_T$中的每个词是依次生成的，我们有\n\n\n$$\n\n\\begin{align*}\nP(w_1, w_2, \\ldots, w_T)\n&= \\prod_{t=1}^T P(w_t \\mid w_1, \\ldots, w_{t-1})\\\\\n&= P(w_1)P(w_2 \\mid w_1) \\cdots P(w_T \\mid w_1w_2\\cdots w_{T-1})\n\\end{align*}\n\n$$\n\n\n例如，一段含有4个词的文本序列的概率\n\n\n$$\nP(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_1, w_2, w_3).\n$$\n\n\n语言模型的参数就是词的概率以及给定前几个词情况下的条件概率。设训练数据集为一个大型文本语料库，如维基百科的所有条目，词的概率可以通过该词在训练数据集中的相对词频来计算，例如，$w_1$的概率可以计算为：\n\n\n$$\n\n\\hat P(w_1) = \\frac{n(w_1)}{n}\n\n$$\n\n\n其中$n(w_1)$为语料库中以$w_1$作为第一个词的文本的数量，$n$为语料库中文本的总数量。\n\n类似的，给定$w_1$情况下，$w_2$的条件概率可以计算为：\n\n\n$$\n\n\\hat P(w_2 \\mid w_1) = \\frac{n(w_1, w_2)}{n(w_1)}\n\n$$\n\n\n其中$n(w_1, w_2)$为语料库中以$w_1$作为第一个词，$w_2$作为第二个词的文本的数量。"},{"metadata":{"id":"58762DC94C394B178977B2945B386FBB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## n元语法\n\n序列长度增加，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$元语法通过马尔可夫假设简化模型，马尔科夫假设是指一个词的出现只与前面$n$个词相关，即$n$阶马尔可夫链（Markov chain of order $n$），如果$n=1$，那么有$P(w_3 \\mid w_1, w_2) = P(w_3 \\mid w_2)$。基于$n-1$阶马尔可夫链，我们可以将语言模型改写为\n\n\n$$\nP(w_1, w_2, \\ldots, w_T) = \\prod_{t=1}^T P(w_t \\mid w_{t-(n-1)}, \\ldots, w_{t-1}) .\n$$\n\n\n以上也叫$n$元语法（$n$-grams），它是基于$n - 1$阶马尔可夫链的概率语言模型。例如，当$n=2$时，含有4个词的文本序列的概率就可以改写为：\n\n\n$$\n\n\\begin{align*}\nP(w_1, w_2, w_3, w_4)\n&= P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_1, w_2, w_3)\\\\\n&= P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_2) P(w_4 \\mid w_3)\n\\end{align*}\n\n$$\n\n\n当$n$分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列$w_1, w_2, w_3, w_4$在一元语法、二元语法和三元语法中的概率分别为\n\n\n$$\n\n\\begin{aligned}\nP(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\\\\nP(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_2) P(w_4 \\mid w_3) ,\\\\\nP(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_2, w_3) .\n\\end{aligned}\n\n$$\n\n\n当$n$较小时，$n$元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当$n$较大时，$n$元语法需要计算并存储大量的词频和多词相邻频率。"},{"metadata":{"id":"641481B68C28432B972A8AD3A6A490E2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"**n元语法存在的缺陷：**\n\n> 1. 参数空间过大\n> 2. 数据稀疏"},{"metadata":{"id":"45AFF2003F6F427DBEBF2A82C0C5DFFF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"63282\n想要有直升机\n想要和你飞到宇宙去\n想要和你融化在一起\n融化在宇宙里\n我每天每天每\n","name":"stdout"}],"source":"with open('/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt') as f:\n    corpus_chars = f.read()\nprint(len(corpus_chars))\nprint(corpus_chars[: 40])\ncorpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\ncorpus_chars = corpus_chars[: 10000]","execution_count":18},{"metadata":{"id":"D535DAED6B3F429B8171FA0CC70073D8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"1027\nchars: 想要有直升机 想要和你飞到宇宙去 想要和\nindices: [323, 485, 966, 660, 244, 38, 373, 323, 485, 285, 798, 44, 302, 235, 320, 23, 373, 323, 485, 285]\n","name":"stdout"}],"source":"idx_to_char = list(set(corpus_chars)) # 去重，得到索引到字符的映射\nchar_to_idx = {char: i for i, char in enumerate(idx_to_char)} # 字符到索引的映射\nvocab_size = len(char_to_idx)\nprint(vocab_size)\n\ncorpus_indices = [char_to_idx[char] for char in corpus_chars]  # 将每个字符转化为索引，得到一个索引的序列\nsample = corpus_indices[: 20]\nprint('chars:', ''.join([idx_to_char[idx] for idx in sample]))\nprint('indices:', sample)","execution_count":19},{"metadata":{"id":"381EF319A898435B85F7E6024881E955","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def load_data_jay_lyrics():\n    with open('/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt') as f:\n        corpus_chars = f.read()\n    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n    corpus_chars = corpus_chars[0:10000]\n    idx_to_char = list(set(corpus_chars))\n    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n    vocab_size = len(char_to_idx)\n    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n    return corpus_indices, char_to_idx, idx_to_char, vocab_size","execution_count":20},{"metadata":{"id":"76C36E236B8B42178A1C8CDF567E669D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 时序数据的采样\n\n在训练中我们需要每次随机读取小批量样本和标签。与之前章节的实验数据不同的是，时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”，即$X$=“想要有直升”，$Y$=“要有直升机”。\n\n现在我们考虑序列“想要有直升机，想要和你飞到宇宙去”，如果时间步数为5，有以下可能的样本和标签：\n* $X$：“想要有直升”，$Y$：“要有直升机”\n* $X$：“要有直升机”，$Y$：“有直升机，”\n* $X$：“有直升机，”，$Y$：“直升机，想”\n* ...\n* $X$：“要和你飞到”，$Y$：“和你飞到宇”\n* $X$：“和你飞到宇”，$Y$：“你飞到宇宙”\n* $X$：“你飞到宇宙”，$Y$：“飞到宇宙去”\n\n可以看到，如果序列的长度为$T$，时间步数为$n$，那么一共有$T-n$个合法的样本，但是这些样本有大量的重合，我们通常采用更加高效的采样方式。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。\n\n### 随机采样\n\n下面的代码每次从数据里随机采样一个小批量。其中批量大小`batch_size`是每个小批量的样本数，`num_steps`是每个样本所包含的时间步数。\n在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。"},{"metadata":{"id":"D7794B64715B4F6D875782800F7F22E3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import torch\nimport random\ndef data_iter_random(corpus_indices, batch_size, num_steps, device=None):\n    # 减1是因为对于长度为n的序列，X最多只有包含其中的前n - 1个字符\n    num_examples = (len(corpus_indices) - 1) // num_steps  # 下取整，得到不重叠情况下的样本个数\n    example_indices = [i * num_steps for i in range(num_examples)]  # 每个样本的第一个字符在corpus_indices中的下标\n    random.shuffle(example_indices)\n\n    def _data(i):\n        # 返回从i开始的长为num_steps的序列\n        return corpus_indices[i: i + num_steps]\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    for i in range(0, num_examples, batch_size):\n        # 每次选出batch_size个随机样本\n        batch_indices = example_indices[i: i + batch_size]  # 当前batch的各个样本的首字符的下标\n        X = [_data(j) for j in batch_indices]\n        Y = [_data(j + 1) for j in batch_indices]\n        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)","execution_count":21},{"metadata":{"id":"D0ACA9EED5004DD48BFD85DB0DC36CA3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"X:  tensor([[18, 19, 20, 21, 22, 23],\n        [ 0,  1,  2,  3,  4,  5]]) \nY: tensor([[19, 20, 21, 22, 23, 24],\n        [ 1,  2,  3,  4,  5,  6]]) \n\nX:  tensor([[12, 13, 14, 15, 16, 17],\n        [ 6,  7,  8,  9, 10, 11]]) \nY: tensor([[13, 14, 15, 16, 17, 18],\n        [ 7,  8,  9, 10, 11, 12]]) \n\n","name":"stdout"}],"source":"my_seq = list(range(30))\nfor X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):\n    print('X: ', X, '\\nY:', Y, '\\n')","execution_count":22},{"metadata":{"id":"075D06F6F27C4EF3977A6E83FE5A67E3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"> **随机采样的特点：**\n- 每个样本是原始序列上`任意截取`的`一段序列`，`相邻`的两个随机小批量在`原始序列上`的位置`不一定相邻`。"},{"metadata":{"id":"C3CD9B52EB3B45878D039EBB57EA3389","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 相邻采样\n\n在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻。"},{"metadata":{"id":"442EF9F6D5464CE0B91E442107B1A21B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    corpus_len = len(corpus_indices) // batch_size * batch_size  # 保留下来的序列的长度\n    corpus_indices = corpus_indices[: corpus_len]  # 仅保留前corpus_len个字符\n    indices = torch.tensor(corpus_indices, device=device)\n    indices = indices.view(batch_size, -1)  # resize成(batch_size, )\n    batch_num = (indices.shape[1] - 1) // num_steps\n    for i in range(batch_num):\n        i = i * num_steps\n        X = indices[:, i: i + num_steps]\n        Y = indices[:, i + 1: i + num_steps + 1]\n        yield X, Y","execution_count":23},{"metadata":{"id":"13D617DCD87D42348A6D0EFB63D0921B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"**同样的设置下，打印相邻采样每次读取的小批量样本的输入`X`和标签`Y`。相邻的两个随机小批量在原始序列上的位置相毗邻。**"},{"metadata":{"id":"346E3FEFE596424B84FD840CBA9289EB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"X:  tensor([[ 0,  1,  2,  3,  4,  5],\n        [15, 16, 17, 18, 19, 20]]) \nY: tensor([[ 1,  2,  3,  4,  5,  6],\n        [16, 17, 18, 19, 20, 21]]) \n\nX:  tensor([[ 6,  7,  8,  9, 10, 11],\n        [21, 22, 23, 24, 25, 26]]) \nY: tensor([[ 7,  8,  9, 10, 11, 12],\n        [22, 23, 24, 25, 26, 27]]) \n\n","name":"stdout"}],"source":"for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):\n    print('X: ', X, '\\nY:', Y, '\\n')","execution_count":24},{"metadata":{"id":"5CB7590971204B4586E6C5866AD0C660","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"> **相邻采样的特点：**\n- `相邻`的两个随机小批量在`原始序列上`的位置`相邻`。"},{"metadata":{"id":"43AE2CF3D1AB4302A99281E83DE65B03","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 小结作业练习"},{"metadata":{"id":"E141F73D4D6D4A2686B40D9A1CAB027D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"（1）. 简述n元语法模型的原理与应用\n\n（2）. 包含4个词的文本序列的概率为：\n$$\nP\\left(w_{1}, w_{2}, w_{3}, w_{4}\\right)=P\\left(w_{1}\\right) P\\left(w_{2} | w_{1}\\right) P\\left(w_{3} | w_{1}, w_{2}\\right) P\\left(w_{4} | w_{1}, w_{2}, w_{3}\\right)\n$$\n当 `n = 3` 时，基于`n−1`阶马尔科夫链，该概率表达可以改写为：\n$$\nP\\left(w_{1}, w_{2}, w_{3}, w_{4}\\right)=P\\left(w_{1}\\right) P\\left(w_{2} | w_{1}\\right) P\\left(w_{3} | w_{1}, w_{2}\\right) P\\left(w_{4} | w_{2}, w_{3}\\right)\n$$\n\n（3）. 下列关于随机采样的描述中错误的是（）\nA. 训练数据中的每个字符最多可以出现在一个样本中\nB. 每个小批量包含的样本数是`batch_size`，每个样本的长度为`num_steps`\nC. 在一个样本中，前后字符是连续的\nD. 前一个小批量数据和后一个小批量数据是连续的\n> 解析：随机采样中前后批量中的数据是不连续的。\n\n（4）. 给定训练数据`[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`，批量大小为`batch_size=2`，时间步数为2，使用本节课的实现方法进行相邻采样，第二个批量为多少？\n> 解析：\n因为训练数据中总共有11个样本，而批量大小为2，所以数据集会被拆分成2段，每段包含5个样本：[0, 1, 2, 3, 4]和[5, 6, 7, 8, 9]，而时间步数为2，所以第二个批量为[2, 3]和[7, 8]。"},{"metadata":{"id":"931439283D064409ADFDAA5334B781C4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 循环神经网络基础\n- **基于循环神经网络的语言模型**\n- **从零开始的实现循环神经网络**\n- **循环神经网络与简洁实现**"},{"metadata":{"id":"A61245A7DE4546298D61054FED7D343B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"> 本节介绍循环神经网络，下图展示了如何基于循环神经网络实现语言模型。我们的目的是基于当前的输入与过去的输入序列，预测序列的下一个字符。循环神经网络引入一个隐藏变量$H$，用$H_{t}$表示$H$在时间步$t$的值。$H_{t}$的计算基于$X_{t}$和$H_{t-1}$，可以认为$H_{t}$记录了到当前字符为止的序列信息，利用$H_{t}$对序列的下一个字符进行预测。\n![Image Name](https://cdn.kesci.com/upload/image/q5jkm0v44i.png?imageView2/0/w/640/h/640)"},{"metadata":{"id":"22DF4E8C04AE4A3F820CBC196BDDA5F8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 循环神经网络的构造\n\n我们先看循环神经网络的具体构造。假设$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$是时间步$t$的小批量输入，$\\boldsymbol{H}_t  \\in \\mathbb{R}^{n \\times h}$是该时间步的隐藏变量，则：\n\n\n$$\n\\boldsymbol{H}_t = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}  + \\boldsymbol{b}_h).\n$$\n\n\n其中，$\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$，$\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$，$\\boldsymbol{b}_{h} \\in \\mathbb{R}^{1 \\times h}$，$\\phi$函数是非线性激活函数。由于引入了$\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}$，$H_{t}$能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。由于$H_{t}$的计算基于$H_{t-1}$，上式的计算是循环的，使用循环计算的网络即循环神经网络（recurrent neural network）。\n\n在时间步$t$，输出层的输出为：\n\n\n$$\n\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q.\n$$\n\n\n其中$\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{h \\times q}$，$\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$。"},{"metadata":{"id":"F33B0A902A1049FB8402C8F8A9746799","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 代码复现\n\n- 从零开始实现循环神经网络\n- 循环神经网络的简介实现"},{"metadata":{"id":"7CE453BEE1BA4D02A6549165FDE3B572","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 从零开始实现循环神经网络\n我们先尝试从零开始实现一个基于字符级循环神经网络的语言模型，这里我们使用周杰伦的歌词作为语料，首先我们读入数据："},{"metadata":{"id":"95DAD1C148664175B573C91A0B6B4F4D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import torch\nimport torch.nn as nn\nimport time\nimport math\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2l_jay9460 as d2l\n(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":26},{"metadata":{"id":"100A25D0395748EA84176C00EC8376B7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### one-hot向量\n\n我们需要将字符表示成向量，这里采用one-hot向量。假设词典大小是$N$，每次字符对应一个从$0$到$N-1$的唯一的索引，则该字符的向量是一个长度为$N$的向量，若字符的索引是$i$，则该向量的第$i$个位置为$1$，其他位置为$0$。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。"},{"metadata":{"id":"0F593091EE274CB88BB274B5F119F2F4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 1.,  ..., 0., 0., 0.]])\ntorch.Size([2, 1027])\ntensor([1., 1.])\n","name":"stdout"}],"source":"def one_hot(x, n_class, dtype=torch.float32):\n    result = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)  # shape: (n, n_class)\n    result.scatter_(1, x.long().view(-1, 1), 1)  # result[i, x[i, 0]] = 1\n    return result\n    \nx = torch.tensor([0, 2])\nx_one_hot = one_hot(x, vocab_size)\nprint(x_one_hot)\nprint(x_one_hot.shape)\nprint(x_one_hot.sum(axis=1))","execution_count":27},{"metadata":{"id":"91961047DECB4C3A9B96E74A573B09D1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"我们每次采样的小批量的形状是（批量大小, 时间步数）。下面的函数将这样的小批量变换成数个形状为（批量大小, 词典大小）的矩阵，矩阵个数等于时间步数。也就是说，时间步$t$的输入为$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$，其中$n$为批量大小，$d$为词向量大小，即one-hot向量长度（词典大小）。"},{"metadata":{"id":"ADD180D5B3CD4AD0A3137F0D3C6F91BC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"5 torch.Size([2, 1027])\n","name":"stdout"}],"source":"def to_onehot(X, n_class):\n    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]\n\nX = torch.arange(10).view(2, 5)\ninputs = to_onehot(X, vocab_size)\nprint(len(inputs), inputs[0].shape)","execution_count":28},{"metadata":{"id":"3C64F86130704FD4A57E6EB1DF41E89F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 初始化模型参数"},{"metadata":{"id":"5B3E31E52A1545B2A0C7056B408DE361","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n# num_inputs: d\n# num_hiddens: h, 隐藏单元的个数是超参数\n# num_outputs: q\n\ndef get_params():\n    def _one(shape):\n        param = torch.zeros(shape, device=device, dtype=torch.float32)\n        nn.init.normal_(param, 0, 0.01)\n        return torch.nn.Parameter(param)\n\n    # 隐藏层参数\n    W_xh = _one((num_inputs, num_hiddens))\n    W_hh = _one((num_hiddens, num_hiddens))\n    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device))\n    # 输出层参数\n    W_hq = _one((num_hiddens, num_outputs))\n    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device))\n    return (W_xh, W_hh, b_h, W_hq, b_q)","execution_count":29},{"metadata":{"id":"28137311C0A644918F87FE8835A24BA5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 定义模型\n\n函数`rnn`用循环的方式依次完成循环神经网络每个时间步的计算。\n"},{"metadata":{"id":"6FC2A8E68BD248758C4AF126180FB054","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def rnn(inputs, state, params):\n    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n    W_xh, W_hh, b_h, W_hq, b_q = params\n    H, = state\n    outputs = []\n    for X in inputs:\n        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n        Y = torch.matmul(H, W_hq) + b_q\n        outputs.append(Y)\n    return outputs, (H,)","execution_count":30},{"metadata":{"id":"0F959DA6435C4A2E8D9238A5E4737612","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 函数init_rnn_state初始化隐藏变量，这里的返回值是一个元组\ndef init_rnn_state(batch_size, num_hiddens, device):\n    return (torch.zeros((batch_size, num_hiddens), device=device), )","execution_count":31},{"metadata":{"id":"56C1E49D2A264510BA7BEC4490388850","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"torch.Size([2, 5])\n256\n1027\n5 torch.Size([2, 1027])\n5 torch.Size([2, 1027])\n1 torch.Size([2, 256])\n1 torch.Size([2, 256])\n","name":"stdout"}],"source":"# 测试输出结果的个数（时间步数），以及第一个时间步的输出层输出的形状和隐藏状态的形状\nprint(X.shape)\nprint(num_hiddens)\nprint(vocab_size)\nstate = init_rnn_state(X.shape[0], num_hiddens, device)\ninputs = to_onehot(X.to(device), vocab_size)\nparams = get_params()\noutputs, state_new = rnn(inputs, state, params)\nprint(len(inputs), inputs[0].shape)\nprint(len(outputs), outputs[0].shape)\nprint(len(state), state[0].shape)\nprint(len(state_new), state_new[0].shape)","execution_count":32},{"metadata":{"id":"03DDDE72CD394B2A8A822EE254413F88","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 裁剪梯度\n\n循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。裁剪梯度（clip gradient）是一种应对梯度爆炸的方法。假设我们把所有模型参数的梯度拼接成一个向量 $\\boldsymbol{g}$，并设裁剪的阈值是$\\theta$。裁剪后的梯度\n\n\n$$\n \\min\\left(\\frac{\\theta}{\\|\\boldsymbol{g}\\|}, 1\\right)\\boldsymbol{g}\n$$\n\n\n的$L_2$范数不超过$\\theta$。\n"},{"metadata":{"id":"5E5849E385AD4330880C187C383CC403","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def grad_clipping(params, theta, device):\n    norm = torch.tensor([0.0], device=device)\n    for param in params:\n        norm += (param.grad.data ** 2).sum()\n    norm = norm.sqrt().item()\n    if norm > theta:\n        for param in params:\n            param.grad.data *= (theta / norm)","execution_count":33},{"metadata":{"id":"77E8C354CA3442068193B1AAEB9080A8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 定义预测函数\n\n以下函数基于前缀`prefix`（含有数个字符的字符串）来预测接下来的`num_chars`个字符。这个函数稍显复杂，其中我们将循环神经单元`rnn`设置成了函数参数，这样在后面小节介绍其他循环神经网络时能重复使用这个函数。\n"},{"metadata":{"id":"1DC83C640A2A4B2C827ECA96BCFA4637","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):\n    state = init_rnn_state(1, num_hiddens, device)\n    output = [char_to_idx[prefix[0]]]   # output记录prefix加上预测的num_chars个字符\n    for t in range(num_chars + len(prefix) - 1):\n        # 将上一时间步的输出作为当前时间步的输入\n        X = to_onehot(torch.tensor([[output[-1]]], device=device), vocab_size)\n        # 计算输出和更新隐藏状态\n        (Y, state) = rnn(X, state, params)\n        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\n        if t < len(prefix) - 1:\n            output.append(char_to_idx[prefix[t + 1]])\n        else:\n            output.append(Y[0].argmax(dim=1).item())\n    return ''.join([idx_to_char[i] for i in output])","execution_count":34},{"metadata":{"id":"7BDB305E39D34AA38EF997CD5FDEDAE9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"我们先测试一下`predict_rnn`函数。我们将根据前缀“分开”创作长度为10个字符（不考虑前缀长度）的一段歌词。因为模型参数为随机值，所以预测结果也是随机的。"},{"metadata":{"id":"7E1D97CB827C4F8B8EED9EC2D73422E8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'分开目田市楷人地惹难颗软'"},"transient":{},"execution_count":35}],"source":"predict_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n            device, idx_to_char, char_to_idx)","execution_count":35},{"metadata":{"id":"FC3E4CEC2E4E452F89B2A958CB34AE97","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 困惑度\n\n我们通常使用困惑度（perplexity）来评价语言模型的好坏。回忆一下[“softmax回归”](../chapter_deep-learning-basics/softmax-regression.ipynb)一节中交叉熵损失函数的定义。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，\n\n* 最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；\n* 最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；\n* 基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。\n\n显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典大小`vocab_size`。\n\n### 定义模型训练函数\n\n跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：\n\n1. 使用困惑度评价模型。\n2. 在迭代模型参数前裁剪梯度。\n3. 对时序数据采用不同采样方法将导致隐藏状态初始化的不同。"},{"metadata":{"id":"35EE620A0AD648268F292BE82AE072E7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n                          vocab_size, device, corpus_indices, idx_to_char,\n                          char_to_idx, is_random_iter, num_epochs, num_steps,\n                          lr, clipping_theta, batch_size, pred_period,\n                          pred_len, prefixes):\n    if is_random_iter:\n        data_iter_fn = d2l.data_iter_random\n    else:\n        data_iter_fn = d2l.data_iter_consecutive\n    params = get_params()\n    loss = nn.CrossEntropyLoss()\n\n    for epoch in range(num_epochs):\n        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态\n            state = init_rnn_state(batch_size, num_hiddens, device)\n        l_sum, n, start = 0.0, 0, time.time()\n        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)\n        for X, Y in data_iter:\n            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态\n                state = init_rnn_state(batch_size, num_hiddens, device)\n            else:  # 否则需要使用detach函数从计算图分离隐藏状态\n                for s in state:\n                    s.detach_()\n            # inputs是num_steps个形状为(batch_size, vocab_size)的矩阵\n            inputs = to_onehot(X, vocab_size)\n            # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\n            (outputs, state) = rnn(inputs, state, params)\n            # 拼接之后形状为(num_steps * batch_size, vocab_size)\n            outputs = torch.cat(outputs, dim=0)\n            # Y的形状是(batch_size, num_steps)，转置后再变成形状为\n            # (num_steps * batch_size,)的向量，这样跟输出的行一一对应\n            y = torch.flatten(Y.T)\n            # 使用交叉熵损失计算平均分类误差\n            l = loss(outputs, y.long())\n            \n            # 梯度清0\n            if params[0].grad is not None:\n                for param in params:\n                    param.grad.data.zero_()\n            l.backward()\n            grad_clipping(params, clipping_theta, device)  # 裁剪梯度\n            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\n            l_sum += l.item() * y.shape[0]\n            n += y.shape[0]\n\n        if (epoch + 1) % pred_period == 0:\n            print('epoch %d, perplexity %f, time %.2f sec' % (\n                epoch + 1, math.exp(l_sum / n), time.time() - start))\n            for prefix in prefixes:\n                print(' -', predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,\n                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))","execution_count":36},{"metadata":{"id":"721D0DDEA4E547128707E8DC9F88B532","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 训练模型并创作歌词\n\n现在我们可以训练模型了。首先，设置模型超参数。我们将根据前缀“分开”和“不分开”分别创作长度为50个字符（不考虑前缀长度）的一段歌词。我们每过50个迭代周期便根据当前训练的模型创作一段歌词。"},{"metadata":{"id":"47FA6109A85840E793BA793F47D0EA77","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\npred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']","execution_count":37},{"metadata":{"id":"A53F2FDB0EB4466381A51471AE5B5666","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 50, perplexity 66.704400, time 0.79 sec\n - 分开 我想要这爱 我不要再不 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我\n - 不分开  我想你这里 我不要的可 我不要这不 我不要再不 我不要再想 我不要再想 我不要再想 我不要再想 \nepoch 100, perplexity 9.884122, time 0.86 sec\n - 分开 我想想这样牵着你的手不放开 爱可不能我 谁你的没有 我 说要你的单车 我想要你的微笑每天都能 在我\n - 不分开吗 我不能再想 我不能 想情我的爱画就像 卷明的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的\nepoch 150, perplexity 2.933500, time 0.60 sec\n - 分开 一直在停留 谁话它停留 这里在角落 不里什么走 不里什么走 不里什么走 不里什么走 不里什么走 不\n - 不分开吗 我不能再想 我不 我不 我不要 爱情走的太快就像龙卷风 不能承受我已无处可躲 我不要再想 我不要\nepoch 200, perplexity 1.577487, time 0.62 sec\n - 分开 一直在停留 谁让它停留 这里什么落 不被横著走 到里什么奇怪的事都有 包括像猫的狗 印地安老斑鸠 \n - 不分开期 我不能再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我已无处可躲 我不要再想 我不要\nepoch 250, perplexity 1.283051, time 0.68 sec\n - 分开 一只心停心仪的母斑鸠 牛仔红蕃 在小镇 背对背决斗 一只灰狼 问候的空去 让制茶  全箱梦空出秋 \n - 不分开扫把的胖女巫 用拉丁文念咒语啦啦呜 她养的黑猫笑起来像哭 啦啦啦呜 情原我一 这迷有吗后嘛这样 何必\n","name":"stdout"}],"source":"# 采用随机采样训练模型\ntrain_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n                      vocab_size, device, corpus_indices, idx_to_char,\n                      char_to_idx, True, num_epochs, num_steps, lr,\n                      clipping_theta, batch_size, pred_period, pred_len,\n                      prefixes)","execution_count":38},{"metadata":{"id":"55E6A17E5AF14D6D8B4A2174FE7E3F02","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 50, perplexity 62.739496, time 0.61 sec\n - 分开 我想要这 你你了双 我有一空 我有我的 爱女人 我想就这 在谁了双 我有一空 我有我的 爱女人 我\n - 不分开 别要我有 你谁一直 我有你的 爱颗我的 快谁一直 在谁了空 我有一空 我有我的 爱女人 我想就这 \nepoch 100, perplexity 7.572520, time 0.92 sec\n - 分开 娘子我 别怪我 我想就你样经堡 我想要你的微笑每天都能看到  我知道这里很美但家乡的你更美 只坏风\n - 不分开柳 再样经一个秋 后哼哈兮 快使用人截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 \nepoch 150, perplexity 2.101958, time 0.88 sec\n - 分开 在养我 谁地安的我 一我却红 你你手种 恨的梦空 我想家 连一我 印子安那我有无头一躲戏人 娘子我\n - 不分开觉 会样经很开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生\nepoch 200, perplexity 1.315253, time 0.83 sec\n - 分开 什么我 不地神枪手 巫师 他念念 有词的 对酋长下诅咒 还我骷髅头 相故事 告诉我 印地安的传说 \n - 不分开觉透会 飞去云神多得去 我那你有多 用对再直些 有种拽反落 这里什么奇怪的事都有 包括像猫的狗 印地\nepoch 250, perplexity 1.176141, time 0.69 sec\n - 分开 问候我 谁地神的我 一双翅红 你面的梦步出动抱 她是下午了鼻子衫多b 印在那窗鸠 会学人开口 仙人\n - 不分开觉 为成么空不起 塞天哈兮 快使用双截棍 哼哼哈兮 如果我有轻功 飞檐走壁 为人耿直不屈 一身正气 \n","name":"stdout"}],"source":"# 相邻采样训练模型\ntrain_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n                      vocab_size, device, corpus_indices, idx_to_char,\n                      char_to_idx, False, num_epochs, num_steps, lr,\n                      clipping_theta, batch_size, pred_period, pred_len,\n                      prefixes)","execution_count":39},{"metadata":{"id":"9565DE9B90B0460DA49635C336E47428","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 循环神经网络的简介实现\n\n### 定义模型\n\n我们使用Pytorch中的`nn.RNN`来构造循环神经网络。在本节中，我们主要关注`nn.RNN`的以下几个构造函数参数：\n\n* `input_size` - The number of expected features in the input x\n* `hidden_size` – The number of features in the hidden state h\n* `nonlinearity` – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n* `batch_first` – If True, then the input and output tensors are provided as (batch_size, num_steps, input_size). Default: False\n\n这里的`batch_first`决定了输入的形状，我们使用默认的参数`False`，对应的输入形状是 (num_steps, batch_size, input_size)。\n\n`forward`函数的参数为：\n\n* `input` of shape (num_steps, batch_size, input_size): tensor containing the features of the input sequence. \n* `h_0` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n\n`forward`函数的返回值是：\n\n* `output` of shape (num_steps, batch_size, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.\n* `h_n` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the hidden state for t = num_steps.\n\n现在我们构造一个`nn.RNN`实例，并用一个简单的例子来看一下输出的形状。"},{"metadata":{"id":"C579A795FF07411A861BE9DC85683317","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"torch.Size([35, 2, 256]) torch.Size([1, 2, 256])\n","name":"stdout"}],"source":"rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)\nnum_steps, batch_size = 35, 2\nX = torch.rand(num_steps, batch_size, vocab_size)\nstate = None\nY, state_new = rnn_layer(X, state)\nprint(Y.shape, state_new.shape)","execution_count":40},{"metadata":{"id":"037C164EB9494F0F940245045010A0AB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 完整的基于循环神经网络的语言模型。"},{"metadata":{"id":"50FD1170C5D6471A8F6B8B916154687E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class RNNModel(nn.Module):\n    def __init__(self, rnn_layer, vocab_size):\n        super(RNNModel, self).__init__()\n        self.rnn = rnn_layer\n        self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1) \n        self.vocab_size = vocab_size\n        self.dense = nn.Linear(self.hidden_size, vocab_size)\n\n    def forward(self, inputs, state):\n        # inputs.shape: (batch_size, num_steps)\n        X = to_onehot(inputs, vocab_size)\n        X = torch.stack(X)  # X.shape: (num_steps, batch_size, vocab_size)\n        hiddens, state = self.rnn(X, state)\n        hiddens = hiddens.view(-1, hiddens.shape[-1])  # hiddens.shape: (num_steps * batch_size, hidden_size)\n        output = self.dense(hiddens)\n        return output, state","execution_count":41},{"metadata":{"id":"88782BC0AB3B4988819D2A66F34CA50A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 预测函数的实现"},{"metadata":{"id":"2B4753B583D745C099A353FCD5F98363","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def predict_rnn_pytorch(prefix, num_chars, model, vocab_size, device, idx_to_char,\n                      char_to_idx):\n    state = None\n    output = [char_to_idx[prefix[0]]]  # output记录prefix加上预测的num_chars个字符\n    for t in range(num_chars + len(prefix) - 1):\n        X = torch.tensor([output[-1]], device=device).view(1, 1)\n        (Y, state) = model(X, state)  # 前向计算不需要传入模型参数\n        if t < len(prefix) - 1:\n            output.append(char_to_idx[prefix[t + 1]])\n        else:\n            output.append(Y.argmax(dim=1).item())\n    return ''.join([idx_to_char[i] for i in output])","execution_count":42},{"metadata":{"id":"CB04A1E002A1443696C4C5D3E87263E9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'分开拳滩攻滩攻滩攻滩攻滩'"},"transient":{},"execution_count":43}],"source":"# 使用权重为随机值的模型来预测一次\nmodel = RNNModel(rnn_layer, vocab_size).to(device)\npredict_rnn_pytorch('分开', 10, model, vocab_size, device, idx_to_char, char_to_idx)","execution_count":43},{"metadata":{"id":"04C2B3A348FE4A6889AE01D00F85AEEE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 采用相邻采样实现训练函数\ndef train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n                                corpus_indices, idx_to_char, char_to_idx,\n                                num_epochs, num_steps, lr, clipping_theta,\n                                batch_size, pred_period, pred_len, prefixes):\n    loss = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    model.to(device)\n    for epoch in range(num_epochs):\n        l_sum, n, start = 0.0, 0, time.time()\n        data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # 相邻采样\n        state = None\n        for X, Y in data_iter:\n            if state is not None:\n                # 使用detach函数从计算图分离隐藏状态\n                if isinstance (state, tuple): # LSTM, state:(h, c)  \n                    state[0].detach_()\n                    state[1].detach_()\n                else: \n                    state.detach_()\n            (output, state) = model(X, state) # output.shape: (num_steps * batch_size, vocab_size)\n            y = torch.flatten(Y.T)\n            l = loss(output, y.long())\n            \n            optimizer.zero_grad()\n            l.backward()\n            grad_clipping(model.parameters(), clipping_theta, device)\n            optimizer.step()\n            l_sum += l.item() * y.shape[0]\n            n += y.shape[0]\n        \n\n        if (epoch + 1) % pred_period == 0:\n            print('epoch %d, perplexity %f, time %.2f sec' % (\n                epoch + 1, math.exp(l_sum / n), time.time() - start))\n            for prefix in prefixes:\n                print(' -', predict_rnn_pytorch(\n                    prefix, pred_len, model, vocab_size, device, idx_to_char,\n                    char_to_idx))","execution_count":44},{"metadata":{"id":"77A17E89D7EE4F6284E6910CB0D1DA1A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 训练模型"},{"metadata":{"id":"8F440D1CDF5745E184BE002FAEA479B7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 50, perplexity 10.658928, time 0.53 sec\n - 分开不是我不不再我 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 \n - 不分开不我 说你在我 不的我我 你不要  不要再想想我 你你你  不要再想想我 想你你 我想要 你不要  \nepoch 100, perplexity 1.289569, time 0.52 sec\n - 分开不是很久了吧? 我 一场悲剧 我想要这辈子注定一个人演戏 最后再一个人慢慢的回忆 没有了过去 我将往\n - 不分开不了我让了其不你的爱 有说的神 手 家以的 我将上的抽色 随 从发了一个人我后出再 我打多 温著的没\nepoch 150, perplexity 1.065523, time 0.52 sec\n - 分开不是很久了吧  我的认真败的黑色幽默 在的完美主义 太彻底 让我连恨都难以下笔 将真心抽离写成日记 \n - 不分开不了像让她神 过 是 娘子我旧起再想要再这样打我妈妈 难道你手不会痛吗 不要再这样打我妈妈 难道你手\nepoch 200, perplexity 1.033195, time 0.59 sec\n - 分开不会像语 风 一直玫酒 你想你都有 我妈妈妈多 就你 没说你说 是你听让我面红 可爱你人 坏坏的让我\n - 不分开不了像让她神 出著  一颗四颗三颗四颗 连成线乘著风 游荡在蓝天边 一片云掉落在我面前 捏成你的形状\nepoch 250, perplexity 1.021002, time 0.48 sec\n - 分开不会像约翰脸 像aby  你的世界已狂风暴雨 Wu  爱情来的太快就像龙卷风 离不开暴风圈来不及逃 \n - 不分开不了像让她神 出发对 几色开口吴侬软语的姑 在吹着头发飘动落牵着你的手 一阵风名 前 我感你很疲倦离\n","name":"stdout"}],"source":"num_epochs, batch_size, lr, clipping_theta = 250, 32, 1e-3, 1e-2\npred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\ntrain_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n                            corpus_indices, idx_to_char, char_to_idx,\n                            num_epochs, num_steps, lr, clipping_theta,\n                            batch_size, pred_period, pred_len, prefixes)","execution_count":45},{"metadata":{"id":"E145FDB4CB4E492BBEEACF5958E522FC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 小结作业练习"},{"metadata":{"id":"5355DFF62A84499D823029CE3BB64494","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"（1）关于循环神经网络描述错误的是：\n\nA. 在同一个批量中，处理不同语句用到的模型参数$W_{h}$和$b_{h}$是一样的\nB. 循环神经网络处理一个长度为`T`的输入序列，需要维护`T`组模型参数\nC. 各个时间步的隐藏状态$H_{t}$不能并行计算\nD. 可以认为第`t`个时间步的隐藏状态$H_t$包含截止到第`t`个时间步的序列的历史信息\n> 解析：批量训练的过程中，参数是以批（`batch`）为单位更新的，每个批次内模型的参数都是一样的。循环神经网络通过不断`循环使用同样一组参数`来应对不同长度的序列，故网络的`参数数量与输入序列长度无关`。隐状态$H_t$的值依赖于$H_1, H_2, ... H_t-1$，并不能并行计算。\n\n（2）关于梯度裁剪描述错误的是：\n\nA. 梯度裁剪之后的梯度小于或者等于原梯度\nB. 梯度裁剪是应对梯度爆炸的一种方法\nC. 裁剪之后的梯度`L2`范数小于阈值$\\theta$\nD. 梯度裁剪也是应对梯度消失的一种方法\n> 解析：梯度裁剪是应对梯度爆炸的一种方法，`LSTM`是应对梯度消失的一种方法。\n\n（3）关于困惑度的描述错误的是：\n\nA. 困惑度用来评价语言模型的好坏\nB. 困惑度越低语言模型越好\nC. 有效模型的困惑度应该大于类别个数\n> 解析：一个随机分类模型（基线模型）的困惑度等于分类问题的类别个数，有效模型的困惑度应小于类别个数（换言之，任何一个有效模型的困惑度必须小于类别个数）。\n\n（4）关于采样方法和隐藏状态初始化的描述错误的是：\n\nA. 采用的采样方法不同会导致隐藏状态初始化方式发生变化\nB. 采用相邻采样仅在每个训练周期开始的时候初始化隐藏状态，是因为相邻的两个批量在原始数据上是连续的\nC. 采用随机采样需要在`每个小批量更新前`初始化隐藏状态，是因为每个样本包含完整的时间序列信息\n> 解析：随机采样中每个样本只包含局部的时间序列信息，因为样本不完整所以每个批量需要重新初始化隐藏状态。"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}