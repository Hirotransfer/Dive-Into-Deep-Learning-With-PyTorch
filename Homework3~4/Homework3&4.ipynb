{"cells":[{"metadata":{"id":"72DE9C74ACDE4CBBAE44AD3EA84F7C01","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Homework3 & Homework4\n- Task06：批量归一化和残差网络；凸优化；梯度下降；优化算法进阶\n- Task07：目标检测基础；图像风格迁移；图像分类案例1\n- Task08：图像分类案例2；GAN；DCGAN"},{"metadata":{"id":"922DE5BD3300498F9BB1EEB2A7F88C29","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Task06：批量归一化和残差网络；凸优化；梯度下降；优化算法进阶\n- 批量归一化和残差网络\n- 凸优化\n- 梯度下降\n- 优化算法进阶"},{"metadata":{"id":"08741AD11088469C83904A295BB3D6F1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 批量归一化和残差网络\n- Batch Normalization(BN)\n- ResNet\n- DenseNet"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9B8F749E4C954AD8AFE6B96DD1F8B830","mdEditEnable":false},"source":"## 批量归一化（Batch Normalization）\n#### 对输入的标准化（浅层模型）\n处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。  \n标准化处理输入数据使各个特征的分布相近\n#### 批量归一化（深度模型）\n利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3E15ABE688814DCB811DF2D4D3A0BBDB","mdEditEnable":false},"source":"### 1.对全连接层做批量归一化\n位置：全连接层中的仿射变换和激活函数之间。  \n**全连接：**  \n$$\n\\boldsymbol{x} = \\boldsymbol{W\\boldsymbol{u} + \\boldsymbol{b}} \\\\\n output =\\phi(\\boldsymbol{x})\n $$   \n\n\n**批量归一化：**\n$$ \noutput=\\phi(\\text{BN}(\\boldsymbol{x}))$$\n\n\n$$\n\\boldsymbol{y}^{(i)} = \\text{BN}(\\boldsymbol{x}^{(i)})\n$$\n\n\n$$\n\\boldsymbol{\\mu}_\\mathcal{B} \\leftarrow \\frac{1}{m}\\sum_{i = 1}^{m} \\boldsymbol{x}^{(i)},\n$$ \n$$\n\\boldsymbol{\\sigma}_\\mathcal{B}^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}(\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}_\\mathcal{B})^2,\n$$\n\n\n$$\n\\hat{\\boldsymbol{x}}^{(i)} \\leftarrow \\frac{\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}_\\mathcal{B}}{\\sqrt{\\boldsymbol{\\sigma}_\\mathcal{B}^2 + \\epsilon}},\n$$\n\n这里ϵ > 0是个很小的常数，保证分母大于0\n\n\n$$\n{\\boldsymbol{y}}^{(i)} \\leftarrow \\boldsymbol{\\gamma} \\odot\n\\hat{\\boldsymbol{x}}^{(i)} + \\boldsymbol{\\beta}.\n$$\n\n\n引入可学习参数：拉伸参数γ和偏移参数β。若$\\boldsymbol{\\gamma} = \\sqrt{\\boldsymbol{\\sigma}_\\mathcal{B}^2 + \\epsilon}$和$\\boldsymbol{\\beta} = \\boldsymbol{\\mu}_\\mathcal{B}$，批量归一化无效。\n\n### 2.对卷积层做批量归一化\n位置：卷积计算之后、应在激活函数之前。  \n如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数。\n计算：对单通道，batchsize=m,卷积计算输出=pxq\n对该通道中m×p×q个元素同时做批量归一化,使用相同的均值和方差。\n\n### 3.预测时的批量归一化\n训练：以batch为单位,对每个batch计算均值和方差。  \n预测：用移动平均估算整个训练数据集的样本均值和方差。\n### 从零实现"},{"metadata":{"id":"39F35F2C0FE148B498B49A9E7D5A96C1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"#考虑到本代码中的模型过大，CPU训练较慢，\n#我们还将代码上传了一份到 https://www.kaggle.com/boyuai/boyu-d2l-deepcnn\n#如希望提前使用gpu运行请至kaggle。","execution_count":2},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"96E72C7A9429485180F9B63F7FB07CB9","scrolled":false},"outputs":[],"source":"import time\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport torchvision\nimport sys\nsys.path.append(\"/home/kesci/input/\") \nimport d2lzh1981 as d2l\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n    # 判断当前模式是训练模式还是预测模式\n    if not is_training:\n        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n    else:\n        assert len(X.shape) in (2, 4)\n        if len(X.shape) == 2:\n            # 使用全连接层的情况，计算特征维上的均值和方差\n            mean = X.mean(dim=0)\n            var = ((X - mean) ** 2).mean(dim=0)\n        else:\n            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n            # X的形状以便后面可以做广播运算\n            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n        # 训练模式下用当前的均值和方差做标准化\n        X_hat = (X - mean) / torch.sqrt(var + eps)\n        # 更新移动平均的均值和方差\n        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n        moving_var = momentum * moving_var + (1.0 - momentum) * var\n    Y = gamma * X_hat + beta  # 拉伸和偏移\n    return Y, moving_mean, moving_var"},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"04A65ABC9F20404E83F166F43226873D","scrolled":false},"outputs":[],"source":"class BatchNorm(nn.Module):\n    def __init__(self, num_features, num_dims):\n        super(BatchNorm, self).__init__()\n        if num_dims == 2:\n            shape = (1, num_features) #全连接层输出神经元\n        else:\n            shape = (1, num_features, 1, 1)  #通道数\n        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n        self.gamma = nn.Parameter(torch.ones(shape))\n        self.beta = nn.Parameter(torch.zeros(shape))\n        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n        self.moving_mean = torch.zeros(shape)\n        self.moving_var = torch.zeros(shape)\n\n    def forward(self, X):\n        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n        if self.moving_mean.device != X.device:\n            self.moving_mean = self.moving_mean.to(X.device)\n            self.moving_var = self.moving_var.to(X.device)\n        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false\n        Y, self.moving_mean, self.moving_var = batch_norm(self.training, \n            X, self.gamma, self.beta, self.moving_mean,\n            self.moving_var, eps=1e-5, momentum=0.9)\n        return Y"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4FE611706B7F4C1B8B57182BC1CE450B","mdEditEnable":false},"source":"### 基于LeNet的应用"},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1B18E8F1B4C843E6852A0B50CFFD9C53","scrolled":false},"outputs":[{"output_type":"stream","text":"Sequential(\n  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (1): BatchNorm()\n  (2): Sigmoid()\n  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (5): BatchNorm()\n  (6): Sigmoid()\n  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (8): FlattenLayer()\n  (9): Linear(in_features=256, out_features=120, bias=True)\n  (10): BatchNorm()\n  (11): Sigmoid()\n  (12): Linear(in_features=120, out_features=84, bias=True)\n  (13): BatchNorm()\n  (14): Sigmoid()\n  (15): Linear(in_features=84, out_features=10, bias=True)\n)\n","name":"stdout"}],"source":"net = nn.Sequential(\n            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n            BatchNorm(6, num_dims=4),\n            nn.Sigmoid(),\n            nn.MaxPool2d(2, 2), # kernel_size, stride\n            nn.Conv2d(6, 16, 5),\n            BatchNorm(16, num_dims=4),\n            nn.Sigmoid(),\n            nn.MaxPool2d(2, 2),\n            d2l.FlattenLayer(),\n            nn.Linear(16*4*4, 120),\n            BatchNorm(120, num_dims=2),\n            nn.Sigmoid(),\n            nn.Linear(120, 84),\n            BatchNorm(84, num_dims=2),\n            nn.Sigmoid(),\n            nn.Linear(84, 10)\n        )\nprint(net)"},{"metadata":{"id":"F11883676ED64A2DB05BC3480AF28BFF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"#batch_size = 256  \n##cpu要调小batchsize\nbatch_size=16\n\ndef load_data_fashion_mnist(batch_size, resize=None, root='/home/kesci/input/FashionMNIST2065'):\n    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n    trans = []\n    if resize:\n        trans.append(torchvision.transforms.Resize(size=resize))\n    trans.append(torchvision.transforms.ToTensor())\n    \n    transform = torchvision.transforms.Compose(trans)\n    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n\n    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=2)\n    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return train_iter, test_iter\ntrain_iter, test_iter = load_data_fashion_mnist(batch_size)","execution_count":5},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9F9DC5C22F5942A48A8A4B656B33D543","scrolled":false},"outputs":[],"source":"lr, num_epochs = 0.001, 5\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nd2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"AC692570D35A492BB82A50E9BBBBB598","mdEditEnable":false},"source":"### 简洁实现"},{"cell_type":"code","metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EAC5ACDF548B4831992A653DF4FD4348","scrolled":false},"outputs":[],"source":"net = nn.Sequential(\n            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n            nn.BatchNorm2d(6),\n            nn.Sigmoid(),\n            nn.MaxPool2d(2, 2), # kernel_size, stride\n            nn.Conv2d(6, 16, 5),\n            nn.BatchNorm2d(16),\n            nn.Sigmoid(),\n            nn.MaxPool2d(2, 2),\n            d2l.FlattenLayer(),\n            nn.Linear(16*4*4, 120),\n            nn.BatchNorm1d(120),\n            nn.Sigmoid(),\n            nn.Linear(120, 84),\n            nn.BatchNorm1d(84),\n            nn.Sigmoid(),\n            nn.Linear(84, 10)\n        )\n\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nd2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)","execution_count":null},{"attachments":{"image.png":{"image/png":"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"}},"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"50F442253C51495181611BB088576C75","mdEditEnable":false},"source":"## 残差网络（ResNet）\n深度学习的问题：深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，准确率也变得更差。\n### 残差块（Residual Block）\n恒等映射：  \n左边：f(x)=x                                                  \n右边：f(x)-x=0 （易于捕捉恒等映射的细微波动）\n\n![Image Name](https://cdn.kesci.com/upload/image/q5l8lhnot4.png?imageView2/0/w/600/h/600)\n\n在残差块中，输⼊可通过跨层的数据线路更快 地向前传播。"},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3F859601FD264D1A8D5E5E51B9D83D0D","scrolled":false},"outputs":[],"source":"class Residual(nn.Module):  # 本类已保存在d2lzh_pytorch包中方便以后使用\n    #可以设定输出通道数、是否使用额外的1x1卷积层来修改通道数以及卷积层的步幅。\n    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\n        super(Residual, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        if use_1x1conv:\n            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, X):\n        Y = F.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3:\n            X = self.conv3(X)\n        return F.relu(Y + X)"},{"cell_type":"code","execution_count":7,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2E2CF8846F5C43618B6DB004FA95BC0E","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([4, 3, 6, 6])"},"transient":{},"execution_count":7}],"source":"blk = Residual(3, 3)\nX = torch.rand((4, 3, 6, 6))\nblk(X).shape # torch.Size([4, 3, 6, 6])"},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C8F6DFFB58344B669C24A18721B8A66F","scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([4, 6, 3, 3])"},"transient":{},"execution_count":8}],"source":"blk = Residual(3, 6, use_1x1conv=True, stride=2)\nblk(X).shape # torch.Size([4, 6, 3, 3])"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CEFB948C07AE428D8F5F756CDE556CF9","mdEditEnable":false},"source":"### ResNet模型\n卷积(64,7x7,3)  \n批量一体化  \n最大池化(3x3,2)  \n\n残差块x4 (通过步幅为2的残差块在每个模块之间减小高和宽)\n\n全局平均池化\n\n全连接"},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6117AE144A6B4EF58B382D31F1C2F760","scrolled":false},"outputs":[],"source":"net = nn.Sequential(\n        nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n        nn.BatchNorm2d(64), \n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D1E8052B1A8242F285BDD3E54418419B","scrolled":false},"outputs":[],"source":"def resnet_block(in_channels, out_channels, num_residuals, first_block=False):\n    if first_block:\n        assert in_channels == out_channels # 第一个模块的通道数同输入通道数一致\n    blk = []\n    for i in range(num_residuals):\n        if i == 0 and not first_block:\n            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))\n        else:\n            blk.append(Residual(out_channels, out_channels))\n    return nn.Sequential(*blk)\n\nnet.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\nnet.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\nnet.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\nnet.add_module(\"resnet_block4\", resnet_block(256, 512, 2))"},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"93D7FFC7581F438A82E92D85E3F6D24E","scrolled":false},"outputs":[],"source":"net.add_module(\"global_avg_pool\", d2l.GlobalAvgPool2d()) # GlobalAvgPool2d的输出: (Batch, 512, 1, 1)\nnet.add_module(\"fc\", nn.Sequential(d2l.FlattenLayer(), nn.Linear(512, 10))) "},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EE1FD4E1510941AB98B3C8840DA47E44","scrolled":false},"outputs":[{"output_type":"stream","text":"0  output shape:\t torch.Size([1, 64, 112, 112])\n1  output shape:\t torch.Size([1, 64, 112, 112])\n2  output shape:\t torch.Size([1, 64, 112, 112])\n3  output shape:\t torch.Size([1, 64, 56, 56])\nresnet_block1  output shape:\t torch.Size([1, 64, 56, 56])\nresnet_block2  output shape:\t torch.Size([1, 128, 28, 28])\nresnet_block3  output shape:\t torch.Size([1, 256, 14, 14])\nresnet_block4  output shape:\t torch.Size([1, 512, 7, 7])\nglobal_avg_pool  output shape:\t torch.Size([1, 512, 1, 1])\nfc  output shape:\t torch.Size([1, 10])\n","name":"stdout"}],"source":"X = torch.rand((1, 1, 224, 224))\nfor name, layer in net.named_children():\n    X = layer(X)\n    print(name, ' output shape:\\t', X.shape)"},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7A68C8B9480C4DB8935436EC136068F1","scrolled":false},"outputs":[],"source":"lr, num_epochs = 0.001, 5\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nd2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"},{"attachments":{"1576638871%281%29.png":{"image/png":"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"}},"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"610FDA7246E64E6AAA8669EEDEA33BB1","mdEditEnable":false},"source":"## 稠密连接网络（DenseNet）\n\n![Image Name](https://cdn.kesci.com/upload/image/q5l8mi78yz.png?imageView2/0/w/600/h/600)\n\n###主要构建模块：  \n稠密块（dense block）： 定义了输入和输出是如何连结的。  \n过渡层（transition layer）：用来控制通道数，使之不过大。\n### 稠密块"},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"644303248ACD46C9886553FB41750A4F","scrolled":false},"outputs":[],"source":"def conv_block(in_channels, out_channels):\n    blk = nn.Sequential(nn.BatchNorm2d(in_channels), \n                        nn.ReLU(),\n                        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n    return blk\n\nclass DenseBlock(nn.Module):\n    def __init__(self, num_convs, in_channels, out_channels):\n        super(DenseBlock, self).__init__()\n        net = []\n        for i in range(num_convs):\n            in_c = in_channels + i * out_channels\n            net.append(conv_block(in_c, out_channels))\n        self.net = nn.ModuleList(net)\n        self.out_channels = in_channels + num_convs * out_channels # 计算输出通道数\n\n    def forward(self, X):\n        for blk in self.net:\n            Y = blk(X)\n            X = torch.cat((X, Y), dim=1)  # 在通道维上将输入和输出连结\n        return X"},{"cell_type":"code","execution_count":14,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"72F1294D9DDD454D9831BCA94764B9DE","scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([4, 23, 8, 8])"},"transient":{},"execution_count":14}],"source":"blk = DenseBlock(2, 3, 10)\nX = torch.rand(4, 3, 8, 8)\nY = blk(X)\nY.shape # torch.Size([4, 23, 8, 8])"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"23FDE43573EA40389189D2D4659A59D9","mdEditEnable":false},"source":"### 过渡层\n$1\\times1$卷积层：来减小通道数  \n步幅为2的平均池化层：减半高和宽"},{"cell_type":"code","execution_count":15,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"ED891AD5805040F6A93AD06B1E7C0FA8","scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([4, 10, 4, 4])"},"transient":{},"execution_count":15}],"source":"def transition_block(in_channels, out_channels):\n    blk = nn.Sequential(\n            nn.BatchNorm2d(in_channels), \n            nn.ReLU(),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n            nn.AvgPool2d(kernel_size=2, stride=2))\n    return blk\n\nblk = transition_block(23, 10)\nblk(Y).shape # torch.Size([4, 10, 4, 4])"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"AE6DADC26CE04D2C86F49867321D491F","mdEditEnable":false},"source":"### DenseNet模型"},{"cell_type":"code","execution_count":16,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E32E30B6657E4CB5B3027B663927AA83","scrolled":false},"outputs":[],"source":"net = nn.Sequential(\n        nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n        nn.BatchNorm2d(64), \n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8464B9866CBF4EB280381A63A9F4557E","scrolled":false},"outputs":[],"source":"num_channels, growth_rate = 64, 32  # num_channels为当前的通道数\nnum_convs_in_dense_blocks = [4, 4, 4, 4]\n\nfor i, num_convs in enumerate(num_convs_in_dense_blocks):\n    DB = DenseBlock(num_convs, num_channels, growth_rate)\n    net.add_module(\"DenseBlosk_%d\" % i, DB)\n    # 上一个稠密块的输出通道数\n    num_channels = DB.out_channels\n    # 在稠密块之间加入通道数减半的过渡层\n    if i != len(num_convs_in_dense_blocks) - 1:\n        net.add_module(\"transition_block_%d\" % i, transition_block(num_channels, num_channels // 2))\n        num_channels = num_channels // 2"},{"cell_type":"code","execution_count":18,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"76C89C4331674B8C8F531F4157CACAB1","scrolled":false},"outputs":[{"output_type":"stream","text":"0  output shape:\t torch.Size([1, 64, 48, 48])\n1  output shape:\t torch.Size([1, 64, 48, 48])\n2  output shape:\t torch.Size([1, 64, 48, 48])\n3  output shape:\t torch.Size([1, 64, 24, 24])\nDenseBlosk_0  output shape:\t torch.Size([1, 192, 24, 24])\ntransition_block_0  output shape:\t torch.Size([1, 96, 12, 12])\nDenseBlosk_1  output shape:\t torch.Size([1, 224, 12, 12])\ntransition_block_1  output shape:\t torch.Size([1, 112, 6, 6])\nDenseBlosk_2  output shape:\t torch.Size([1, 240, 6, 6])\ntransition_block_2  output shape:\t torch.Size([1, 120, 3, 3])\nDenseBlosk_3  output shape:\t torch.Size([1, 248, 3, 3])\nBN  output shape:\t torch.Size([1, 248, 3, 3])\nrelu  output shape:\t torch.Size([1, 248, 3, 3])\nglobal_avg_pool  output shape:\t torch.Size([1, 248, 1, 1])\nfc  output shape:\t torch.Size([1, 10])\n","name":"stdout"}],"source":"net.add_module(\"BN\", nn.BatchNorm2d(num_channels))\nnet.add_module(\"relu\", nn.ReLU())\nnet.add_module(\"global_avg_pool\", d2l.GlobalAvgPool2d()) # GlobalAvgPool2d的输出: (Batch, num_channels, 1, 1)\nnet.add_module(\"fc\", nn.Sequential(d2l.FlattenLayer(), nn.Linear(num_channels, 10))) \n\nX = torch.rand((1, 1, 96, 96))\nfor name, layer in net.named_children():\n    X = layer(X)\n    print(name, ' output shape:\\t', X.shape)"},{"cell_type":"code","execution_count":20,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F70449A99F984956B8BEC295A53E0A99","scrolled":false},"outputs":[],"source":"#batch_size = 256\nbatch_size=16\n# 如出现“out of memory”的报错信息，可减小batch_size或resize\ntrain_iter, test_iter =load_data_fashion_mnist(batch_size, resize=96)\nlr, num_epochs = 0.001, 5\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nd2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"},{"metadata":{"id":"3309EF2E8F36401882653BED9972EAF4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 凸优化\n> 深度学习中的优化问题和凸性介绍\n- 优化与深度学习\n- 凸性"},{"metadata":{"id":"3E4FDAD5D63B45888F5DAD0FCAF0B6F8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 优化与深度学习\n**优化与估计**\n\n尽管优化方法可以最小化深度学习中的损失函数值，但本质上优化方法达到的目标与深度学习的目标并不相同。\n- 优化方法目标：训练集损失函数值\n- 深度学习目标：测试集损失函数值（泛化性）"},{"metadata":{"id":"15F2E2FFE7E844B482FA13D737D7B7AE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"%matplotlib inline\nimport sys\nsys.path.append('/home/kesci/input')\nimport d2lzh1981 as d2l\nfrom mpl_toolkits import mplot3d # 三维画图\nimport numpy as np","execution_count":null},{"metadata":{"id":"9CE85ECA81444C66AB4F384B70E88A13","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def f(x): return x * np.cos(np.pi * x)\ndef g(x): return f(x) + 0.2 * np.cos(5 * np.pi * x)\n\nd2l.set_figsize((5, 3))\nx = np.arange(0.5, 1.5, 0.01)\nfig_f, = d2l.plt.plot(x, f(x),label=\"train error\")\nfig_g, = d2l.plt.plot(x, g(x),'--', c='purple', label=\"test error\")\nfig_f.axes.annotate('empirical risk', (1.0, -1.2), (0.5, -1.1),arrowprops=dict(arrowstyle='->'))\nfig_g.axes.annotate('expected risk', (1.1, -1.05), (0.95, -0.5),arrowprops=dict(arrowstyle='->'))\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('risk')\nd2l.plt.legend(loc=\"upper right\")","execution_count":null},{"metadata":{"id":"FB7BBA1132B84245867D282C1241949B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 优化在深度学习中的挑战\n1. 局部最小值\n2. 鞍点\n3. 梯度消失"},{"metadata":{"id":"21371675214B4C698FD4580789FD4E77","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 局部最小值\n\n$$\nf(x) = x\\cos \\pi x\n$$\n"},{"metadata":{"id":"EB351317991C4A7E81BABD2614EFA39F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def f(x):\n    return x * np.cos(np.pi * x)\n\nd2l.set_figsize((4.5, 2.5))\nx = np.arange(-1.0, 2.0, 0.1)\nfig,  = d2l.plt.plot(x, f(x))\nfig.axes.annotate('local minimum', xy=(-0.3, -0.25), xytext=(-0.77, -1.0),\n                  arrowprops=dict(arrowstyle='->'))\nfig.axes.annotate('global minimum', xy=(1.1, -0.95), xytext=(0.6, 0.8),\n                  arrowprops=dict(arrowstyle='->'))\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('f(x)');","execution_count":null},{"metadata":{"id":"70D68D815B454F1E89885620B553DF5F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 鞍点"},{"metadata":{"id":"A2F1263DDF544E30869638B11D76C75E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"x = np.arange(-2.0, 2.0, 0.1)\nfig, = d2l.plt.plot(x, x**3)\nfig.axes.annotate('saddle point', xy=(0, -0.2), xytext=(-0.52, -5.0),\n                  arrowprops=dict(arrowstyle='->'))\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('f(x)');","execution_count":null},{"metadata":{"id":"EB41DADF03E9426C9DA4EB28F94897BB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"x, y = np.mgrid[-1: 1: 31j, -1: 1: 31j]\nz = x**2 - y**2\n\nd2l.set_figsize((6, 4))\nax = d2l.plt.figure().add_subplot(111, projection='3d')\nax.plot_wireframe(x, y, z, **{'rstride': 2, 'cstride': 2})\nax.plot([0], [0], [0], 'ro', markersize=10)\nticks = [-1,  0, 1]\nd2l.plt.xticks(ticks)\nd2l.plt.yticks(ticks)\nax.set_zticks(ticks)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('y');","execution_count":null},{"metadata":{"id":"E23B5E409648421A856404A3009E1FF9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 梯度消失"},{"metadata":{"id":"BF90EE1B6AB848CD8F8E25A3C1A7E4F6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"x = np.arange(-2.0, 5.0, 0.01)\nfig, = d2l.plt.plot(x, np.tanh(x))\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('f(x)')\nfig.axes.annotate('vanishing gradient', (4, 1), (2, 0.0) ,arrowprops=dict(arrowstyle='->'))","execution_count":null},{"metadata":{"id":"2EA06B369D64499CB280C17F4BE4AAF3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 凸性 （Convexity）"},{"metadata":{"id":"2F82F4CD21584B89923687EF50E44CAB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 基础\n### 集合\n![Image Name](https://cdn.kesci.com/upload/image/q5p1yhqzm8.png?imageView2/0/w/640/h/640)\n![Image Name](https://cdn.kesci.com/upload/image/q5p1xz9gvz.png?imageView2/0/w/640/h/640)\n![Image Name](https://cdn.kesci.com/upload/image/q5p1yue9fu.png?imageView2/0/w/320/h/320)"},{"metadata":{"id":"ACE0E94F7246486684F78BCED83C54F4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 函数"},{"metadata":{"id":"58FB3D98079940268F6436CF74FA396D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def f(x):\n    return 0.5 * x**2  # Convex\n\ndef g(x):\n    return np.cos(np.pi * x)  # Nonconvex\n\ndef h(x):\n    return np.exp(0.5 * x)  # Convex\n\nx, segment = np.arange(-2, 2, 0.01), np.array([-1.5, 1])\nd2l.use_svg_display()\n_, axes = d2l.plt.subplots(1, 3, figsize=(9, 3))\n\nfor ax, func in zip(axes, [f, g, h]):\n    ax.plot(x, func(x))\n    ax.plot(segment, func(segment),'--', color=\"purple\")\n    # d2l.plt.plot([x, segment], [func(x), func(segment)], axes=ax)","execution_count":null},{"metadata":{"id":"0D1002003EC74408B68F5D6A6F4400C3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### Jensen 不等式\n\n$$\n\\sum_{i} \\alpha_{i} f\\left(x_{i}\\right) \\geq f\\left(\\sum_{i} \\alpha_{i} x_{i}\\right) \\text { and } E_{x}[f(x)] \\geq f\\left(E_{x}[x]\\right)\n$$\n"},{"metadata":{"id":"DE8C10D3945B443F9AF4D54E302E69F5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 性质\n1. 无局部极小值\n2. 与凸集的关系\n3. 二阶条件"},{"metadata":{"id":"8C588DA0FA6C432DA38C27030A220983","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"###  无局部最小值\n证明：假设存在 $x \\in X$ 是局部最小值，则存在全局最小值 $x' \\in X$, 使得 $f(x) > f(x')$, 则对 $\\lambda \\in(0,1]$:\n\n$$\nf(x)>\\lambda f(x)+(1-\\lambda) f(x^{\\prime}) \\geq f(\\lambda x+(1-\\lambda) x^{\\prime})\n$$\n"},{"metadata":{"id":"7424CB433F8141BFBCE54673C7EE78E9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 与凸集的关系\n对于凸函数 $f(x)$，定义集合 $S_{b}:=\\{x | x \\in X \\text { and } f(x) \\leq b\\}$，则集合 $S_b$ 为凸集\n\n证明：对于点 $x,x' \\in S_b$, 有 $f\\left(\\lambda x+(1-\\lambda) x^{\\prime}\\right) \\leq \\lambda f(x)+(1-\\lambda) f\\left(x^{\\prime}\\right) \\leq b$, 故 $\\lambda x+(1-\\lambda) x^{\\prime} \\in S_{b}$\n\n$f(x, y)=0.5 x^{2}+\\cos (2 \\pi y)$"},{"metadata":{"id":"E2B352A54D794D5F81F53455A12F44DB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"x, y = np.meshgrid(np.linspace(-1, 1, 101), np.linspace(-1, 1, 101),\n                   indexing='ij')\n\nz = x**2 + 0.5 * np.cos(2 * np.pi * y)\n\n# Plot the 3D surface\nd2l.set_figsize((6, 4))\nax = d2l.plt.figure().add_subplot(111, projection='3d')\nax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})\nax.contour(x, y, z, offset=-1)\nax.set_zlim(-1, 1.5)\n\n# Adjust labels\nfor func in [d2l.plt.xticks, d2l.plt.yticks, ax.set_zticks]:\n    func([-1, 0, 1])","execution_count":null},{"metadata":{"id":"95B46EBC682C45F4921D79F0D48CE160","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 凸函数与二阶导数"},{"metadata":{"id":"79DE2CC7E68B4C9295E832C04834ED74","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"$f^{''}(x) \\ge 0 \\Longleftrightarrow f(x)$ 是凸函数\n\n**必要性 ($\\Leftarrow$):**\n\n对于凸函数：\n\n$$\n\\frac{1}{2} f(x+\\epsilon)+\\frac{1}{2} f(x-\\epsilon) \\geq f\\left(\\frac{x+\\epsilon}{2}+\\frac{x-\\epsilon}{2}\\right)=f(x)\n$$\n\n故:\n\n$$\nf^{\\prime \\prime}(x)=\\lim _{\\varepsilon \\rightarrow 0} \\frac{\\frac{f(x+\\epsilon) - f(x)}{\\epsilon}-\\frac{f(x) - f(x-\\epsilon)}{\\epsilon}}{\\epsilon}\n$$\n\n\n$$\nf^{\\prime \\prime}(x)=\\lim _{\\varepsilon \\rightarrow 0} \\frac{f(x+\\epsilon)+f(x-\\epsilon)-2 f(x)}{\\epsilon^{2}} \\geq 0\n$$\n\n\n**充分性 ($\\Rightarrow$):**\n\n令 $a < x < b$ 为 $f(x)$ 上的三个点，由拉格朗日中值定理:\n\n$$\n\\begin{array}{l}{f(x)-f(a)=(x-a) f^{\\prime}(\\alpha) \\text { for some } \\alpha \\in[a, x] \\text { and }} \\\\ {f(b)-f(x)=(b-x) f^{\\prime}(\\beta) \\text { for some } \\beta \\in[x, b]}\\end{array}\n$$\n\n\n根据单调性，有 $f^{\\prime}(\\beta) \\geq f^{\\prime}(\\alpha)$, 故:\n\n$$\n\\begin{aligned} f(b)-f(a) &=f(b)-f(x)+f(x)-f(a) \\\\ &=(b-x) f^{\\prime}(\\beta)+(x-a) f^{\\prime}(\\alpha) \\\\ & \\geq(b-a) f^{\\prime}(\\alpha) \\end{aligned}\n$$"},{"metadata":{"id":"48A6C82D58C545ED85321A5A7641FBAC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 限制条件\n\n### 拉格朗日乘子法\n[Boyd & Vandenberghe, 2004](https://d2l.ai/chapter_references/zreferences.html#boyd-vandenberghe-2004)"},{"metadata":{"id":"394D4E966DA64FA8A12907DFC4CD2B09","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 惩罚项\n\n欲使 $c_i(x) \\leq 0$, 将项 $\\alpha_ic_i(x)$ 加入目标函数，如多层感知机章节中的 $\\frac{\\lambda}{2} ||w||^2$"},{"metadata":{"id":"D8C25D569A214F309A5B94856856A71B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 投影\n\n\n$$\n\\operatorname{Proj}_{X}(\\mathbf{x})=\\underset{\\mathbf{x}^{\\prime} \\in X}{\\operatorname{argmin}}\\left\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\right\\|_{2}\n$$\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5p241skoc.png?imageView2/0/w/640/h/640)"},{"metadata":{"id":"40C9DCD3F38C4734830E71155CD135AD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 梯度下降 \n（[Boyd & Vandenberghe, 2004](https://d2l.ai/chapter_references/zreferences.html#boyd-vandenberghe-2004)）\n- 梯度下降\n- 随机梯度下降\n- 小批量梯度下降"},{"metadata":{"id":"463A8320F52C4E6E8BB29EC5D30BCF3A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"%matplotlib inline\nimport numpy as np\nimport torch\nimport time\nfrom torch import nn, optim\nimport math\nimport sys\nsys.path.append('/home/kesci/input')\nimport d2lzh1981 as d2l","execution_count":null},{"metadata":{"id":"DAD50977F195458C85DA12AF9037CBFA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 一维梯度下降\n\n**证明：沿梯度反方向移动自变量可以减小函数值**\n\n泰勒展开：\n\n$$\nf(x+\\epsilon)=f(x)+\\epsilon f^{\\prime}(x)+\\mathcal{O}\\left(\\epsilon^{2}\\right)\n$$\n\n代入沿梯度方向的移动量 $\\eta f^{\\prime}(x)$：\n\n$$\nf\\left(x-\\eta f^{\\prime}(x)\\right)=f(x)-\\eta f^{\\prime 2}(x)+\\mathcal{O}\\left(\\eta^{2} f^{\\prime 2}(x)\\right)\n$$\n\n$$\nf\\left(x-\\eta f^{\\prime}(x)\\right) \\lesssim f(x)\n$$\n\n\n$$\nx \\leftarrow x-\\eta f^{\\prime}(x)\n$$\n"},{"metadata":{"id":"933D155C17A046879DAD63A11431B11A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def f(x):\n    return x**2  # Objective function\n\ndef gradf(x):\n    return 2 * x  # Its derivative\n\ndef gd(eta):\n    x = 10\n    results = [x]\n    for i in range(10):\n        x -= eta * gradf(x)\n        results.append(x)\n    print('epoch 10, x:', x)\n    return results\n\nres = gd(0.2)","execution_count":null},{"metadata":{"id":"51EE145BDE994A95AFB448D13CD981CC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def show_trace(res):\n    n = max(abs(min(res)), abs(max(res)))\n    f_line = np.arange(-n, n, 0.01)\n    d2l.set_figsize((3.5, 2.5))\n    d2l.plt.plot(f_line, [f(x) for x in f_line],'-')\n    d2l.plt.plot(res, [f(x) for x in res],'-o')\n    d2l.plt.xlabel('x')\n    d2l.plt.ylabel('f(x)')\n    \n\nshow_trace(res)","execution_count":null},{"metadata":{"id":"7B0386C7E78D4F7280B88BB597290691","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 学习率\nshow_trace(gd(0.05))","execution_count":null},{"metadata":{"id":"EF61D2DF595745AD842500AAEF7D8D6C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"show_trace(gd(1.1))","execution_count":null},{"metadata":{"id":"CDDE608B7B784C3B85266E4F089D92E9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 局部极小值"},{"metadata":{"id":"767AFCCCD47E42198D1030F29187B258","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"c = 0.15 * np.pi\n\ndef f(x):\n    return x * np.cos(c * x)\n\ndef gradf(x):\n    return np.cos(c * x) - c * x * np.sin(c * x)\n\nshow_trace(gd(2))","execution_count":null},{"metadata":{"id":"2F0292817C1A413FBDD18CABE786AEE3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 多维梯度下降"},{"metadata":{"id":"5A1B1072268349CE99397DCD102913E5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def train_2d(trainer, steps=20):\n    x1, x2 = -5, -2\n    results = [(x1, x2)]\n    for i in range(steps):\n        x1, x2 = trainer(x1, x2)\n        results.append((x1, x2))\n    print('epoch %d, x1 %f, x2 %f' % (i + 1, x1, x2))\n    return results\n\ndef show_trace_2d(f, results): \n    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')\n    x1, x2 = np.meshgrid(np.arange(-5.5, 1.0, 0.1), np.arange(-3.0, 1.0, 0.1))\n    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n    d2l.plt.xlabel('x1')\n    d2l.plt.ylabel('x2')","execution_count":null},{"metadata":{"id":"23AC873347D94EE184B05C64B212105A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"eta = 0.1\n\ndef f_2d(x1, x2):  # 目标函数\n    return x1 ** 2 + 2 * x2 ** 2\n\ndef gd_2d(x1, x2):\n    return (x1 - eta * 2 * x1, x2 - eta * 4 * x2)\n\nshow_trace_2d(f_2d, train_2d(gd_2d))","execution_count":null},{"metadata":{"id":"8AEF2BDDC20047458409AEB88678B1C6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 自适应方法"},{"metadata":{"id":"5CB09C3745334BB28574910E08429379","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 牛顿法\n\n在 $x + \\epsilon$ 处泰勒展开：\n\n$$\nf(\\mathbf{x}+\\epsilon)=f(\\mathbf{x})+\\epsilon^{\\top} \\nabla f(\\mathbf{x})+\\frac{1}{2} \\epsilon^{\\top} \\nabla \\nabla^{\\top} f(\\mathbf{x}) \\epsilon+\\mathcal{O}\\left(\\|\\epsilon\\|^{3}\\right)\n$$\n\n最小值点处满足: $\\nabla f(\\mathbf{x})=0$, 即我们希望 $\\nabla f(\\mathbf{x} + \\epsilon)=0$, 对上式关于 $\\epsilon$ 求导，忽略高阶无穷小，有：\n\n$$\n\\nabla f(\\mathbf{x})+\\boldsymbol{H}_{f} \\boldsymbol{\\epsilon}=0 \\text { and hence } \\epsilon=-\\boldsymbol{H}_{f}^{-1} \\nabla f(\\mathbf{x})\n$$\n"},{"metadata":{"id":"E4AFC0D1DDE34A9AA83882E1A60E782B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"c = 0.5\n\ndef f(x):\n    return np.cosh(c * x)  # Objective\n\ndef gradf(x):\n    return c * np.sinh(c * x)  # Derivative\n\ndef hessf(x):\n    return c**2 * np.cosh(c * x)  # Hessian\n\n# Hide learning rate for now\ndef newton(eta=1):\n    x = 10\n    results = [x]\n    for i in range(10):\n        x -= eta * gradf(x) / hessf(x)\n        results.append(x)\n    print('epoch 10, x:', x)\n    return results\n\nshow_trace(newton())","execution_count":null},{"metadata":{"id":"87FFEDF787D24BECB4BDE74412E6D83B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"c = 0.15 * np.pi\n\ndef f(x):\n    return x * np.cos(c * x)\n\ndef gradf(x):\n    return np.cos(c * x) - c * x * np.sin(c * x)\n\ndef hessf(x):\n    return - 2 * c * np.sin(c * x) - x * c**2 * np.cos(c * x)\n\nshow_trace(newton())","execution_count":null},{"metadata":{"id":"396C8EEE0359495F8D22DFD8FD95E574","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"show_trace(newton(0.5))","execution_count":null},{"metadata":{"id":"C9C25CA6656548738EBFDE54A6E16D78","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 收敛性分析\n\n只考虑在函数为凸函数, 且最小值点上 $f''(x^*) > 0$ 时的收敛速度：\n\n令 $x_k$ 为第 $k$ 次迭代后 $x$ 的值， $e_{k}:=x_{k}-x^{*}$ 表示 $x_k$ 到最小值点 $x^{*}$ 的距离，由 $f'(x^{*}) = 0$:\n\n$$\n0=f^{\\prime}\\left(x_{k}-e_{k}\\right)=f^{\\prime}\\left(x_{k}\\right)-e_{k} f^{\\prime \\prime}\\left(x_{k}\\right)+\\frac{1}{2} e_{k}^{2} f^{\\prime \\prime \\prime}\\left(\\xi_{k}\\right) \\text{for some } \\xi_{k} \\in\\left[x_{k}-e_{k}, x_{k}\\right]\n$$\n\n两边除以 $f''(x_k)$, 有：\n\n$$\ne_{k}-f^{\\prime}\\left(x_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right)=\\frac{1}{2} e_{k}^{2} f^{\\prime \\prime \\prime}\\left(\\xi_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right)\n$$\n\n代入更新方程 $x_{k+1} = x_{k} - f^{\\prime}\\left(x_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right)$, 得到：\n\n$$\nx_k - x^{*} - f^{\\prime}\\left(x_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right) =\\frac{1}{2} e_{k}^{2} f^{\\prime \\prime \\prime}\\left(\\xi_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right)\n$$\n\n\n$$\nx_{k+1} - x^{*} = e_{k+1} = \\frac{1}{2} e_{k}^{2} f^{\\prime \\prime \\prime}\\left(\\xi_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right)\n$$\n\n当 $\\frac{1}{2} f^{\\prime \\prime \\prime}\\left(\\xi_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right) \\leq c$ 时，有:\n\n$$\ne_{k+1} \\leq c e_{k}^{2}\n$$\n"},{"metadata":{"id":"8D9F4F39954C438A944927E1A9907386","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 预处理 （Heissan阵辅助梯度下降）\n\n\n$$\n\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\operatorname{diag}\\left(H_{f}\\right)^{-1} \\nabla \\mathbf{x}\n$$\n"},{"metadata":{"id":"86CF07F78E75462D89635BA9F9FAA674","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 梯度下降与线性搜索（共轭梯度法）"},{"metadata":{"id":"311BFAF2D865429C81BCC560F85931D1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 随机梯度下降"},{"metadata":{"id":"EF261E30241C42E2867E1228F71DFC36","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 随机梯度下降参数更新\n\n对于有 $n$ 个样本对训练数据集，设 $f_i(x)$ 是第 $i$ 个样本的损失函数, 则目标函数为:\n\n$$\nf(\\mathbf{x})=\\frac{1}{n} \\sum_{i=1}^{n} f_{i}(\\mathbf{x})\n$$\n\n其梯度为:\n\n$$\n\\nabla f(\\mathbf{x})=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_{i}(\\mathbf{x})\n$$\n\n使用该梯度的一次更新的时间复杂度为 $\\mathcal{O}(n)$\n\n随机梯度下降更新公式 $\\mathcal{O}(1)$:\n\n$$\n\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\nabla f_{i}(\\mathbf{x})\n$$\n\n且有：\n\n$$\n\\mathbb{E}_{i} \\nabla f_{i}(\\mathbf{x})=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_{i}(\\mathbf{x})=\\nabla f(\\mathbf{x})\n$$\n"},{"metadata":{"id":"D99D34F38FF34BDABF3C4139265A6860","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def f(x1, x2):\n    return x1 ** 2 + 2 * x2 ** 2  # Objective\n\ndef gradf(x1, x2):\n    return (2 * x1, 4 * x2)  # Gradient\n\ndef sgd(x1, x2):  # Simulate noisy gradient\n    global lr  # Learning rate scheduler\n    (g1, g2) = gradf(x1, x2)  # Compute gradient\n    (g1, g2) = (g1 + np.random.normal(0.1), g2 + np.random.normal(0.1))\n    eta_t = eta * lr()  # Learning rate at time t\n    return (x1 - eta_t * g1, x2 - eta_t * g2)  # Update variables\n\neta = 0.1\nlr = (lambda: 1)  # Constant learning rate\nshow_trace_2d(f, train_2d(sgd, steps=50))\n","execution_count":null},{"metadata":{"id":"02FF85D0DAFB46A28D2D94AA6D9597A9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 动态学习率"},{"metadata":{"id":"F88B597038DE44A9A9A4A5FCE0AD88F9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def exponential():\n    global ctr\n    ctr += 1\n    return math.exp(-0.1 * ctr)\n\nctr = 1\nlr = exponential  # Set up learning rate\nshow_trace_2d(f, train_2d(sgd, steps=1000))","execution_count":null},{"metadata":{"id":"46BB50A459394CF3862346D1C1A567DA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def polynomial():\n    global ctr\n    ctr += 1\n    return (1 + 0.1 * ctr)**(-0.5)\n\nctr = 1\nlr = polynomial  # Set up learning rate\nshow_trace_2d(f, train_2d(sgd, steps=50))","execution_count":null},{"metadata":{"id":"D09A647CF3014CF6BB8BEB6B12A68A44","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 小批量随机梯度下降"},{"metadata":{"id":"95D8001117D04FDEBB82978D81F7C759","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 读取数据\n[读取数据](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise)"},{"metadata":{"id":"10347F4365AE48D0805734C974400530","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def get_data_ch7():  # 本函数已保存在d2lzh_pytorch包中方便以后使用\n    data = np.genfromtxt('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t')\n    data = (data - data.mean(axis=0)) / data.std(axis=0) # 标准化\n    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n           torch.tensor(data[:1500, -1], dtype=torch.float32) # 前1500个样本(每个样本5个特征)\n\nfeatures, labels = get_data_ch7()\nfeatures.shape","execution_count":null},{"metadata":{"id":"329D1A314C9F4F599AE86A43DC807C91","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"code","outputs":[],"source":"import pandas as pd\ndf = pd.read_csv('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t', header=None)\ndf.head(10)","execution_count":null},{"metadata":{"id":"B88F056EBE26468E81985AC9CF450A10","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## PyTorch从零开始实现"},{"metadata":{"id":"5BECF6286B25473282F1214BA29D2E99","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def sgd(params, states, hyperparams):\n    for p in params:\n        p.data -= hyperparams['lr'] * p.grad.data","execution_count":null},{"metadata":{"id":"C00C10A99C4B48F58616A5F294C775DD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 本函数已保存在d2lzh_pytorch包中方便以后使用\ndef train_ch7(optimizer_fn, states, hyperparams, features, labels,\n              batch_size=10, num_epochs=2):\n    # 初始化模型\n    net, loss = d2l.linreg, d2l.squared_loss\n    \n    w = torch.nn.Parameter(torch.tensor(np.random.normal(0, 0.01, size=(features.shape[1], 1)), dtype=torch.float32),\n                           requires_grad=True)\n    b = torch.nn.Parameter(torch.zeros(1, dtype=torch.float32), requires_grad=True)\n\n    def eval_loss():\n        return loss(net(features, w, b), labels).mean().item()\n\n    ls = [eval_loss()]\n    data_iter = torch.utils.data.DataLoader(\n        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=True)\n    \n    for _ in range(num_epochs):\n        start = time.time()\n        for batch_i, (X, y) in enumerate(data_iter):\n            l = loss(net(X, w, b), y).mean()  # 使用平均损失\n            \n            # 梯度清零\n            if w.grad is not None:\n                w.grad.data.zero_()\n                b.grad.data.zero_()\n                \n            l.backward()\n            optimizer_fn([w, b], states, hyperparams)  # 迭代模型参数\n            if (batch_i + 1) * batch_size % 100 == 0:\n                ls.append(eval_loss())  # 每100个样本记录下当前训练误差\n    # 打印结果和作图\n    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))\n    d2l.set_figsize()\n    d2l.plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\n    d2l.plt.xlabel('epoch')\n    d2l.plt.ylabel('loss')","execution_count":null},{"metadata":{"id":"D42F2D17ABC54BE98E8EF2CFE2B436D0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def train_sgd(lr, batch_size, num_epochs=2):\n    train_ch7(sgd, None, {'lr': lr}, features, labels, batch_size, num_epochs)","execution_count":null},{"metadata":{"id":"5D2846CD8A3042BEB0C6F065A0768AC7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"train_sgd(1, 1500, 6)","execution_count":null},{"metadata":{"id":"4CFC6232567E4CB68DED331E91B84B7D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"train_sgd(0.005, 1)","execution_count":null},{"metadata":{"id":"F399FD7FA5FD4857BE5D06B1CCEA7F2E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"train_sgd(0.05, 10)","execution_count":null},{"metadata":{"id":"202C0C23E77941FC8B929CE1FC520103","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 简洁实现"},{"metadata":{"id":"314C18A182C343B29DF4B74CE283AC08","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 本函数与原书不同的是这里第一个参数优化器函数而不是优化器的名字\n# 例如: optimizer_fn=torch.optim.SGD, optimizer_hyperparams={\"lr\": 0.05}\ndef train_pytorch_ch7(optimizer_fn, optimizer_hyperparams, features, labels,\n                    batch_size=10, num_epochs=2):\n    # 初始化模型\n    net = nn.Sequential(\n        nn.Linear(features.shape[-1], 1)\n    )\n    loss = nn.MSELoss()\n    optimizer = optimizer_fn(net.parameters(), **optimizer_hyperparams)\n\n    def eval_loss():\n        return loss(net(features).view(-1), labels).item() / 2\n\n    ls = [eval_loss()]\n    data_iter = torch.utils.data.DataLoader(\n        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=True)\n\n    for _ in range(num_epochs):\n        start = time.time()\n        for batch_i, (X, y) in enumerate(data_iter):\n            # 除以2是为了和train_ch7保持一致, 因为squared_loss中除了2\n            l = loss(net(X).view(-1), y) / 2 \n            \n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            if (batch_i + 1) * batch_size % 100 == 0:\n                ls.append(eval_loss())\n    # 打印结果和作图\n    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))\n    d2l.set_figsize()\n    d2l.plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\n    d2l.plt.xlabel('epoch')\n    d2l.plt.ylabel('loss')","execution_count":null},{"metadata":{"id":"3D2E585BA13648AA8EE2D8C821AE96D1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"train_pytorch_ch7(optim.SGD, {\"lr\": 0.05}, features, labels, 10)","execution_count":null},{"metadata":{"id":"8F4FD7F53B07446882F63FACB4B96AFA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 优化算法进阶"},{"metadata":{"id":"DCD1C10CFE5742CEA17A0B97E0E3CDDC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Momentum\n\n在 [SGD](https://d2l.ai/chapter_optimization/sgd.html#sec-sgd) 中，我们提到，目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。对于noisy gradient,我们需要谨慎的选取学习率和batch size, 来控制梯度方差和收敛的结果。\n\n\n$$\n\\mathbf{g}_t = \\partial_{\\mathbf{w}} \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} f(\\mathbf{x}_{i}, \\mathbf{w}_{t-1}) = \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} \\mathbf{g}_{i, t-1}.\n$$"},{"metadata":{"id":"CFFEAD2E8C104D09872FE12EB0135733","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## An ill-conditioned Problem\n\nCondition Number of Hessian Matrix:\n   \n$$\n cond_{H} = \\frac{\\lambda_{max}}{\\lambda_{min}} \n$$\n \nwhere $\\lambda_{max}, \\lambda_{min}$ is the maximum amd minimum eignvalue of Hessian matrix.\n\n让我们考虑一个输入和输出分别为二维向量$\\boldsymbol{x} = [x_1, x_2]^\\top$和标量的目标函数:\n\n\n$$\n f(\\boldsymbol{x})=0.1x_1^2+2x_2^2\n$$\n\n\n\n$$\n cond_{H} = \\frac{4}{0.2} = 20 \\quad \\rightarrow \\quad \\text{ill-conditioned} \n$$\n \n\n## Maximum Learning Rate\n+ For $f(x)$, according to convex optimizaiton conclusions, we need step size $\\eta$.\n+ To guarantee the convergence, we need to have $\\eta$ .\n\n## Supp: Preconditioning\n\n在二阶优化中，我们使用Hessian matrix的逆矩阵(或者pseudo inverse)来左乘梯度向量 $i.e. \\Delta_{x} = H^{-1}\\mathbf{g}$，这样的做法称为precondition，相当于将 $H$ 映射为一个单位矩阵，拥有分布均匀的Spectrum，也即我们去优化的等价标函数的Hessian matrix为良好的identity matrix。\n\n\n与[Section 11.4](https://d2l.ai/chapter_optimization/sgd.html#sec-sgd)一节中不同，这里将$x_1^2$系数从$1$减小到了$0.1$。下面实现基于这个目标函数的梯度下降，并演示使用学习率为$0.4$时自变量的迭代轨迹。"},{"metadata":{"id":"84CDED47B7044EDB8F75FAE778C48C72","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"%matplotlib inline\nimport sys\nsys.path.append(\"/home/kesci/input\") \nimport d2lzh1981 as d2l\nimport torch\n\neta = 0.4\n\ndef f_2d(x1, x2):\n    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n\ndef gd_2d(x1, x2, s1, s2):\n    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\n\nd2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))","execution_count":null},{"metadata":{"id":"EA25C4F593844FAB859F33500865F976","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"可以看到，同一位置上，目标函数在竖直方向（$x_2$轴方向）比在水平方向（$x_1$轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。\n\n下面我们试着将学习率调得稍大一点，此时自变量在竖直方向不断越过最优解并逐渐发散。\n\n### Solution to ill-condition\n+ __Preconditioning gradient vector__: applied in Adam, RMSProp, AdaGrad, Adelta, KFC, Natural gradient and other secord-order optimization algorithms.\n+ __Averaging history gradient__: like momentum, which allows larger learning rates to accelerate convergence; applied in Adam, RMSProp, SGD momentum.  "},{"metadata":{"id":"885DDEE5915B4E56ABB10003A3445C98","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"eta = 0.6\nd2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))","execution_count":null},{"metadata":{"id":"21C10EFD8C494471A969E537C2CF1B0A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Momentum Algorithm\n\n动量法的提出是为了解决梯度下降的上述问题。设时间步 $t$ 的自变量为 $\\boldsymbol{x}_t$，学习率为 $\\eta_t$。\n在时间步 $t=0$，动量法创建速度变量 $\\boldsymbol{m}_0$，并将其元素初始化成 0。在时间步 $t>0$，动量法对每次迭代的步骤做如下修改：\n\n\n$$\n\\begin{aligned}\n\\boldsymbol{m}_t &\\leftarrow \\beta \\boldsymbol{m}_{t-1} + \\eta_t \\boldsymbol{g}_t, \\\\\n\\boldsymbol{x}_t &\\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{m}_t,\n\\end{aligned}\n$$\n\nAnother version:\n\n$$\n\\begin{aligned}\n\\boldsymbol{m}_t &\\leftarrow \\beta \\boldsymbol{m}_{t-1} + (1-\\beta) \\boldsymbol{g}_t, \\\\\n\\boldsymbol{x}_t &\\leftarrow \\boldsymbol{x}_{t-1} - \\alpha_t \\boldsymbol{m}_t,\n\\end{aligned}\n$$\n\n$$\n\\alpha_t = \\frac{\\eta_t}{1-\\beta} \n$$\n\n其中，动量超参数 $\\beta$满足 $0 \\leq \\beta < 1$。当 $\\beta=0$ 时，动量法等价于小批量随机梯度下降。\n\n在解释动量法的数学原理前，让我们先从实验中观察梯度下降在使用动量法后的迭代轨迹。"},{"metadata":{"id":"9B3282BF747543C5838975677CAB3C5F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def momentum_2d(x1, x2, v1, v2):\n    v1 = beta * v1 + eta * 0.2 * x1\n    v2 = beta * v2 + eta * 4 * x2\n    return x1 - v1, x2 - v2, v1, v2\n\neta, beta = 0.4, 0.5\nd2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))","execution_count":null},{"metadata":{"id":"454F6E1434CC4B93805BACFA51061292","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"可以看到使用较小的学习率 $\\eta=0.4$ 和动量超参数 $\\beta=0.5$ 时，动量法在竖直方向上的移动更加平滑，且在水平方向上更快逼近最优解。下面使用较大的学习率 $\\eta=0.6$，此时自变量也不再发散。"},{"metadata":{"id":"F7C27176931E4F00858DC08C282C4F98","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"eta = 0.6\nd2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))","execution_count":null},{"metadata":{"id":"5BF6C25E05FF4584B4B05A1FFA132329","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### Exponential Moving Average\n\n为了从数学上理解动量法，让我们先解释一下指数加权移动平均（exponential moving average）。给定超参数 $0 \\leq \\beta < 1$，当前时间步 $t$ 的变量 $y_t$ 是上一时间步 $t-1$ 的变量 $y_{t-1}$ 和当前时间步另一变量 $x_t$ 的线性组合：\n\n$$\ny_t = \\beta y_{t-1} + (1-\\beta) x_t.\n$$\n\n我们可以对 $y_t$ 展开：\n\n$$\n\\begin{aligned}\ny_t  &= (1-\\beta) x_t + \\beta y_{t-1}\\\\\n         &= (1-\\beta)x_t + (1-\\beta) \\cdot \\beta x_{t-1} + \\beta^2y_{t-2}\\\\\n         &= (1-\\beta)x_t + (1-\\beta) \\cdot \\beta x_{t-1} + (1-\\beta) \\cdot \\beta^2x_{t-2} + \\beta^3y_{t-3}\\\\\n         &= (1-\\beta) \\sum_{i=0}^{t} \\beta^{i}x_{t-i}\n\\end{aligned}\n$$\n\n$$\n(1-\\beta)\\sum_{i=0}^{t} \\beta^{i} = \\frac{1-\\beta^{t}}{1-\\beta} (1-\\beta) = (1-\\beta^{t})\n$$\n\n### Supp\nApproximate Average of $\\frac{1}{1-\\beta}$ Steps\n\n令 $n = 1/(1-\\beta)$，那么 $\\left(1-1/n\\right)^n = \\beta^{1/(1-\\beta)}$。因为\n\n$$\n \\lim_{n \\rightarrow \\infty}  \\left(1-\\frac{1}{n}\\right)^n = \\exp(-1) \\approx 0.3679,\n$$\n\n所以当 $\\beta \\rightarrow 1$时，$\\beta^{1/(1-\\beta)}=\\exp(-1)$，如 $0.95^{20} \\approx \\exp(-1)$。如果把 $\\exp(-1)$ 当作一个比较小的数，我们可以在近似中忽略所有含 $\\beta^{1/(1-\\beta)}$ 和比 $\\beta^{1/(1-\\beta)}$ 更高阶的系数的项。例如，当 $\\beta=0.95$ 时，\n\n$$\ny_t \\approx 0.05 \\sum_{i=0}^{19} 0.95^i x_{t-i}.\n$$\n\n因此，在实际中，我们常常将 $y_t$ 看作是对最近 $1/(1-\\beta)$ 个时间步的 $x_t$ 值的加权平均。例如，当 $\\gamma = 0.95$ 时，$y_t$ 可以被看作对最近20个时间步的 $x_t$ 值的加权平均；当 $\\beta = 0.9$ 时，$y_t$ 可以看作是对最近10个时间步的 $x_t$ 值的加权平均。而且，离当前时间步 $t$ 越近的 $x_t$ 值获得的权重越大（越接近1）。\n\n\n### 由指数加权移动平均理解动量法\n\n现在，我们对动量法的速度变量做变形：\n\n$$\n\\boldsymbol{m}_t \\leftarrow \\beta \\boldsymbol{m}_{t-1} + (1 - \\beta) \\left(\\frac{\\eta_t}{1 - \\beta} \\boldsymbol{g}_t\\right). \n$$\n\nAnother version:\n\n$$\n\\boldsymbol{m}_t \\leftarrow \\beta \\boldsymbol{m}_{t-1} + (1 - \\beta) \\boldsymbol{g}_t. \n$$\n\n\n$$\n\\begin{aligned}\n\\boldsymbol{x}_t &\\leftarrow \\boldsymbol{x}_{t-1} - \\alpha_t \\boldsymbol{m}_t,\n\\end{aligned}\n$$\n\n\n$$\n\\alpha_t = \\frac{\\eta_t}{1-\\beta} \n$$\n\n\n由指数加权移动平均的形式可得，速度变量 $\\boldsymbol{v}_t$ 实际上对序列 $\\{\\eta_{t-i}\\boldsymbol{g}_{t-i} /(1-\\beta):i=0,\\ldots,1/(1-\\beta)-1\\}$ 做了指数加权移动平均。换句话说，相比于小批量随机梯度下降，动量法在每个时间步的自变量更新量近似于将前者对应的最近 $1/(1-\\beta)$ 个时间步的更新量做了指数加权移动平均后再除以 $1-\\beta$。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。在本节之前示例的优化问题中，所有梯度在水平方向上为正（向右），而在竖直方向上时正（向上）时负（向下）。这样，我们就可以使用较大的学习率，从而使自变量向最优解更快移动。\n\n\n## Implement\n\n相对于小批量随机梯度下降，动量法需要对每一个自变量维护一个同它一样形状的速度变量，且超参数里多了动量超参数。实现中，我们将速度变量用更广义的状态变量`states`表示。"},{"metadata":{"id":"8ED4F01C00214A018499562F12A78CF5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def get_data_ch7():  \n    data = np.genfromtxt('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t')\n    data = (data - data.mean(axis=0)) / data.std(axis=0)\n    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n        torch.tensor(data[:1500, -1], dtype=torch.float32)\n\nfeatures, labels = get_data_ch7()\n\ndef init_momentum_states():\n    v_w = torch.zeros((features.shape[1], 1), dtype=torch.float32)\n    v_b = torch.zeros(1, dtype=torch.float32)\n    return (v_w, v_b)\n\ndef sgd_momentum(params, states, hyperparams):\n    for p, v in zip(params, states):\n        v.data = hyperparams['momentum'] * v.data + hyperparams['lr'] * p.grad.data\n        p.data -= v.data","execution_count":null},{"metadata":{"id":"0B15CC4CFD284F5588376C470DBA7556","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"我们先将动量超参数`momentum`设0.5"},{"metadata":{"id":"A9788D0CE1334FC7834857A4EBC07BA0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"d2l.train_ch7(sgd_momentum, init_momentum_states(),\n              {'lr': 0.02, 'momentum': 0.5}, features, labels)","execution_count":null},{"metadata":{"id":"3C42F75CB0BB44BA95A9B3480F1FB011","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"将动量超参数`momentum`增大到0.9"},{"metadata":{"id":"0FB87521E0AC477080A896B753F5C3F1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"d2l.train_ch7(sgd_momentum, init_momentum_states(),\n              {'lr': 0.02, 'momentum': 0.9}, features, labels)","execution_count":null},{"metadata":{"id":"A1885AB91DD04BBBA76F904412B0BEBF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"可见目标函数值在后期迭代过程中的变化不够平滑。直觉上，10倍小批量梯度比2倍小批量梯度大了5倍，我们可以试着将学习率减小到原来的1/5。此时目标函数值在下降了一段时间后变化更加平滑。"},{"metadata":{"id":"F88E4B14D1804D40886FE8C1005BB8A6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"d2l.train_ch7(sgd_momentum, init_momentum_states(),\n              {'lr': 0.004, 'momentum': 0.9}, features, labels)","execution_count":null},{"metadata":{"id":"DECA1B1B93D6481692A8DE7E8EAB1BC7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Pytorch Class\n\n在Pytorch中，```torch.optim.SGD```已实现了Momentum。"},{"metadata":{"id":"E144AD2E9D8F4AEF99A46663E3D3A8C9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"d2l.train_pytorch_ch7(torch.optim.SGD, {'lr': 0.004, 'momentum': 0.9},\n                    features, labels)","execution_count":null},{"metadata":{"id":"4C6673D8E5D34E08BF5297BD0DF81C88","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# AdaGrad\n\n在之前介绍过的优化算法中，目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。举个例子，假设目标函数为$f$，自变量为一个二维向量$[x_1, x_2]^\\top$，该向量中每一个元素在迭代时都使用相同的学习率。例如，在学习率为$\\eta$的梯度下降中，元素$x_1$和$x_2$都使用相同的学习率$\\eta$来自我迭代：\n\n\n$$\n\nx_1 \\leftarrow x_1 - \\eta \\frac{\\partial{f}}{\\partial{x_1}}, \\quad\nx_2 \\leftarrow x_2 - \\eta \\frac{\\partial{f}}{\\partial{x_2}}.\n\n$$\n\n\n在[“动量法”](./momentum.ipynb)一节里我们看到当$x_1$和$x_2$的梯度值有较大差别时，需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。本节我们介绍AdaGrad算法，它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题 [1]。\n\n\n## Algorithm\n\nAdaGrad算法会使用一个小批量随机梯度$\\boldsymbol{g}_t$按元素平方的累加变量$\\boldsymbol{s}_t$。在时间步0，AdaGrad将$\\boldsymbol{s}_0$中每个元素初始化为0。在时间步$t$，首先将小批量随机梯度$\\boldsymbol{g}_t$按元素平方后累加到变量$\\boldsymbol{s}_t$：\n\n\n$$\n\\boldsymbol{s}_t \\leftarrow \\boldsymbol{s}_{t-1} + \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t,\n$$\n\n\n其中$\\odot$是按元素相乘。接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：\n\n\n$$\n\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\frac{\\eta}{\\sqrt{\\boldsymbol{s}_t + \\epsilon}} \\odot \\boldsymbol{g}_t,\n$$\n\n\n其中$\\eta$是学习率，$\\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。\n\n## Feature\n\n需要强调的是，小批量随机梯度按元素平方的累加变量$\\boldsymbol{s}_t$出现在学习率的分母项中。因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。然而，由于$\\boldsymbol{s}_t$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。\n\n下面我们仍然以目标函数$f(\\boldsymbol{x})=0.1x_1^2+2x_2^2$为例观察AdaGrad算法对自变量的迭代轨迹。我们实现AdaGrad算法并使用和上一节实验中相同的学习率0.4。可以看到，自变量的迭代轨迹较平滑。但由于$\\boldsymbol{s}_t$的累加效果使学习率不断衰减，自变量在迭代后期的移动幅度较小。"},{"metadata":{"id":"6916065FDB2B4D5C8D8B8479C0E11B7E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"%matplotlib inline\nimport math\nimport torch\nimport sys\nsys.path.append(\"/home/kesci/input\") \nimport d2lzh1981 as d2l\n\ndef adagrad_2d(x1, x2, s1, s2):\n    g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6  # 前两项为自变量梯度\n    s1 += g1 ** 2\n    s2 += g2 ** 2\n    x1 -= eta / math.sqrt(s1 + eps) * g1\n    x2 -= eta / math.sqrt(s2 + eps) * g2\n    return x1, x2, s1, s2\n\ndef f_2d(x1, x2):\n    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n\neta = 0.4\nd2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))","execution_count":null},{"metadata":{"id":"463433E9099147589C52CAB8DAA48535","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"下面将学习率增大到2。可以看到自变量更为迅速地逼近了最优解。"},{"metadata":{"id":"5C9F7848FAF04187968424B58EEC668D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"eta = 2\nd2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))","execution_count":null},{"metadata":{"id":"21F2BB4A298D4C96AE958ADE51078D5D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Implement\n\n同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。我们根据AdaGrad算法中的公式实现该算法。"},{"metadata":{"id":"394A9C6013D943B2AFF187DEBBE06DB1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def get_data_ch7():  \n    data = np.genfromtxt('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t')\n    data = (data - data.mean(axis=0)) / data.std(axis=0)\n    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n        torch.tensor(data[:1500, -1], dtype=torch.float32)\n        \nfeatures, labels = get_data_ch7()\n\ndef init_adagrad_states():\n    s_w = torch.zeros((features.shape[1], 1), dtype=torch.float32)\n    s_b = torch.zeros(1, dtype=torch.float32)\n    return (s_w, s_b)\n\ndef adagrad(params, states, hyperparams):\n    eps = 1e-6\n    for p, s in zip(params, states):\n        s.data += (p.grad.data**2)\n        p.data -= hyperparams['lr'] * p.grad.data / torch.sqrt(s + eps)","execution_count":null},{"metadata":{"id":"84C88DFC5DBC4D7DBB8896CCFBBFD4C3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 使用更大的学习率来训练模型。\nd2l.train_ch7(adagrad, init_adagrad_states(), {'lr': 0.1}, features, labels)","execution_count":null},{"metadata":{"id":"E9FA1FE3DA8A4DA9995B9FBA9C22BA52","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Pytorch Class\n\n通过名称为“adagrad”的`Trainer`实例，我们便可使用Pytorch提供的AdaGrad算法来训练模型。"},{"metadata":{"id":"39C7CD50D1FD43E4AD138717A5F70E97","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"d2l.train_pytorch_ch7(torch.optim.Adagrad, {'lr': 0.1}, features, labels)","execution_count":null},{"metadata":{"id":"CFB6B2B308DF4CC08C10176D22F9F162","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# RMSProp\n\n我们在[“AdaGrad算法”](adagrad.ipynb)一节中提到，因为调整学习率时分母上的变量$\\boldsymbol{s}_t$一直在累加按元素平方的小批量随机梯度，所以目标函数自变量每个元素的学习率在迭代过程中一直在降低（或不变）。因此，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp算法对AdaGrad算法做了修改。该算法源自Coursera上的一门课程，即“机器学习的神经网络”。\n\n## Algorithm\n\n我们在[“动量法”](momentum.ipynb)一节里介绍过指数加权移动平均。不同于AdaGrad算法里状态变量$\\boldsymbol{s}_t$是截至时间步$t$所有小批量随机梯度$\\boldsymbol{g}_t$按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。具体来说，给定超参数$0 \\leq \\gamma 0$计算\n\n\n$$\n\\boldsymbol{v}_t \\leftarrow \\beta \\boldsymbol{v}_{t-1} + (1 - \\beta) \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t. \n$$\n\n\n和AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量\n\n\n$$\n\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\frac{\\alpha}{\\sqrt{\\boldsymbol{v}_t + \\epsilon}} \\odot \\boldsymbol{g}_t, \n$$\n\n\n其中$\\eta$是学习率，$\\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。因为RMSProp算法的状态变量$\\boldsymbol{s}_t$是对平方项$\\boldsymbol{g}_t \\odot \\boldsymbol{g}_t$的指数加权移动平均，所以可以看作是最近$1/(1-\\beta)$个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。\n\n照例，让我们先观察RMSProp算法对目标函数$f(\\boldsymbol{x})=0.1x_1^2+2x_2^2$中自变量的迭代轨迹。回忆在[“AdaGrad算法”](adagrad.ipynb)一节使用的学习率为0.4的AdaGrad算法，自变量在迭代后期的移动幅度较小。但在同样的学习率下，RMSProp算法可以更快逼近最优解。"},{"metadata":{"id":"01FFC4461A3F4A88AB9FF4A7175C475A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"%matplotlib inline\nimport math\nimport torch\nimport sys\nsys.path.append(\"/home/kesci/input\") \nimport d2lzh1981 as d2l\n\ndef rmsprop_2d(x1, x2, s1, s2):\n    g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6\n    s1 = beta * s1 + (1 - beta) * g1 ** 2\n    s2 = beta * s2 + (1 - beta) * g2 ** 2\n    x1 -= alpha / math.sqrt(s1 + eps) * g1\n    x2 -= alpha / math.sqrt(s2 + eps) * g2\n    return x1, x2, s1, s2\n\ndef f_2d(x1, x2):\n    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n\nalpha, beta = 0.4, 0.9\nd2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))","execution_count":null},{"metadata":{"id":"8F8C28EC7A2C42DD817B4CB58ADE4957","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Implement\n\n接下来按照RMSProp算法中的公式实现该算法。"},{"metadata":{"id":"290603CB52DF4FE1808BF57BD0E6EF24","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def get_data_ch7():  \n    data = np.genfromtxt('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t')\n    data = (data - data.mean(axis=0)) / data.std(axis=0)\n    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n        torch.tensor(data[:1500, -1], dtype=torch.float32)\n        \nfeatures, labels = get_data_ch7()\n\ndef init_rmsprop_states():\n    s_w = torch.zeros((features.shape[1], 1), dtype=torch.float32)\n    s_b = torch.zeros(1, dtype=torch.float32)\n    return (s_w, s_b)\n\ndef rmsprop(params, states, hyperparams):\n    gamma, eps = hyperparams['beta'], 1e-6\n    for p, s in zip(params, states):\n        s.data = gamma * s.data + (1 - gamma) * (p.grad.data)**2\n        p.data -= hyperparams['lr'] * p.grad.data / torch.sqrt(s + eps)","execution_count":null},{"metadata":{"id":"526326C01B234DB19548A261826A5D51","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"我们将初始学习率设为0.01，并将超参数$\\gamma$设为0.9。此时，变量$\\boldsymbol{s}_t$可看作是最近$1/(1-0.9) = 10$个时间步的平方项$\\boldsymbol{g}_t \\odot \\boldsymbol{g}_t$的加权平均。"},{"metadata":{"id":"BE526D1962484B298FB4225D14AC8677","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"d2l.train_ch7(rmsprop, init_rmsprop_states(), {'lr': 0.01, 'beta': 0.9},\n              features, labels)","execution_count":null},{"metadata":{"id":"B8FE4BF1012E428A8523B1BADBFCC233","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Pytorch Class\n\n通过名称为“rmsprop”的`Trainer`实例，我们便可使用Gluon提供的RMSProp算法来训练模型。注意，超参数$\\gamma$通过`gamma1`指定。"},{"metadata":{"id":"BAA999DFD7C243758A0AC2C8874DA180","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"d2l.train_pytorch_ch7(torch.optim.RMSprop, {'lr': 0.01, 'alpha': 0.9},\n                    features, labels)","execution_count":null},{"metadata":{"id":"90FA5B73ED554EE885E9928BEEDF73FD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# AdaDelta\n\n除了RMSProp算法以外，另一个常用优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有用解的问题做了改进 [1]。有意思的是，AdaDelta算法没有学习率这一超参数。\n\n## Algorithm\n\nAdaDelta算法也像RMSProp算法一样，使用了小批量随机梯度$\\boldsymbol{g}_t$按元素平方的指数加权移动平均变量$\\boldsymbol{s}_t$。在时间步0，它的所有元素被初始化为0。给定超参数$0 \\leq \\rho 0$，同RMSProp算法一样计算\n\n\n$$\n\\boldsymbol{s}_t \\leftarrow \\rho \\boldsymbol{s}_{t-1} + (1 - \\rho) \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t. \n$$\n\n\n与RMSProp算法不同的是，AdaDelta算法还维护一个额外的状态变量$\\Delta\\boldsymbol{x}_t$，其元素同样在时间步0时被初始化为0。我们使用$\\Delta\\boldsymbol{x}_{t-1}$来计算自变量的变化量：\n\n\n$$\n \\boldsymbol{g}_t' \\leftarrow \\sqrt{\\frac{\\Delta\\boldsymbol{x}_{t-1} + \\epsilon}{\\boldsymbol{s}_t + \\epsilon}}   \\odot \\boldsymbol{g}_t, \n$$\n\n\n其中$\\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-5}$。接着更新自变量：\n\n\n$$\n\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{g}'_t. \n$$\n\n\n最后，我们使用$\\Delta\\boldsymbol{x}_t$来记录自变量变化量$\\boldsymbol{g}'_t$按元素平方的指数加权移动平均：\n\n\n$$\n\\Delta\\boldsymbol{x}_t \\leftarrow \\rho \\Delta\\boldsymbol{x}_{t-1} + (1 - \\rho) \\boldsymbol{g}'_t \\odot \\boldsymbol{g}'_t. \n$$\n\n\n可以看到，如不考虑$\\epsilon$的影响，AdaDelta算法与RMSProp算法的不同之处在于使用$\\sqrt{\\Delta\\boldsymbol{x}_{t-1}}$来替代超参数$\\eta$。\n\n\n## Implement\n\nAdaDelta算法需要对每个自变量维护两个状态变量，即$\\boldsymbol{s}_t$和$\\Delta\\boldsymbol{x}_t$。我们按AdaDelta算法中的公式实现该算法。"},{"metadata":{"id":"5535757D3FE843E888079E3B25A5E7CA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def init_adadelta_states():\n    s_w, s_b = torch.zeros((features.shape[1], 1), dtype=torch.float32), torch.zeros(1, dtype=torch.float32)\n    delta_w, delta_b = torch.zeros((features.shape[1], 1), dtype=torch.float32), torch.zeros(1, dtype=torch.float32)\n    return ((s_w, delta_w), (s_b, delta_b))\n\ndef adadelta(params, states, hyperparams):\n    rho, eps = hyperparams['rho'], 1e-5\n    for p, (s, delta) in zip(params, states):\n        s[:] = rho * s + (1 - rho) * (p.grad.data**2)\n        g =  p.grad.data * torch.sqrt((delta + eps) / (s + eps))\n        p.data -= g\n        delta[:] = rho * delta + (1 - rho) * g * g","execution_count":null},{"metadata":{"id":"45866F6039C54F7AB57A87FC82FAFA2A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"d2l.train_ch7(adadelta, init_adadelta_states(), {'rho': 0.9}, features, labels)","execution_count":null},{"metadata":{"id":"FEE773280C044BDC8991D871C18CA578","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Pytorch Class\n\n通过名称为“adadelta”的`Trainer`实例，我们便可使用pytorch提供的AdaDelta算法。它的超参数可以通过`rho`来指定。"},{"metadata":{"id":"A5E84BFB46BC4BECB8822AD12CEEF0EB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"d2l.train_pytorch_ch7(torch.optim.Adadelta, {'rho': 0.9}, features, labels)","execution_count":null},{"metadata":{"id":"78B76DAAE6B84D15BDDC249BB3CF27B7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Adam\n\nAdam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均 [1]。下面我们来介绍这个算法。\n\n## Algorithm\n\nAdam算法使用了动量变量$\\boldsymbol{m}_t$和RMSProp算法中小批量随机梯度按元素平方的指数加权移动平均变量$\\boldsymbol{v}_t$，并在时间步0将它们中每个元素初始化为0。给定超参数$0 \\leq \\beta_1 < 1$（算法作者建议设为0.9），时间步$t$的动量变量$\\boldsymbol{m}_t$即小批量随机梯度$\\boldsymbol{g}_t$的指数加权移动平均：\n\n\n$$\n\\boldsymbol{m}_t \\leftarrow \\beta_1 \\boldsymbol{m}_{t-1} + (1 - \\beta_1) \\boldsymbol{g}_t. \n$$\n\n\n和RMSProp算法中一样，给定超参数$0 \\leq \\beta_2 < 1$（算法作者建议设为0.999），\n将小批量随机梯度按元素平方后的项$\\boldsymbol{g}_t \\odot \\boldsymbol{g}_t$做指数加权移动平均得到$\\boldsymbol{v}_t$：\n\n\n$$\n\\boldsymbol{v}_t \\leftarrow \\beta_2 \\boldsymbol{v}_{t-1} + (1 - \\beta_2) \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t. \n$$\n\n\n由于我们将$\\boldsymbol{m}_0$和$\\boldsymbol{s}_0$中的元素都初始化为0，\n在时间步$t$我们得到$\\boldsymbol{m}_t =  (1-\\beta_1) \\sum_{i=1}^t \\beta_1^{t-i} \\boldsymbol{g}_i$。将过去各时间步小批量随机梯度的权值相加，得到 $(1-\\beta_1) \\sum_{i=1}^t \\beta_1^{t-i} = 1 - \\beta_1^t$。需要注意的是，当$t$较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当$\\beta_1 = 0.9$时，$\\boldsymbol{m}_1 = 0.1\\boldsymbol{g}_1$。为了消除这样的影响，对于任意时间步$t$，我们可以将$\\boldsymbol{m}_t$再除以$1 - \\beta_1^t$，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，我们对变量$\\boldsymbol{m}_t$和$\\boldsymbol{v}_t$均作偏差修正：\n\n\n$$\n\\hat{\\boldsymbol{m}}_t \\leftarrow \\frac{\\boldsymbol{m}_t}{1 - \\beta_1^t}, \n$$\n\n\n\n$$\n\\hat{\\boldsymbol{v}}_t \\leftarrow \\frac{\\boldsymbol{v}_t}{1 - \\beta_2^t}. \n$$\n\n\n\n接下来，Adam算法使用以上偏差修正后的变量$\\hat{\\boldsymbol{m}}_t$和$\\hat{\\boldsymbol{m}}_t$，将模型参数中每个元素的学习率通过按元素运算重新调整：\n\n\n$$\n\\boldsymbol{g}_t' \\leftarrow \\frac{\\eta \\hat{\\boldsymbol{m}}_t}{\\sqrt{\\hat{\\boldsymbol{v}}_t} + \\epsilon},\n$$\n\n\n其中$\\eta$是学习率，$\\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-8}$。和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用$\\boldsymbol{g}_t'$迭代自变量：\n\n\n$$\n\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{g}_t'. \n$$\n\n\n## Implement\n\n我们按照Adam算法中的公式实现该算法。其中时间步$t$通过`hyperparams`参数传入`adam`函数。"},{"metadata":{"id":"04D42819BBBC43ACB93AF1CDCC6DDD3A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"%matplotlib inline\nimport torch\nimport sys\nsys.path.append(\"/home/kesci/input\") \nimport d2lzh1981 as d2l\n\ndef get_data_ch7():  \n    data = np.genfromtxt('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t')\n    data = (data - data.mean(axis=0)) / data.std(axis=0)\n    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n        torch.tensor(data[:1500, -1], dtype=torch.float32)\n        \nfeatures, labels = get_data_ch7()\n\ndef init_adam_states():\n    v_w, v_b = torch.zeros((features.shape[1], 1), dtype=torch.float32), torch.zeros(1, dtype=torch.float32)\n    s_w, s_b = torch.zeros((features.shape[1], 1), dtype=torch.float32), torch.zeros(1, dtype=torch.float32)\n    return ((v_w, s_w), (v_b, s_b))\n\ndef adam(params, states, hyperparams):\n    beta1, beta2, eps = 0.9, 0.999, 1e-6\n    for p, (v, s) in zip(params, states):\n        v[:] = beta1 * v + (1 - beta1) * p.grad.data\n        s[:] = beta2 * s + (1 - beta2) * p.grad.data**2\n        v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n        s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n        p.data -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps)\n    hyperparams['t'] += 1","execution_count":null},{"metadata":{"id":"9CEB5B9265E14B81BA4B5D7B1EDCCBFF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"d2l.train_ch7(adam, init_adam_states(), {'lr': 0.01, 't': 1}, features, labels)","execution_count":null},{"metadata":{"id":"85296B34557342398C10512D76103A97","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Pytorch Class"},{"metadata":{"id":"E814038F88AE43E8B8E5467B2C1DDFAA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"d2l.train_pytorch_ch7(torch.optim.Adam, {'lr': 0.01}, features, labels)","execution_count":null},{"metadata":{"id":"FBB46601B0BD4C0DB53573F0C010F17D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"062DCA6444CF4EB38E28F777D484C959","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"AD10322230A045CD8A47DCDF5CF02C02","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"BD7FA38318FF47B799FA34EB39C62417","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"19D9F18429FF42848338A19CAED95874","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Task07：目标检测基础；图像风格迁移；图像分类案例1\n- 目标检测基础\n- 图像风格迁移\n- 图像分类案例1"},{"metadata":{"id":"94828137FC8B4771B928A227D3410815","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 目标检测基础"},{"metadata":{"id":"0AB719365DAB446D8B52336BE6D37EBE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 目标检测和边界框"},{"metadata":{"id":"15BE9FB2807D4D439513B333010DFC8E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"%matplotlib inline\nfrom PIL import Image\n\nimport sys\nsys.path.append('/home/kesci/input/')\nimport d2lzh1981 as d2l","execution_count":null},{"metadata":{"id":"664DF4EB6D6A49418F7B8FB3BB4F6B7B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 展示用于目标检测的图\nd2l.set_figsize()\nimg = Image.open('/home/kesci/input/img2083/img/catdog.jpg')\nd2l.plt.imshow(img); # 加分号只显示图","execution_count":null},{"metadata":{"id":"97E41A6D380F44F6BB3E7E57EF918F9C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 边界框"},{"metadata":{"id":"B7B1DE993D2944898F6A5B7B875FC348","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# bbox是bounding box的缩写\ndog_bbox, cat_bbox = [60, 45, 378, 516], [400, 112, 655, 493]","execution_count":null},{"metadata":{"id":"8EDAE44604DC410B83BD2EE823F3D580","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def bbox_to_rect(bbox, color):  # 本函数已保存在d2lzh_pytorch中方便以后使用\n    # 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：\n    # ((左上x, 左上y), 宽, 高)\n    return d2l.plt.Rectangle(\n        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],\n        fill=False, edgecolor=color, linewidth=2)","execution_count":null},{"metadata":{"id":"485EA81478F9496581B52D242A7F5A7F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"fig = d2l.plt.imshow(img)\nfig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))\nfig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'));","execution_count":null},{"metadata":{"id":"61A68FA911CB4A908E0133E3984B9F70","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 锚框\n\n目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边缘从而更准确地预测目标的真实边界框（ground-truth bounding box）。不同的模型使用的区域采样方法可能不同。这里我们介绍其中的一种方法：它以每个像素为中心生成多个大小和宽高比（aspect ratio）不同的边界框。这些边界框被称为锚框（anchor box）。我们将在后面基于锚框实践目标检测。\n> 注: 建议想学习用PyTorch做检测的童鞋阅读一下仓库[a-PyTorch-Tutorial-to-Object-Detection](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection)。\n\n先导入一下相关包。\n"},{"metadata":{"id":"5F86E773ABB0428E9A7D6F348354D2E2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import numpy as np\nimport math\nimport torch\nimport os\nIMAGE_DIR = '/home/kesci/input/img2083/img/'\nprint(torch.__version__)","execution_count":null},{"metadata":{"id":"F34C102E09224C529A5CD2ADF46BF299","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 生成多个锚框"},{"metadata":{"id":"6432C16E460E44D1B2475A37425924A3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"假设输入图像高为 $h$，宽为$w$。我们分别以图像的每个像素为中心生成不同形状的锚框。设大小为$s\\in (0,1]$且宽高比为$r > 0$，那么锚框的宽和高将分别为$ws\\sqrt{r}$和$hs/\\sqrt{r}$。当中心位置给定时，已知宽和高的锚框是确定的。\n\n下面我们分别设定好一组大小$s_1,\\ldots,s_n$和一组宽高比$r_1,\\ldots,r_m$。如果以每个像素为中心时使用所有的大小与宽高比的组合，输入图像将一共得到$whnm$个锚框。虽然这些锚框可能覆盖了所有的真实边界框，但计算复杂度容易过高。因此，我们通常只对包含$s_1$或$r_1$的大小与宽高比的组合感兴趣，即\n\n$$\n(s_1, r_1), (s_1, r_2), \\ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \\ldots, (s_n, r_1).\n$$\n\n也就是说，以相同像素为中心的锚框的数量为$n+m-1$。对于整个输入图像，我们将一共生成$wh(n+m-1)$个锚框。\n\n以上生成锚框的方法已实现在`MultiBoxPrior`函数中。指定输入、一组大小和一组宽高比，该函数将返回输入的所有锚框。"},{"metadata":{"id":"861746AA8CA64C76820CCAE831F9F957","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"d2l.set_figsize()\nimg = Image.open(os.path.join(IMAGE_DIR, 'catdog.jpg'))\nw, h = img.size\nprint(\"w = %d, h = %d\" % (w, h))\n\n# d2l.plt.imshow(img);  # 加分号只显示图","execution_count":null},{"metadata":{"id":"9E4E14DE17B546F1A4FDE4301030E26F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 本函数已保存在d2lzh_pytorch包中方便以后使用\ndef MultiBoxPrior(feature_map, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5]):\n    \"\"\"\n    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成(xmin, ymin, xmax, ymax).\n    https://zh.d2l.ai/chapter_computer-vision/anchor.html\n    Args:\n        feature_map: torch tensor, Shape: [N, C, H, W].\n        sizes: List of sizes (0~1) of generated MultiBoxPriores. \n        ratios: List of aspect ratios (non-negative) of generated MultiBoxPriores. \n    Returns:\n        anchors of shape (1, num_anchors, 4). 由于batch里每个都一样, 所以第一维为1\n    \"\"\"\n    pairs = [] # pair of (size, sqrt(ration))\n    \n    # 生成n + m -1个框\n    for r in ratios:\n        pairs.append([sizes[0], math.sqrt(r)])\n    for s in sizes[1:]:\n        pairs.append([s, math.sqrt(ratios[0])])\n    \n    pairs = np.array(pairs)\n    \n    # 生成相对于坐标中心点的框（x,y,x,y）\n    ss1 = pairs[:, 0] * pairs[:, 1] # size * sqrt(ration)\n    ss2 = pairs[:, 0] / pairs[:, 1] # size / sqrt(ration)\n    \n    base_anchors = np.stack([-ss1, -ss2, ss1, ss2], axis=1) / 2\n    \n    #将坐标点和anchor组合起来生成hw（n+m-1）个框输出\n    h, w = feature_map.shape[-2:]\n    shifts_x = np.arange(0, w) / w\n    shifts_y = np.arange(0, h) / h\n    shift_x, shift_y = np.meshgrid(shifts_x, shifts_y)\n    \n    shift_x = shift_x.reshape(-1)\n    shift_y = shift_y.reshape(-1)\n    \n    shifts = np.stack((shift_x, shift_y, shift_x, shift_y), axis=1)\n    anchors = shifts.reshape((-1, 1, 4)) + base_anchors.reshape((1, -1, 4))\n    \n    return torch.tensor(anchors, dtype=torch.float32).view(1, -1, 4)","execution_count":null},{"metadata":{"id":"F491FB36A6EE40C0B308CFBD446AD995","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"X = torch.Tensor(1, 3, h, w)  # 构造输入数据\nY = MultiBoxPrior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\nY.shape","execution_count":null},{"metadata":{"id":"2958D151025B42909A3C586B1CC4DA5B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"\n我们看到，返回锚框变量`y`的形状为（1，锚框个数，4）。将锚框变量`y`的形状变为（图像高，图像宽，以相同像素为中心的锚框个数，4）后，我们就可以通过指定像素位置来获取所有以该像素为中心的锚框了。下面的例子里我们访问以（250，250）为中心的第一个锚框。它有4个元素，分别是锚框左上角的$x$和$y$轴坐标和右下角的$x$和$y$轴坐标，其中$x$和$y$轴的坐标值分别已除以图像的宽和高，因此值域均为0和1之间。\n"},{"metadata":{"id":"892142C942DC45119B13EAD76FA30A3D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 展示某个像素点的anchor\nboxes = Y.reshape((h, w, 5, 4))\nboxes[250, 250, 0, :]# * torch.tensor([w, h, w, h], dtype=torch.float32)\n# 第一个size和ratio分别为0.75和1, 则宽高均为0.75 = 0.7184 + 0.0316 = 0.8206 - 0.0706","execution_count":null},{"metadata":{"id":"60DD94B447574E3084B19E24E350C14A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"> 可以验证一下以上输出对不对：size和ratio分别为0.75和1, 则(归一化后的)宽高均为0.75, 所以输出是正确的（0.75 = 0.7184 + 0.0316 = 0.8206 - 0.0706）。\n\n为了描绘图像中以某个像素为中心的所有锚框，我们先定义`show_bboxes`函数以便在图像上画出多个边界框。"},{"metadata":{"id":"B9A92F6DE6A64306BF0DF81766637418","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 本函数已保存在dd2lzh_pytorch包中方便以后使用\ndef show_bboxes(axes, bboxes, labels=None, colors=None):\n    def _make_list(obj, default_values=None):\n        if obj is None:\n            obj = default_values\n        elif not isinstance(obj, (list, tuple)):\n            obj = [obj]\n        return obj\n\n    labels = _make_list(labels)\n    colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])\n    for i, bbox in enumerate(bboxes):\n        color = colors[i % len(colors)]\n        rect = d2l.bbox_to_rect(bbox.detach().cpu().numpy(), color)\n        axes.add_patch(rect)\n        if labels and len(labels) > i:\n            text_color = 'k' if color == 'w' else 'w'\n            axes.text(rect.xy[0], rect.xy[1], labels[i],\n                      va='center', ha='center', fontsize=6, color=text_color,\n                      bbox=dict(facecolor=color, lw=0))","execution_count":null},{"metadata":{"id":"69A299157C3C4A8D80562D59C03065EC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"刚刚我们看到，变量`boxes`中$x$和$y$轴的坐标值分别已除以图像的宽和高。在绘图时，我们需要恢复锚框的原始坐标值，并因此定义了变量`bbox_scale`。现在，我们可以画出图像中以(250, 250)为中心的所有锚框了。可以看到，大小为0.75且宽高比为1的锚框较好地覆盖了图像中的狗。"},{"metadata":{"id":"7BF7965ADE89450883429D704590EF54","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 展示 250 250像素点的anchor\nd2l.set_figsize()\nfig = d2l.plt.imshow(img)\nbbox_scale = torch.tensor([[w, h, w, h]], dtype=torch.float32)\nshow_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,\n            ['s=0.75, r=1', 's=0.75, r=2', 's=0.75, r=0.5', 's=0.5, r=1', 's=0.25, r=1'])","execution_count":null},{"metadata":{"id":"F0BBCA59EA8D42A38AEEC6CB8364A69C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 交并比"},{"metadata":{"id":"0F97E1A352CE453D8160AED3CC17B408","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"我们刚刚提到某个锚框较好地覆盖了图像中的狗。如果该目标的真实边界框已知，这里的“较好”该如何量化呢？一种直观的方法是衡量锚框和真实边界框之间的相似度。我们知道，Jaccard系数（Jaccard index）可以衡量两个集合的相似度。给定集合$\\mathcal{A}$和$\\mathcal{B}$，它们的Jaccard系数即二者交集大小除以二者并集大小：\n\n$$\nJ(\\mathcal{A},\\mathcal{B}) = \\frac{\\left|\\mathcal{A} \\cap \\mathcal{B}\\right|}{\\left| \\mathcal{A} \\cup \\mathcal{B}\\right|}.\n$$\n\n\n实际上，我们可以把边界框内的像素区域看成是像素的集合。如此一来，我们可以用两个边界框的像素集合的Jaccard系数衡量这两个边界框的相似度。当衡量两个边界框的相似度时，我们通常将Jaccard系数称为交并比（Intersection over Union，IoU），即两个边界框相交面积与相并面积之比，如图9.2所示。交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框相等。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5vs9jkw9f.png?imageView2/0/w/640/h/640)\n"},{"metadata":{"id":"04242158CF8841C0865097ADF7850E5C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 以下函数已保存在d2lzh_pytorch包中方便以后使用\ndef compute_intersection(set_1, set_2):\n    \"\"\"\n    计算anchor之间的交集\n    Args:\n        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)\n        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)\n    Returns:\n        intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\n    \"\"\"\n    # PyTorch auto-broadcasts singleton dimensions\n    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n\n\ndef compute_jaccard(set_1, set_2):\n    \"\"\"\n    计算anchor之间的Jaccard系数(IoU)\n    Args:\n        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)\n        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)\n    Returns:\n        Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\n    \"\"\"\n    # Find intersections\n    intersection = compute_intersection(set_1, set_2)  # (n1, n2)\n\n    # Find areas of each box in both sets\n    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n\n    # Find the union\n    # PyTorch auto-broadcasts singleton dimensions\n    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n\n    return intersection / union  # (n1, n2)","execution_count":null},{"metadata":{"id":"C7BD9B5EC043456C8E90B9823B9C32D0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 标注训练集的锚框\n\n在训练集中，我们将每个锚框视为一个训练样本。为了训练目标检测模型，我们需要为每个锚框标注两类标签：一是锚框所含目标的类别，简称类别；二是真实边界框相对锚框的偏移量，简称偏移量（offset）。在目标检测时，我们首先生成多个锚框，然后为每个锚框预测类别以及偏移量，接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框。\n\n\n我们知道，在目标检测的训练集中，每个图像已标注了真实边界框的位置以及所含目标的类别。在生成锚框之后，我们主要依据与锚框相似的真实边界框的位置和类别信息为锚框标注。那么，该如何为锚框分配与其相似的真实边界框呢？\n\n\n假设图像中锚框分别为$A_1, A_2, \\ldots, A_{n_a}$，真实边界框分别为$B_1, B_2, \\ldots, B_{n_b}$，且$n_a \\geq n_b$。定义矩阵$\\boldsymbol{X} \\in \\mathbb{R}^{n_a \\times n_b}$，其中第$i$行第$j$列的元素$x_{ij}$为锚框$A_i$与真实边界框$B_j$的交并比。\n首先，我们找出矩阵$\\boldsymbol{X}$中最大元素，并将该元素的行索引与列索引分别记为$i_1,j_1$。我们为锚框$A_{i_1}$分配真实边界框$B_{j_1}$。显然，锚框$A_{i_1}$和真实边界框$B_{j_1}$在所有的“锚框—真实边界框”的配对中相似度最高。接下来，将矩阵$\\boldsymbol{X}$中第$i_1$行和第$j_1$列上的所有元素丢弃。找出矩阵$\\boldsymbol{X}$中剩余的最大元素，并将该元素的行索引与列索引分别记为$i_2,j_2$。我们为锚框$A_{i_2}$分配真实边界框$B_{j_2}$，再将矩阵$\\boldsymbol{X}$中第$i_2$行和第$j_2$列上的所有元素丢弃。此时矩阵$\\boldsymbol{X}$中已有两行两列的元素被丢弃。\n依此类推，直到矩阵$\\boldsymbol{X}$中所有$n_b$列元素全部被丢弃。这个时候，我们已为$n_b$个锚框各分配了一个真实边界框。\n接下来，我们只遍历剩余的$n_a - n_b$个锚框：给定其中的锚框$A_i$，根据矩阵$\\boldsymbol{X}$的第$i$行找到与$A_i$交并比最大的真实边界框$B_j$，且只有当该交并比大于预先设定的阈值时，才为锚框$A_i$分配真实边界框$B_j$。\n\n\n如图9.3（左）所示，假设矩阵$\\boldsymbol{X}$中最大值为$x_{23}$，我们将为锚框$A_2$分配真实边界框$B_3$。然后，丢弃矩阵中第2行和第3列的所有元素，找出剩余阴影部分的最大元素$x_{71}$，为锚框$A_7$分配真实边界框$B_1$。接着如图9.3（中）所示，丢弃矩阵中第7行和第1列的所有元素，找出剩余阴影部分的最大元素$x_{54}$，为锚框$A_5$分配真实边界框$B_4$。最后如图9.3（右）所示，丢弃矩阵中第5行和第4列的所有元素，找出剩余阴影部分的最大元素$x_{92}$，为锚框$A_9$分配真实边界框$B_2$。之后，我们只需遍历除去$A_2, A_5, A_7, A_9$的剩余锚框，并根据阈值判断是否为剩余锚框分配真实边界框。\n\n![Image Name](https://cdn.kesci.com/upload/image/q5vsc1hcg8.png?imageView2/0/w/640/h/640)\n\n现在我们可以标注锚框的类别和偏移量了。如果一个锚框$A$被分配了真实边界框$B$，将锚框$A$的类别设为$B$的类别，并根据$B$和$A$的中心坐标的相对位置以及两个框的相对大小为锚框$A$标注偏移量。由于数据集中各个框的位置和大小各异，因此这些相对位置和相对大小通常需要一些特殊变换，才能使偏移量的分布更均匀从而更容易拟合。设锚框$A$及其被分配的真实边界框$B$的中心坐标分别为$(x_a, y_a)$和$(x_b, y_b)$，$A$和$B$的宽分别为$w_a$和$w_b$，高分别为$h_a$和$h_b$，一个常用的技巧是将$A$的偏移量标注为\n\n$$\n\\left( \\frac{ \\frac{x_b - x_a}{w_a} - \\mu_x }{\\sigma_x},\n\\frac{ \\frac{y_b - y_a}{h_a} - \\mu_y }{\\sigma_y},\n\\frac{ \\log \\frac{w_b}{w_a} - \\mu_w }{\\sigma_w},\n\\frac{ \\log \\frac{h_b}{h_a} - \\mu_h }{\\sigma_h}\\right),\n$$\n\n其中常数的默认值为$\\mu_x = \\mu_y = \\mu_w = \\mu_h = 0, \\sigma_x=\\sigma_y=0.1, \\sigma_w=\\sigma_h=0.2$。如果一个锚框没有被分配真实边界框，我们只需将该锚框的类别设为背景。类别为背景的锚框通常被称为负类锚框，其余则被称为正类锚框。\n\n\n下面演示一个具体的例子。我们为读取的图像中的猫和狗定义真实边界框，其中第一个元素为类别（0为狗，1为猫），剩余4个元素分别为左上角的$x$和$y$轴坐标以及右下角的$x$和$y$轴坐标（值域在0到1之间）。这里通过左上角和右下角的坐标构造了5个需要标注的锚框，分别记为$A_0, \\ldots, A_4$（程序中索引从0开始）。先画出这些锚框与真实边界框在图像中的位置。"},{"metadata":{"id":"8A675D2BEF144D56BDE008052DAED964","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"bbox_scale = torch.tensor((w, h, w, h), dtype=torch.float32)\nground_truth = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],\n                            [1, 0.55, 0.2, 0.9, 0.88]])\nanchors = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],\n                    [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],\n                    [0.57, 0.3, 0.92, 0.9]])\n\nfig = d2l.plt.imshow(img)\nshow_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')\nshow_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4']);","execution_count":null},{"metadata":{"id":"EB5CDAA26D3142248F7DEEBC31BEF534","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"compute_jaccard(anchors, ground_truth[:, 1:]) # 验证一下写的compute_jaccard函数","execution_count":null},{"metadata":{"id":"7871B1ED57824458866527D81040C1CA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"\n下面实现`MultiBoxTarget`函数来为锚框标注类别和偏移量。该函数将背景类别设为0，并令从零开始的目标类别的整数索引自加1（1为狗，2为猫）。\n"},{"metadata":{"id":"B727A49622D8400A8B51934339F6AFBB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 以下函数已保存在d2lzh_pytorch包中方便以后使用\ndef assign_anchor(bb, anchor, jaccard_threshold=0.5):\n    \"\"\"\n    # 按照「9.4.1. 生成多个锚框」图9.3所讲为每个anchor分配真实的bb, anchor表示成归一化(xmin, ymin, xmax, ymax).\n    https://zh.d2l.ai/chapter_computer-vision/anchor.html\n    Args:\n        bb: 真实边界框(bounding box), shape:（nb, 4）\n        anchor: 待分配的anchor, shape:（na, 4）\n        jaccard_threshold: 预先设定的阈值\n    Returns:\n        assigned_idx: shape: (na, ), 每个anchor分配的真实bb对应的索引, 若未分配任何bb则为-1\n    \"\"\"\n    na = anchor.shape[0] \n    nb = bb.shape[0]\n    jaccard = compute_jaccard(anchor, bb).detach().cpu().numpy() # shape: (na, nb)\n    assigned_idx = np.ones(na) * -1  # 存放标签初始全为-1\n    \n    # 先为每个bb分配一个anchor(不要求满足jaccard_threshold)\n    jaccard_cp = jaccard.copy()\n    for j in range(nb):\n        i = np.argmax(jaccard_cp[:, j])\n        assigned_idx[i] = j\n        jaccard_cp[i, :] = float(\"-inf\") # 赋值为负无穷, 相当于去掉这一行\n     \n    # 处理还未被分配的anchor, 要求满足jaccard_threshold\n    for i in range(na):\n        if assigned_idx[i] == -1:\n            j = np.argmax(jaccard[i, :])\n            if jaccard[i, j] >= jaccard_threshold:\n                assigned_idx[i] = j\n                \n    return torch.tensor(assigned_idx, dtype=torch.long)\n\n\ndef xy_to_cxcy(xy):\n    \"\"\"\n    将(x_min, y_min, x_max, y_max)形式的anchor转换成(center_x, center_y, w, h)形式的.\n    https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n    Args:\n        xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n    Returns: \n        bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n    \"\"\"\n    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\n                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\n\ndef MultiBoxTarget(anchor, label):\n    \"\"\"\n    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成归一化(xmin, ymin, xmax, ymax).\n    https://zh.d2l.ai/chapter_computer-vision/anchor.html\n    Args:\n        anchor: torch tensor, 输入的锚框, 一般是通过MultiBoxPrior生成, shape:（1，锚框总数，4）\n        label: 真实标签, shape为(bn, 每张图片最多的真实锚框数, 5)\n               第二维中，如果给定图片没有这么多锚框, 可以先用-1填充空白, 最后一维中的元素为[类别标签, 四个坐标值]\n    Returns:\n        列表, [bbox_offset, bbox_mask, cls_labels]\n        bbox_offset: 每个锚框的标注偏移量，形状为(bn，锚框总数*4)\n        bbox_mask: 形状同bbox_offset, 每个锚框的掩码, 一一对应上面的偏移量, 负类锚框(背景)对应的掩码均为0, 正类锚框的掩码均为1\n        cls_labels: 每个锚框的标注类别, 其中0表示为背景, 形状为(bn，锚框总数)\n    \"\"\"\n    assert len(anchor.shape) == 3 and len(label.shape) == 3\n    bn = label.shape[0]\n    \n    def MultiBoxTarget_one(anc, lab, eps=1e-6):\n        \"\"\"\n        MultiBoxTarget函数的辅助函数, 处理batch中的一个\n        Args:\n            anc: shape of (锚框总数, 4)\n            lab: shape of (真实锚框数, 5), 5代表[类别标签, 四个坐标值]\n            eps: 一个极小值, 防止log0\n        Returns:\n            offset: (锚框总数*4, )\n            bbox_mask: (锚框总数*4, ), 0代表背景, 1代表非背景\n            cls_labels: (锚框总数, 4), 0代表背景\n        \"\"\"\n        an = anc.shape[0]\n        # 变量的意义\n        assigned_idx = assign_anchor(lab[:, 1:], anc) # (锚框总数, )\n        print(\"a: \",  assigned_idx.shape)\n        print(assigned_idx)\n        bbox_mask = ((assigned_idx >= 0).float().unsqueeze(-1)).repeat(1, 4) # (锚框总数, 4)\n        print(\"b: \" , bbox_mask.shape)\n        print(bbox_mask)\n\n        cls_labels = torch.zeros(an, dtype=torch.long) # 0表示背景\n        assigned_bb = torch.zeros((an, 4), dtype=torch.float32) # 所有anchor对应的bb坐标\n        for i in range(an):\n            bb_idx = assigned_idx[i]\n            if bb_idx >= 0: # 即非背景\n                cls_labels[i] = lab[bb_idx, 0].long().item() + 1 # 注意要加一\n                assigned_bb[i, :] = lab[bb_idx, 1:]\n        # 如何计算偏移量\n        center_anc = xy_to_cxcy(anc) # (center_x, center_y, w, h)\n        center_assigned_bb = xy_to_cxcy(assigned_bb)\n\n        offset_xy = 10.0 * (center_assigned_bb[:, :2] - center_anc[:, :2]) / center_anc[:, 2:]\n        offset_wh = 5.0 * torch.log(eps + center_assigned_bb[:, 2:] / center_anc[:, 2:])\n        offset = torch.cat([offset_xy, offset_wh], dim = 1) * bbox_mask # (锚框总数, 4)\n\n        return offset.view(-1), bbox_mask.view(-1), cls_labels\n    # 组合输出\n    batch_offset = []\n    batch_mask = []\n    batch_cls_labels = []\n    for b in range(bn):\n        offset, bbox_mask, cls_labels = MultiBoxTarget_one(anchor[0, :, :], label[b, :, :])\n        \n        batch_offset.append(offset)\n        batch_mask.append(bbox_mask)\n        batch_cls_labels.append(cls_labels)\n    \n    bbox_offset = torch.stack(batch_offset)\n    bbox_mask = torch.stack(batch_mask)\n    cls_labels = torch.stack(batch_cls_labels)\n    \n    return [bbox_offset, bbox_mask, cls_labels]","execution_count":null},{"metadata":{"id":"E0B45CBA4F0F402A8635F468E8F031F4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 通过unsqueeze函数为锚框和真实边界框添加样本维\nlabels = MultiBoxTarget(anchors.unsqueeze(dim=0),\n                        ground_truth.unsqueeze(dim=0))","execution_count":null},{"metadata":{"id":"86D7D0B1A78E49B482481027AE118207","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 返回的结果里有3项，均为`Tensor`。第三项表示为锚框标注的类别\nlabels[2]","execution_count":null},{"metadata":{"id":"101BC2CAFDB4435FA1CABF0895B281CC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"我们根据锚框与真实边界框在图像中的位置来分析这些标注的类别。首先，在所有的“锚框—真实边界框”的配对中，锚框$A_4$与猫的真实边界框的交并比最大，因此锚框$A_4$的类别标注为猫。不考虑锚框$A_4$或猫的真实边界框，在剩余的“锚框—真实边界框”的配对中，最大交并比的配对为锚框$A_1$和狗的真实边界框，因此锚框$A_1$的类别标注为狗。接下来遍历未标注的剩余3个锚框：与锚框$A_0$交并比最大的真实边界框的类别为狗，但交并比小于阈值（默认为0.5），因此类别标注为背景；与锚框$A_2$交并比最大的真实边界框的类别为猫，且交并比大于阈值，因此类别标注为猫；与锚框$A_3$交并比最大的真实边界框的类别为猫，但交并比小于阈值，因此类别标注为背景。\n\n\n返回值的第二项为掩码（mask）变量，形状为(批量大小, 锚框个数的四倍)。掩码变量中的元素与每个锚框的4个偏移量一一对应。\n由于我们不关心对背景的检测，有关负类的偏移量不应影响目标函数。通过按元素乘法，掩码变量中的0可以在计算目标函数之前过滤掉负类的偏移量。"},{"metadata":{"id":"48DBFE1735C64A1F91595FB6926FF120","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"labels[1]","execution_count":null},{"metadata":{"id":"D7F6EB1EC8CA434381AF9B94455C85EC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 返回的第一项是为每个锚框标注的四个偏移量，其中负类锚框的偏移量标注为0。\nlabels[0]","execution_count":null},{"metadata":{"id":"41818AA72CF04B328FAFB3595FEE2862","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 输出预测边界框\n在模型预测阶段，我们先为图像生成多个锚框，并为这些锚框一一预测类别和偏移量。随后，我们根据锚框及其预测偏移量得到预测边界框。当锚框数量较多时，同一个目标上可能会输出较多相似的预测边界框。为了使结果更加简洁，我们可以移除相似的预测边界框。常用的方法叫作非极大值抑制（non-maximum suppression，NMS）。\n\n我们来描述一下非极大值抑制的工作原理。对于一个预测边界框$B$，模型会计算各个类别的预测概率。设其中最大的预测概率为$p$，该概率所对应的类别即$B$的预测类别。我们也将$p$称为预测边界框$B$的置信度。在同一图像上，我们将预测类别非背景的预测边界框按置信度从高到低排序，得到列表$L$。从$L$中选取置信度最高的预测边界框$B_1$作为基准，将所有与$B_1$的交并比大于某阈值的非基准预测边界框从$L$中移除。这里的阈值是预先设定的超参数。此时，$L$保留了置信度最高的预测边界框并移除了与其相似的其他预测边界框。\n接下来，从$L$中选取置信度第二高的预测边界框$B_2$作为基准，将所有与$B_2$的交并比大于某阈值的非基准预测边界框从$L$中移除。重复这一过程，直到$L$中所有的预测边界框都曾作为基准。此时$L$中任意一对预测边界框的交并比都小于阈值。最终，输出列表$L$中的所有预测边界框。\n\n下面来看一个具体的例子。先构造4个锚框。简单起见，我们假设预测偏移量全是0：预测边界框即锚框。最后，我们构造每个类别的预测概率。"},{"metadata":{"id":"CA649C630DB644708998AFED9291F568","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"anchors = torch.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],\n                        [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])\noffset_preds = torch.tensor([0.0] * (4 * len(anchors)))\ncls_probs = torch.tensor([[0., 0., 0., 0.,],  # 背景的预测概率\n                          [0.9, 0.8, 0.7, 0.1],  # 狗的预测概率\n                          [0.1, 0.2, 0.3, 0.9]])  # 猫的预测概率","execution_count":null},{"metadata":{"id":"8C266F8FD31F4C95967FD9B325E4801F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 在图像上打印预测边界框和它们的置信度\nfig = d2l.plt.imshow(img)\nshow_bboxes(fig.axes, anchors * bbox_scale,\n            ['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])","execution_count":null},{"metadata":{"id":"13FF1C59CB3148E68F13636B53C64E0D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"下面我们实现`MultiBoxDetection`函数来执行非极大值抑制"},{"metadata":{"id":"6C86A5B07BE94ECBB03A753D25326033","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 以下函数已保存在d2lzh_pytorch包中方便以后使用\nfrom collections import namedtuple\nPred_BB_Info = namedtuple(\"Pred_BB_Info\", [\"index\", \"class_id\", \"confidence\", \"xyxy\"])\n\ndef non_max_suppression(bb_info_list, nms_threshold = 0.5):\n    \"\"\"\n    非极大抑制处理预测的边界框\n    Args:\n        bb_info_list: Pred_BB_Info的列表, 包含预测类别、置信度等信息\n        nms_threshold: 阈值\n    Returns:\n        output: Pred_BB_Info的列表, 只保留过滤后的边界框信息\n    \"\"\"\n    output = []\n    # 先根据置信度从高到低排序\n    sorted_bb_info_list = sorted(bb_info_list, key = lambda x: x.confidence, reverse=True)\n    \n    # 循环遍历删除冗余输出\n    while len(sorted_bb_info_list) != 0:\n        best = sorted_bb_info_list.pop(0)\n        output.append(best)\n        \n        if len(sorted_bb_info_list) == 0:\n            break\n\n        bb_xyxy = []\n        for bb in sorted_bb_info_list:\n            bb_xyxy.append(bb.xyxy)\n        \n        iou = compute_jaccard(torch.tensor([best.xyxy]), \n                              torch.tensor(bb_xyxy))[0] # shape: (len(sorted_bb_info_list), )\n        \n        n = len(sorted_bb_info_list)\n        sorted_bb_info_list = [sorted_bb_info_list[i] for i in range(n) if iou[i] <= nms_threshold]\n    return output\n\ndef MultiBoxDetection(cls_prob, loc_pred, anchor, nms_threshold = 0.5):\n    \"\"\"\n    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成归一化(xmin, ymin, xmax, ymax).\n    https://zh.d2l.ai/chapter_computer-vision/anchor.html\n    Args:\n        cls_prob: 经过softmax后得到的各个锚框的预测概率, shape:(bn, 预测总类别数+1, 锚框个数)\n        loc_pred: 预测的各个锚框的偏移量, shape:(bn, 锚框个数*4)\n        anchor: MultiBoxPrior输出的默认锚框, shape: (1, 锚框个数, 4)\n        nms_threshold: 非极大抑制中的阈值\n    Returns:\n        所有锚框的信息, shape: (bn, 锚框个数, 6)\n        每个锚框信息由[class_id, confidence, xmin, ymin, xmax, ymax]表示\n        class_id=-1 表示背景或在非极大值抑制中被移除了\n    \"\"\"\n    assert len(cls_prob.shape) == 3 and len(loc_pred.shape) == 2 and len(anchor.shape) == 3\n    bn = cls_prob.shape[0]\n    \n    def MultiBoxDetection_one(c_p, l_p, anc, nms_threshold = 0.5):\n        \"\"\"\n        MultiBoxDetection的辅助函数, 处理batch中的一个\n        Args:\n            c_p: (预测总类别数+1, 锚框个数)\n            l_p: (锚框个数*4, )\n            anc: (锚框个数, 4)\n            nms_threshold: 非极大抑制中的阈值\n        Return:\n            output: (锚框个数, 6)\n        \"\"\"\n        pred_bb_num = c_p.shape[1]\n        anc = (anc + l_p.view(pred_bb_num, 4)).detach().cpu().numpy() # 加上偏移量\n        \n        confidence, class_id = torch.max(c_p, 0)\n        confidence = confidence.detach().cpu().numpy()\n        class_id = class_id.detach().cpu().numpy()\n        \n        pred_bb_info = [Pred_BB_Info(\n                            index = i,\n                            class_id = class_id[i] - 1, # 正类label从0开始\n                            confidence = confidence[i],\n                            xyxy=[*anc[i]]) # xyxy是个列表\n                        for i in range(pred_bb_num)]\n        \n        # 正类的index\n        obj_bb_idx = [bb.index for bb in non_max_suppression(pred_bb_info, nms_threshold)]\n        \n        output = []\n        for bb in pred_bb_info:\n            output.append([\n                (bb.class_id if bb.index in obj_bb_idx else -1.0),\n                bb.confidence,\n                *bb.xyxy\n            ])\n            \n        return torch.tensor(output) # shape: (锚框个数, 6)\n    \n    batch_output = []\n    for b in range(bn):\n        batch_output.append(MultiBoxDetection_one(cls_prob[b], loc_pred[b], anchor[0], nms_threshold))\n    \n    return torch.stack(batch_output)","execution_count":null},{"metadata":{"id":"72627316A3994A3B87B4EE58B5652E12","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"然后我们运行`MultiBoxDetection`函数并设阈值为0.5。这里为输入都增加了样本维。我们看到，返回的结果的形状为(批量大小, 锚框个数, 6)。其中每一行的6个元素代表同一个预测边界框的输出信息。第一个元素是索引从0开始计数的预测类别（0为狗，1为猫），其中-1表示背景或在非极大值抑制中被移除。第二个元素是预测边界框的置信度。剩余的4个元素分别是预测边界框左上角的$x$和$y$轴坐标以及右下角的$x$和$y$轴坐标（值域在0到1之间）。\n"},{"metadata":{"id":"D947068B18A54D6FAD84B692C3782405","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"output = MultiBoxDetection(\n    cls_probs.unsqueeze(dim=0), offset_preds.unsqueeze(dim=0),\n    anchors.unsqueeze(dim=0), nms_threshold=0.5)\noutput","execution_count":null},{"metadata":{"id":"0E0E60A88376460C808A417A50E93E0A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"fig = d2l.plt.imshow(img)\nfor i in output[0].detach().cpu().numpy():\n    if i[0] == -1:\n        continue\n    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])\n    show_bboxes(fig.axes, [torch.tensor(i[2:]) * bbox_scale], label)","execution_count":null},{"metadata":{"id":"868498CC00CE4D2A815596009A89C374","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"实践中，我们可以在执行非极大值抑制前将置信度较低的预测边界框移除，从而减小非极大值抑制的计算量。我们还可以筛选非极大值抑制的输出，例如，只保留其中置信度较高的结果作为最终输出。\n\n## 小结\n\n* 以每个像素为中心，生成多个大小和宽高比不同的锚框。\n* 交并比是两个边界框相交面积与相并面积之比。\n* 在训练集中，为每个锚框标注两类标签：一是锚框所含目标的类别；二是真实边界框相对锚框的偏移量。\n* 预测时，可以使用非极大值抑制来移除相似的预测边界框，从而令结果简洁。"},{"metadata":{"id":"055F0AEBEE9948408F2960AEC00CFB88","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 多尺度目标检测\n在9.4节（锚框）中，我们在实验中以输入图像的每个像素为中心生成多个锚框。这些锚框是对输入图像不同区域的采样。然而，如果以图像每个像素为中心都生成锚框，很容易生成过多锚框而造成计算量过大。举个例子，假设输入图像的高和宽分别为561像素和728像素，如果以每个像素为中心生成5个不同形状的锚框，那么一张图像上则需要标注并预测200多万个锚框（$561 \\times 728 \\times 5$）。\n\n减少锚框个数并不难。一种简单的方法是在输入图像中均匀采样一小部分像素，并以采样的像素为中心生成锚框。此外，在不同尺度下，我们可以生成不同数量和不同大小的锚框。值得注意的是，较小目标比较大目标在图像上出现位置的可能性更多。举个简单的例子：形状为$1 \\times 1$、$1 \\times 2$和$2 \\times 2$的目标在形状为$2 \\times 2$的图像上可能出现的位置分别有4、2和1种。因此，当使用较小锚框来检测较小目标时，我们可以采样较多的区域；而当使用较大锚框来检测较大目标时，我们可以采样较少的区域。\n\n为了演示如何多尺度生成锚框，我们先读取一张图像。它的高和宽分别为561像素和728像素。\n"},{"metadata":{"id":"5BDD34B7F88440AB8A5E03C412922247","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"w, h = img.size\nw, h","execution_count":null},{"metadata":{"id":"48B82659CA7749CAAE9280986204D0B5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"d2l.set_figsize()\n\ndef display_anchors(fmap_w, fmap_h, s):\n    # 前两维的取值不影响输出结果(原书这里是(1, 10, fmap_w, fmap_h), 我认为错了)\n    fmap = torch.zeros((1, 10, fmap_h, fmap_w), dtype=torch.float32)\n    \n    # 平移所有锚框使均匀分布在图片上\n    offset_x, offset_y = 1.0/fmap_w, 1.0/fmap_h\n    anchors = d2l.MultiBoxPrior(fmap, sizes=s, ratios=[1, 2, 0.5]) + \\\n        torch.tensor([offset_x/2, offset_y/2, offset_x/2, offset_y/2])\n    \n    bbox_scale = torch.tensor([[w, h, w, h]], dtype=torch.float32)\n    d2l.show_bboxes(d2l.plt.imshow(img).axes,\n                    anchors[0] * bbox_scale)","execution_count":null},{"metadata":{"id":"C585BE746154461CBBD352755151E2E0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"display_anchors(fmap_w=4, fmap_h=2, s=[0.15])","execution_count":null},{"metadata":{"id":"0E8706006B934BD98075669AFF4FF5BC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"display_anchors(fmap_w=2, fmap_h=1, s=[0.4])","execution_count":null},{"metadata":{"id":"42BA10E9075A4CE3BD0354C12BB6F31C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"display_anchors(fmap_w=1, fmap_h=1, s=[0.8])","execution_count":null},{"metadata":{"id":"529052AA40AC4A80994E1CBFF70798C5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 图像风格迁移"},{"metadata":{"id":"2A95712F2C6F40CF822DD65AC6613B59","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 样式迁移\n\n如果你是一位摄影爱好者，也许接触过滤镜。它能改变照片的颜色样式，从而使风景照更加锐利或者令人像更加美白。但一个滤镜通常只能改变照片的某个方面。如果要照片达到理想中的样式，经常需要尝试大量不同的组合，其复杂程度不亚于模型调参。\n\n在本节中，我们将介绍如何使用卷积神经网络自动将某图像中的样式应用在另一图像之上，即样式迁移（style transfer）[1]。这里我们需要两张输入图像，一张是内容图像，另一张是样式图像，我们将使用神经网络修改内容图像使其在样式上接近样式图像。图9.12中的内容图像为本书作者在西雅图郊区的雷尼尔山国家公园（Mount Rainier National Park）拍摄的风景照，而样式图像则是一副主题为秋天橡树的油画。最终输出的合成图像在保留了内容图像中物体主体形状的情况下应用了样式图像的油画笔触，同时也让整体颜色更加鲜艳。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5w2i3mjvm.png?imageView2/0/w/960/h/960)\n\n\n## 方法\n\n图9.13用一个例子来阐述基于卷积神经网络的样式迁移方法。首先，我们初始化合成图像，例如将其初始化成内容图像。该合成图像是样式迁移过程中唯一需要更新的变量，即样式迁移所需迭代的模型参数。然后，我们选择一个预训练的卷积神经网络来抽取图像的特征，其中的模型参数在训练中无须更新。深度卷积神经网络凭借多个层逐级抽取图像的特征。我们可以选择其中某些层的输出作为内容特征或样式特征。以图9.13为例，这里选取的预训练的神经网络含有3个卷积层，其中第二层输出图像的内容特征，而第一层和第三层的输出被作为图像的样式特征。接下来，我们通过正向传播（实线箭头方向）计算样式迁移的损失函数，并通过反向传播（虚线箭头方向）迭代模型参数，即不断更新合成图像。样式迁移常用的损失函数由3部分组成：内容损失（content loss）使合成图像与内容图像在内容特征上接近，样式损失（style loss）令合成图像与样式图像在样式特征上接近，而总变差损失（total variation loss）则有助于减少合成图像中的噪点。最后，当模型训练结束时，我们输出样式迁移的模型参数，即得到最终的合成图像。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5w2jrvuc9.png?imageView2/0/w/640/h/640)\n\n\n下面，我们通过实验来进一步了解样式迁移的技术细节。实验需要用到一些导入的包或模块。"},{"metadata":{"id":"A0D09A9D426249C981DBFE0A5117C05C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"%matplotlib inline\nimport time\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport sys\nsys.path.append(\"/home/kesci/input\") \nimport d2len9900 as d2l\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 均已测试\n\nprint(device, torch.__version__)","execution_count":null},{"metadata":{"id":"864BA897FE274003947538F8854C59F5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 读取内容图像和样式图像\n\n首先，我们分别读取内容图像和样式图像。从打印出的图像坐标轴可以看出，它们的尺寸并不一样。"},{"metadata":{"id":"19C3F8CCB4A349AE81762807F22E4170","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"#d2l.set_figsize()\ncontent_img = Image.open('/home/kesci/input/NeuralStyle5603/rainier.jpg')\nplt.imshow(content_img);","execution_count":null},{"metadata":{"id":"4F79A39280CE4E818033B30D76795D6A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"style_img = Image.open('/home/kesci/input/NeuralStyle5603/autumn_oak.jpg')\nplt.imshow(style_img);","execution_count":null},{"metadata":{"id":"F94A3578C7BC4C26942CC4E6C534BF73","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 预处理和后处理图像\n\n下面定义图像的预处理函数和后处理函数。预处理函数`preprocess`对输入图像在RGB三个通道分别做标准化，并将结果变换成卷积神经网络接受的输入格式。后处理函数`postprocess`则将输出图像中的像素值还原回标准化之前的值。由于图像打印函数要求每个像素的浮点数值在0到1之间，我们使用`clamp`函数对小于0和大于1的值分别取0和1。"},{"metadata":{"id":"55ADAB085DC94B5C85989F56F6342153","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"rgb_mean = np.array([0.485, 0.456, 0.406])\nrgb_std = np.array([0.229, 0.224, 0.225])\n\ndef preprocess(PIL_img, image_shape):\n    process = torchvision.transforms.Compose([\n        torchvision.transforms.Resize(image_shape),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n\n    return process(PIL_img).unsqueeze(dim = 0) # (batch_size, 3, H, W)\n\ndef postprocess(img_tensor):\n    inv_normalize = torchvision.transforms.Normalize(\n        mean= -rgb_mean / rgb_std,\n        std= 1/rgb_std)\n    to_PIL_image = torchvision.transforms.ToPILImage()\n    return to_PIL_image(inv_normalize(img_tensor[0].cpu()).clamp(0, 1))","execution_count":null},{"metadata":{"id":"D55E141EC70A427A80D25856CC6BD843","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 抽取特征\n\n我们使用基于ImageNet数据集预训练的VGG-19模型来抽取图像特征 [1]。"},{"metadata":{"id":"EE6CEFF99D9F41A9B0FF84237CB54845","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"!echo $TORCH_HOME # 将会把预训练好的模型下载到此处(没有输出的话默认是.cache/torch)\npretrained_net = torchvision.models.vgg19(pretrained=False)\npretrained_net.load_state_dict(torch.load('/home/kesci/input/vgg193427/vgg19-dcbb9e9d.pth'))","execution_count":null},{"metadata":{"id":"BB03FD2299214ECD8D95F7F6043E98C6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"为了抽取图像的内容特征和样式特征，我们可以选择VGG网络中某些层的输出。一般来说，越靠近输入层的输出越容易抽取图像的细节信息，反之则越容易抽取图像的全局信息。为了避免合成图像过多保留内容图像的细节，我们选择VGG较靠近输出的层，也称内容层，来输出图像的内容特征。我们还从VGG中选择不同层的输出来匹配局部和全局的样式，这些层也叫样式层。在[“使用重复元素的网络（VGG）”](../chapter_convolutional-neural-networks/vgg.ipynb)一节中我们曾介绍过，VGG网络使用了5个卷积块。实验中，我们选择第四卷积块的最后一个卷积层作为内容层，以及每个卷积块的第一个卷积层作为样式层。这些层的索引可以通过打印`pretrained_net`实例来获取。"},{"metadata":{"id":"05D7EFE58C6943109DD0A3A4C1FCBB47","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"style_layers, content_layers = [0, 5, 10, 19, 28], [25]","execution_count":null},{"metadata":{"id":"442B942E289147189161506B9B13145D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"在抽取特征时，我们只需要用到VGG从输入层到最靠近输出层的内容层或样式层之间的所有层。下面构建一个新的网络`net`，它只保留需要用到的VGG的所有层。我们将使用`net`来抽取特征。"},{"metadata":{"id":"6662AACB5FBD4269B226C71F1ED699EC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"net_list = []\nfor i in range(max(content_layers + style_layers) + 1):\n    net_list.append(pretrained_net.features[i])\nnet = torch.nn.Sequential(*net_list)","execution_count":null},{"metadata":{"id":"CA116319CD8441E58432BAEF4DD4EDEF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"给定输入`X`，如果简单调用前向计算`net(X)`，只能获得最后一层的输出。由于我们还需要中间层的输出，因此这里我们逐层计算，并保留内容层和样式层的输出。"},{"metadata":{"id":"DB8895C8FFC74FF1AECC68646411C67D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def extract_features(X, content_layers, style_layers):\n    contents = []\n    styles = []\n    for i in range(len(net)):\n        X = net[i](X)\n        if i in style_layers:\n            styles.append(X)\n        if i in content_layers:\n            contents.append(X)\n    return contents, styles","execution_count":null},{"metadata":{"id":"8C05A6B61EA54C9A92BA47A61BC0C8DF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"下面定义两个函数，其中`get_contents`函数对内容图像抽取内容特征，而`get_styles`函数则对样式图像抽取样式特征。因为在训练时无须改变预训练的VGG的模型参数，所以我们可以在训练开始之前就提取出内容图像的内容特征，以及样式图像的样式特征。由于合成图像是样式迁移所需迭代的模型参数，我们只能在训练过程中通过调用`extract_features`函数来抽取合成图像的内容特征和样式特征。"},{"metadata":{"id":"4EA35C3E471D4B18B44FDD44236CEBB0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def get_contents(image_shape, device):\n    content_X = preprocess(content_img, image_shape).to(device)\n    contents_Y, _ = extract_features(content_X, content_layers, style_layers)\n    return content_X, contents_Y\n\ndef get_styles(image_shape, device):\n    style_X = preprocess(style_img, image_shape).to(device)\n    _, styles_Y = extract_features(style_X, content_layers, style_layers)\n    return style_X, styles_Y","execution_count":null},{"metadata":{"id":"4D8B8FBD826340CFAF1D77B6872AC4A6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 定义损失函数\n\n下面我们来描述样式迁移的损失函数。它由内容损失、样式损失和总变差损失3部分组成。\n\n### 内容损失\n\n与线性回归中的损失函数类似，内容损失通过平方误差函数衡量合成图像与内容图像在内容特征上的差异。平方误差函数的两个输入均为`extract_features`函数计算所得到的内容层的输出。"},{"metadata":{"id":"E4677A960CE04D67B378DC4C6E8C01FF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def content_loss(Y_hat, Y):\n    return F.mse_loss(Y_hat, Y)","execution_count":null},{"metadata":{"id":"FFAF6717A21143328DE2FE34A15C5EDA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 样式损失\n\n样式损失也一样通过平方误差函数衡量合成图像与样式图像在样式上的差异。为了表达样式层输出的样式，我们先通过`extract_features`函数计算样式层的输出。假设该输出的样本数为1，通道数为$c$，高和宽分别为$h$和$w$，我们可以把输出变换成$c$行$hw$列的矩阵$\\boldsymbol{X}$。矩阵$\\boldsymbol{X}$可以看作是由$c$个长度为$hw$的向量$\\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_c$组成的。其中向量$\\boldsymbol{x}_i$代表了通道$i$上的样式特征。这些向量的格拉姆矩阵（Gram matrix）$\\boldsymbol{X}\\boldsymbol{X}^\\top \\in \\mathbb{R}^{c \\times c}$中$i$行$j$列的元素$x_{ij}$即向量$\\boldsymbol{x}_i$与$\\boldsymbol{x}_j$的内积，它表达了通道$i$和通道$j$上样式特征的相关性。我们用这样的格拉姆矩阵表达样式层输出的样式。需要注意的是，当$hw$的值较大时，格拉姆矩阵中的元素容易出现较大的值。此外，格拉姆矩阵的高和宽皆为通道数$c$。为了让样式损失不受这些值的大小影响，下面定义的`gram`函数将格拉姆矩阵除以了矩阵中元素的个数，即$chw$。"},{"metadata":{"id":"C706559614EF4E948A1F47687BFC9A45","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def gram(X):\n    num_channels, n = X.shape[1], X.shape[2] * X.shape[3]\n    X = X.view(num_channels, n)\n    return torch.matmul(X, X.t()) / (num_channels * n)","execution_count":null},{"metadata":{"id":"B12C59D02B6F426BAF981983ED160078","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"自然地，样式损失的平方误差函数的两个格拉姆矩阵输入分别基于合成图像与样式图像的样式层输出。这里假设基于样式图像的格拉姆矩阵`gram_Y`已经预先计算好了。"},{"metadata":{"id":"3DDC6DCA342644E981AC7A5E62D14F4C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def style_loss(Y_hat, gram_Y):\n    return F.mse_loss(gram(Y_hat), gram_Y)","execution_count":null},{"metadata":{"id":"534DBBC99DBD48F99A4274F592D7DE93","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 总变差损失\n\n有时候，我们学到的合成图像里面有大量高频噪点，即有特别亮或者特别暗的颗粒像素。一种常用的降噪方法是总变差降噪（total variation denoising）。假设$x_{i,j}$表示坐标为$(i,j)$的像素值，降低总变差损失\n\n\n$$\n\\sum_{i,j} \\left|x_{i,j} - x_{i+1,j}\\right| + \\left|x_{i,j} - x_{i,j+1}\\right|\n$$\n\n\n能够尽可能使邻近的像素值相似。"},{"metadata":{"id":"D352CF1583884F4391077E262DDB0971","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def tv_loss(Y_hat):\n    return 0.5 * (F.l1_loss(Y_hat[:, :, 1:, :], Y_hat[:, :, :-1, :]) + \n                  F.l1_loss(Y_hat[:, :, :, 1:], Y_hat[:, :, :, :-1]))","execution_count":null},{"metadata":{"id":"431C11668E2049A8810E1AA5D2239100","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 损失函数\n\n样式迁移的损失函数即内容损失、样式损失和总变差损失的加权和。通过调节这些权值超参数，我们可以权衡合成图像在保留内容、迁移样式以及降噪三方面的相对重要性。"},{"metadata":{"id":"3FD59B8E23CD4A159A64A4DB9EAF2CFE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"content_weight, style_weight, tv_weight = 1, 1e3, 10\n\ndef compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):\n    # 分别计算内容损失、样式损失和总变差损失\n    contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(\n        contents_Y_hat, contents_Y)]\n    styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(\n        styles_Y_hat, styles_Y_gram)]\n    tv_l = tv_loss(X) * tv_weight\n    # 对所有损失求和\n    l = sum(styles_l) + sum(contents_l) + tv_l\n    return contents_l, styles_l, tv_l, l","execution_count":null},{"metadata":{"id":"A5C7AEFECD2949778AA25D1DC240C769","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 创建和初始化合成图像\n\n在样式迁移中，合成图像是唯一需要更新的变量。因此，我们可以定义一个简单的模型`GeneratedImage`，并将合成图像视为模型参数。模型的前向计算只需返回模型参数即可。"},{"metadata":{"id":"BE6FC1B0020A42A1892DF315620AA663","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class GeneratedImage(torch.nn.Module):\n    def __init__(self, img_shape):\n        super(GeneratedImage, self).__init__()\n        self.weight = torch.nn.Parameter(torch.rand(*img_shape))\n\n    def forward(self):\n        return self.weight","execution_count":null},{"metadata":{"id":"A1B2A90E80FA4FE18C7BC201F6CBE54E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"下面，我们定义`get_inits`函数。该函数创建了合成图像的模型实例，并将其初始化为图像`X`。样式图像在各个样式层的格拉姆矩阵`styles_Y_gram`将在训练前预先计算好。"},{"metadata":{"id":"BF5E6F5E10AF4AA689F51027040C9446","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def get_inits(X, device, lr, styles_Y):\n    gen_img = GeneratedImage(X.shape).to(device)\n    gen_img.weight.data = X.data\n    optimizer = torch.optim.Adam(gen_img.parameters(), lr=lr)\n    styles_Y_gram = [gram(Y) for Y in styles_Y]\n    return gen_img(), styles_Y_gram, optimizer","execution_count":null},{"metadata":{"id":"0F41CD62E7DF4053B85C6D67963912F7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 训练\n\n在训练模型时，我们不断抽取合成图像的内容特征和样式特征，并计算损失函数。"},{"metadata":{"id":"A0B14AEA6BC541DA8838F790C2BE3E4B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def train(X, contents_Y, styles_Y, device, lr, max_epochs, lr_decay_epoch):\n    print(\"training on \", device)\n    X, styles_Y_gram, optimizer = get_inits(X, device, lr, styles_Y)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_decay_epoch, gamma=0.1)\n    for i in range(max_epochs):\n        start = time.time()\n        \n        contents_Y_hat, styles_Y_hat = extract_features(\n                X, content_layers, style_layers)\n        contents_l, styles_l, tv_l, l = compute_loss(\n                X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n        \n        optimizer.zero_grad()\n        l.backward(retain_graph = True)\n        optimizer.step()\n        scheduler.step()\n        \n        if i % 50 == 0 and i != 0:\n            print('epoch %3d, content loss %.2f, style loss %.2f, '\n                  'TV loss %.2f, %.2f sec'\n                  % (i, sum(contents_l).item(), sum(styles_l).item(), tv_l.item(),\n                     time.time() - start))\n    return X.detach()","execution_count":null},{"metadata":{"id":"D6A952CF627648C98A50FC7493ECAA4B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"下面我们开始训练模型。首先将内容图像和样式图像的高和宽分别调整为150和225像素。合成图像将由内容图像来初始化。"},{"metadata":{"id":"0D6AFFE2579E46AA808ADA362FD3828A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"image_shape =  (150, 225)\nnet = net.to(device)\ncontent_X, contents_Y = get_contents(image_shape, device)\nstyle_X, styles_Y = get_styles(image_shape, device)\noutput = train(content_X, contents_Y, styles_Y, device, 0.01, 500, 200)","execution_count":null},{"metadata":{"id":"50B4BDF392D94ED9B9A185DA96CE498A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"下面我们将训练好的合成图像保存起来。可以看到图9.14中的合成图像保留了内容图像的风景和物体，并同时迁移了样式图像的色彩。因为图像尺寸较小，所以细节上依然比较模糊。"},{"metadata":{"id":"1C954E28542D4E5D8DBA155A92C280DC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"plt.imshow(postprocess(output));","execution_count":null},{"metadata":{"id":"AFF1BE4F4737461FA2FD5872CAF31F51","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"\n![Image Name](https://cdn.kesci.com/upload/image/q5w86ydczk.png?imageView2/0/w/960/h/960)\n\n\n\n为了得到更加清晰的合成图像，下面我们在更大的$300 \\times 450$尺寸上训练。我们将图9.14的高和宽放大2倍，以初始化更大尺寸的合成图像。"},{"metadata":{"id":"92492C6B82D14C1191BED158C297BFE3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"image_shape = (300, 450)\n_, content_Y = get_contents(image_shape, device)\n_, style_Y = get_styles(image_shape, device)\nX = preprocess(postprocess(output), image_shape).to(device)\nbig_output = train(X, content_Y, style_Y, device, 0.01, 500, 200)","execution_count":null},{"metadata":{"id":"72E06775DEA943288E3683CB517E7F95","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"plt.imshow(postprocess(big_output));","execution_count":null},{"metadata":{"id":"7E83D947CAA64FEDAA2C7A409445B785","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"可以看到，由于图像尺寸更大，每一次迭代需要花费更多的时间。从训练得到的图9.15中可以看到，此时的合成图像因为尺寸更大，所以保留了更多的细节。合成图像里面不仅有大块的类似样式图像的油画色彩块，色彩块中甚至出现了细微的纹理。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5w86hx9fd.png?imageView2/0/w/960/h/960)\n\n\n\n## 小结\n\n* 样式迁移常用的损失函数由3部分组成：内容损失使合成图像与内容图像在内容特征上接近，样式损失令合成图像与样式图像在样式特征上接近，而总变差损失则有助于减少合成图像中的噪点。\n* 可以通过预训练的卷积神经网络来抽取图像的特征，并通过最小化损失函数来不断更新合成图像。\n* 用格拉姆矩阵表达样式层输出的样式。\n\n\n## 练习\n\n* 选择不同的内容和样式层，输出有什么变化？\n* 调整损失函数中的权值超参数，输出是否保留更多内容或减少更多噪点？\n* 替换实验中的内容图像和样式图像，你能创作出更有趣的合成图像吗？\n\n## 参考文献\n\n[1] Gatys, L. A., Ecker, A. S., & Bethge, M. (2016). Image style transfer using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2414-2423).\n"},{"metadata":{"id":"2B4A2AAD911A42D8B2775EF9D07ACE7A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 图像分类案例1"},{"metadata":{"id":"F4A3FB68B6024F0C857865D2970123D0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Kaggle上的图像分类（CIFAR-10)\n\n> 利用前面几节中学到的知识来参加Kaggle竞赛，该竞赛解决了CIFAR-10图像分类问题。比赛网址是https://www.kaggle.com/c/cifar-10"},{"metadata":{"id":"1453F9BBB70647E49771052D6CA457B7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# https://www.kaggle.com/boyuai/boyu-d2l-image-classification-cifar-10\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport os\nimport time","execution_count":null},{"metadata":{"id":"C0F569DDBAEF4FAA8DFE34326ED8E5D6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"print(\"PyTorch Version: \",torch.__version__)","execution_count":null},{"metadata":{"id":"EEFBF33B7CB140D389CCE9F588FE174E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 获取和组织数据集\n\n比赛数据分为训练集和测试集。训练集包含 50,000 图片。测试集包含 300,000 图片。两个数据集中的图像格式均为PNG，高度和宽度均为32像素，并具有三个颜色通道（RGB）。图像涵盖10个类别：飞机，汽车，鸟类，猫，鹿，狗，青蛙，马，船和卡车。 为了更容易上手，我们提供了上述数据集的小样本。“ train_tiny.zip”包含 80 训练样本，而“ test_tiny.zip”包含100个测试样本。它们的未压缩文件夹名称分别是“ train_tiny”和“ test_tiny”。"},{"metadata":{"id":"96B510AA058740D5892083ECCA287B28","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 图像增强"},{"metadata":{"id":"D6BEEE786609473F8A964F428A7E7228","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"data_transform = transforms.Compose([\n    transforms.Resize(40),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32),\n    transforms.ToTensor()\n])\ntrainset = torchvision.datasets.ImageFolder(root='/home/kesci/input/CIFAR102891/cifar-10/train'\n                                            , transform=data_transform)","execution_count":null},{"metadata":{"id":"519974A03B1E480B8C50CE20371CD213","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"trainset[0][0].shape","execution_count":null},{"metadata":{"id":"5B0E6AB3D8284DB297942DC4C73E2CA0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"data = [d[0].data.cpu().numpy() for d in trainset]\nnp.mean(data)","execution_count":null},{"metadata":{"id":"B2C89CE48C3348FE8CFBB85243C886BE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"np.std(data)","execution_count":null},{"metadata":{"id":"831B26D4EC3C40D394B0E1B82A300CB3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 图像增强\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),  #先四周填充0，再把图像随机裁剪成32*32\n    transforms.RandomHorizontalFlip(),  #图像一半的概率翻转，一半的概率不翻转\n    transforms.ToTensor(),\n    transforms.Normalize((0.4731, 0.4822, 0.4465), (0.2212, 0.1994, 0.2010)), #R,G,B每层的归一化用到的均值和方差\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4731, 0.4822, 0.4465), (0.2212, 0.1994, 0.2010)),\n])","execution_count":null},{"metadata":{"id":"BD6D544F016A445C8934C317D8C7A7E5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 导入数据集"},{"metadata":{"id":"92DA4CD527B142D995F38039AB7C9324","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"train_dir = '/home/kesci/input/CIFAR102891/cifar-10/train'\ntest_dir = '/home/kesci/input/CIFAR102891/cifar-10/test'\n\ntrainset = torchvision.datasets.ImageFolder(root=train_dir, transform=transform_train)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True)\n\ntestset = torchvision.datasets.ImageFolder(root=test_dir, transform=transform_test)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False)\n\nclasses = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'forg', 'horse', 'ship', 'truck']","execution_count":null},{"metadata":{"id":"EAB93C09A7004506A2952F0621647EF0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 定义模型\n\nResNet-18网络结构：ResNet全名Residual Network残差网络。Kaiming He 的《Deep Residual Learning for Image Recognition》获得了CVPR最佳论文。他提出的深度残差网络在2015年可以说是洗刷了图像方面的各大比赛，以绝对优势取得了多个比赛的冠军。而且它在保证网络精度的前提下，将网络的深度达到了152层，后来又进一步加到1000的深度。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5x9kusfpk.png?imageView2/0/w/960/h/960)"},{"metadata":{"id":"E998D79D795D49F89924D9026A5E0171","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class ResidualBlock(nn.Module):   # 我们定义网络时一般是继承的torch.nn.Module创建新的子类\n\n    def __init__(self, inchannel, outchannel, stride=1):\n        super(ResidualBlock, self).__init__()\n        #torch.nn.Sequential是一个Sequential容器，模块将按照构造函数中传递的顺序添加到模块中。\n        self.left = nn.Sequential(\n            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False), \n            # 添加第一个卷积层,调用了nn里面的Conv2d（）\n            nn.BatchNorm2d(outchannel), # 进行数据的归一化处理\n            nn.ReLU(inplace=True), # 修正线性单元，是一种人工神经网络中常用的激活函数\n            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(outchannel)\n        )\n        self.shortcut = nn.Sequential() \n        if stride != 1 or inchannel != outchannel:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(outchannel)\n            )\n        #  便于之后的联合,要判断Y = self.left(X)的形状是否与X相同\n\n    def forward(self, x): # 将两个模块的特征进行结合，并使用ReLU激活函数得到最终的特征。\n        out = self.left(x)\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, ResidualBlock, num_classes=10):\n        super(ResNet, self).__init__()\n        self.inchannel = 64\n        self.conv1 = nn.Sequential( # 用3个3x3的卷积核代替7x7的卷积核，减少模型参数\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n        ) \n        self.layer1 = self.make_layer(ResidualBlock, 64,  2, stride=1)\n        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n        self.fc = nn.Linear(512, num_classes)\n\n    def make_layer(self, block, channels, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)   #第一个ResidualBlock的步幅由make_layer的函数参数stride指定\n        # ，后续的num_blocks-1个ResidualBlock步幅是1\n        layers = []\n        for stride in strides:\n            layers.append(block(self.inchannel, channels, stride))\n            self.inchannel = channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n\n\ndef ResNet18():\n    return ResNet(ResidualBlock)","execution_count":null},{"metadata":{"id":"726D1385D7384340B3D641D04CD00611","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 训练和测试"},{"metadata":{"id":"BCF5387DD23A4FE59A5A5D66C1710600","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 定义是否使用GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 超参数设置\nEPOCH = 20   #遍历数据集次数\npre_epoch = 0  # 定义已经遍历数据集的次数\nLR = 0.1        #学习率\n\n# 模型定义-ResNet\nnet = ResNet18().to(device)\n\n# 定义损失函数和优化方式\ncriterion = nn.CrossEntropyLoss()  #损失函数为交叉熵，多用于多分类问题\noptimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4) \n#优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n\n# 训练\nif __name__ == \"__main__\":\n    print(\"Start Training, Resnet-18!\")\n    num_iters = 0\n    for epoch in range(pre_epoch, EPOCH):\n        print('\\nEpoch: %d' % (epoch + 1))\n        net.train()\n        sum_loss = 0.0\n        correct = 0.0\n        total = 0\n        for i, data in enumerate(trainloader, 0): \n            #用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，\n            #下标起始位置为0，返回 enumerate(枚举) 对象。\n            \n            num_iters += 1\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()  # 清空梯度\n\n            # forward + backward\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            sum_loss += loss.item() * labels.size(0)\n            _, predicted = torch.max(outputs, 1) #选出每一列中最大的值作为预测结果\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            # 每20个batch打印一次loss和准确率\n            if (i + 1) % 20 == 0:\n                print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% '\n                        % (epoch + 1, num_iters, sum_loss / (i + 1), 100. * correct / total))\n\n    print(\"Training Finished, TotalEPOCH=%d\" % EPOCH)","execution_count":null},{"metadata":{"id":"A3E2646E9CC046C084205F20A26E81A6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Task08：图像分类案例2；GAN；DCGAN\n- 图像分类案例2\n- GAN\n- DCGAN"},{"metadata":{"id":"2D33E9FEF7FE46B9993EBC25F2A96C71","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 图像分类案例2"},{"metadata":{"id":"2DBBA2C3BD8C47F89F0D5E9A94C23D2A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"#  Kaggle上的狗品种识别（ImageNet Dogs）\n\n> 在本节中，我们将解决Kaggle竞赛中的犬种识别挑战，比赛的网址是https://www.kaggle.com/c/dog-breed-identification 在这项比赛中，我们尝试确定120种不同的狗。该比赛中使用的数据集实际上是著名的ImageNet数据集的子集。"},{"metadata":{"id":"39DEBA1FCF8F41198FBA9CA96B402D6B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# https://www.kaggle.com/boyuai/boyu-d2l-dog-breed-identification-imagenet-dogs\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport os\nimport shutil\nimport time\nimport pandas as pd\nimport random","execution_count":null},{"metadata":{"id":"38535B306C7640BC855B6DC79BC203FC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 设置随机数种子\nrandom.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)","execution_count":null},{"metadata":{"id":"3B364374E66E48FAAE1C8B65EE27F237","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 整理数据集\n\n我们可以从比赛网址上下载数据集，其目录结构为：\n\n```\n| Dog Breed Identification\n    | train\n    |   | 000bec180eb18c7604dcecc8fe0dba07.jpg\n    |   | 00a338a92e4e7bf543340dc849230e75.jpg\n    |   | ...\n    | test\n    |   | 00a3edd22dc7859c487a64777fc8d093.jpg\n    |   | 00a6892e5c7f92c1f465e213fd904582.jpg\n    |   | ...\n    | labels.csv\n    | sample_submission.csv\n```\n\ntrain和test目录下分别是训练集和测试集的图像，训练集包含10,222张图像，测试集包含10,357张图像，图像格式都是JPEG，每张图像的文件名是一个唯一的id。labels.csv包含训练集图像的标签，文件包含10,222行，每行包含两列，第一列是图像id，第二列是狗的类别。狗的类别一共有120种。\n\n我们希望对数据进行整理，方便后续的读取，我们的主要目标是：\n\n* 从训练集中划分出验证数据集，用于调整超参数。划分之后，数据集应该包含4个部分：划分后的训练集、划分后的验证集、完整训练集、完整测试集\n* 对于4个部分，建立4个文件夹：train, valid, train_valid, test。在上述文件夹中，对每个类别都建立一个文件夹，在其中存放属于该类别的图像。前三个部分的标签已知，所以各有120个子文件夹，而测试集的标签未知，所以仅建立一个名为unknown的子文件夹，存放所有测试数据。\n\n我们希望整理后的数据集目录结构为：\n```\n| train_valid_test\n    | train\n    |   | affenpinscher\n    |   |   | 00ca18751837cd6a22813f8e221f7819.jpg\n    |   |   | ...\n    |   | afghan_hound\n    |   |   | 0a4f1e17d720cdff35814651402b7cf4.jpg\n    |   |   | ...\n    |   | ...\n    | valid\n    |   | affenpinscher\n    |   |   | 56af8255b46eb1fa5722f37729525405.jpg\n    |   |   | ...\n    |   | afghan_hound\n    |   |   | 0df400016a7e7ab4abff824bf2743f02.jpg\n    |   |   | ...\n    |   | ...\n    | train_valid\n    |   | affenpinscher\n    |   |   | 00ca18751837cd6a22813f8e221f7819.jpg\n    |   |   | ...\n    |   | afghan_hound\n    |   |   | 0a4f1e17d720cdff35814651402b7cf4.jpg\n    |   |   | ...\n    |   | ...\n    | test\n    |   | unknown\n    |   |   | 00a3edd22dc7859c487a64777fc8d093.jpg\n    |   |   | ...\n```"},{"metadata":{"id":"57564EC118C649F88185F1E349E65942","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"data_dir = '/home/kesci/input/Kaggle_Dog6357/dog-breed-identification'  # 数据集目录\nlabel_file, train_dir, test_dir = 'labels.csv', 'train', 'test'  # data_dir中的文件夹、文件\nnew_data_dir = './train_valid_test'  # 整理之后的数据存放的目录\nvalid_ratio = 0.1  # 验证集所占比例","execution_count":null},{"metadata":{"id":"949F6D6378C14D49B630A3D387880666","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def mkdir_if_not_exist(path):\n    # 若目录path不存在，则创建目录\n    if not os.path.exists(os.path.join(*path)):\n        os.makedirs(os.path.join(*path))\n        \ndef reorg_dog_data(data_dir, label_file, train_dir, test_dir, new_data_dir, valid_ratio):\n    # 读取训练数据标签\n    labels = pd.read_csv(os.path.join(data_dir, label_file))\n    id2label = {Id: label for Id, label in labels.values}  # (key: value): (id: label)\n\n    # 随机打乱训练数据\n    train_files = os.listdir(os.path.join(data_dir, train_dir))\n    random.shuffle(train_files)    \n\n    # 原训练集\n    valid_ds_size = int(len(train_files) * valid_ratio)  # 验证集大小\n    for i, file in enumerate(train_files):\n        img_id = file.split('.')[0]  # file是形式为id.jpg的字符串\n        img_label = id2label[img_id]\n        if i < valid_ds_size:\n            mkdir_if_not_exist([new_data_dir, 'valid', img_label])\n            shutil.copy(os.path.join(data_dir, train_dir, file),\n                        os.path.join(new_data_dir, 'valid', img_label))\n        else:\n            mkdir_if_not_exist([new_data_dir, 'train', img_label])\n            shutil.copy(os.path.join(data_dir, train_dir, file),\n                        os.path.join(new_data_dir, 'train', img_label))\n        mkdir_if_not_exist([new_data_dir, 'train_valid', img_label])\n        shutil.copy(os.path.join(data_dir, train_dir, file),\n                    os.path.join(new_data_dir, 'train_valid', img_label))\n\n    # 测试集\n    mkdir_if_not_exist([new_data_dir, 'test', 'unknown'])\n    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n                    os.path.join(new_data_dir, 'test', 'unknown'))","execution_count":null},{"metadata":{"id":"630DB0CBCEB3441A8BFAEADF3A7DD94B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"reorg_dog_data(data_dir, label_file, train_dir, test_dir, new_data_dir, valid_ratio)","execution_count":null},{"metadata":{"id":"3EA6554C34F14572B7A59BDC9B9CC93B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 图像增强"},{"metadata":{"id":"62537AC425F14837966D70AE329ACA82","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"transform_train = transforms.Compose([\n    # 随机对图像裁剪出面积为原图像面积0.08~1倍、且高和宽之比在3/4~4/3的图像，再放缩为高和宽均为224像素的新图像\n    transforms.RandomResizedCrop(224, scale=(0.08, 1.0),  \n                                 ratio=(3.0/4.0, 4.0/3.0)),\n    # 以0.5的概率随机水平翻转\n    transforms.RandomHorizontalFlip(),\n    # 随机更改亮度、对比度和饱和度\n    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n    transforms.ToTensor(),\n    # 对各个通道做标准化，(0.485, 0.456, 0.406)和(0.229, 0.224, 0.225)是在ImageNet上计算得的各通道均值与方差\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet上的均值和方差\n])\n\n# 在测试集上的图像增强只做确定性的操作\ntransform_test = transforms.Compose([\n    transforms.Resize(256),\n    # 将图像中央的高和宽均为224的正方形区域裁剪出来\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])","execution_count":null},{"metadata":{"id":"C884CC4AD3BB49F899C38B9771837569","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 读取数据"},{"metadata":{"id":"8D6DA4B7F0D14B55827ABBD3445CB1A7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# new_data_dir目录下有train, valid, train_valid, test四个目录\n# 这四个目录中，每个子目录表示一种类别，目录中是属于该类别的所有图像\ntrain_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'train'),\n                                            transform=transform_train)\nvalid_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'valid'),\n                                            transform=transform_test)\ntrain_valid_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'train_valid'),\n                                            transform=transform_train)\ntest_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'test'),\n                                            transform=transform_test)","execution_count":null},{"metadata":{"id":"BDC1DFC5214E4909A28F7C202F309F4F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"batch_size = 128\ntrain_iter = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nvalid_iter = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\ntrain_valid_iter = torch.utils.data.DataLoader(train_valid_ds, batch_size=batch_size, shuffle=True)\ntest_iter = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)  # shuffle=False","execution_count":null},{"metadata":{"id":"EF78C27EBECB44CE8CF0CFBC96F8D9E2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 定义模型\n\n这个比赛的数据属于ImageNet数据集的子集，我们使用微调的方法，选用在ImageNet完整数据集上预训练的模型来抽取图像特征，以作为自定义小规模输出网络的输入。\n\n此处我们使用与训练的ResNet-34模型，直接复用预训练模型在输出层的输入，即抽取的特征，然后我们重新定义输出层，本次我们仅对重定义的输出层的参数进行训练，而对于用于抽取特征的部分，我们保留预训练模型的参数。"},{"metadata":{"id":"67A8B79C052947928425CB5A7DDC9070","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def get_net(device):\n    finetune_net = models.resnet34(pretrained=False)  # 预训练的resnet34网络\n    finetune_net.load_state_dict(torch.load('/home/kesci/input/resnet347742/resnet34-333f7ec4.pth'))\n    for param in finetune_net.parameters():  # 冻结参数\n        param.requires_grad = False\n    # 原finetune_net.fc是一个输入单元数为512，输出单元数为1000的全连接层\n    # 替换掉原finetune_net.fc，新finetuen_net.fc中的模型参数会记录梯度\n    finetune_net.fc = nn.Sequential(\n        nn.Linear(in_features=512, out_features=256),\n        nn.ReLU(),\n        nn.Linear(in_features=256, out_features=120)  # 120是输出类别数\n    )\n    return finetune_net","execution_count":null},{"metadata":{"id":"6668C6FDFF044111A2CA41391363ADFE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 定义训练函数"},{"metadata":{"id":"725B3FB6D58A4AE581B7266BF9F84B73","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def evaluate_loss_acc(data_iter, net, device):\n    # 计算data_iter上的平均损失与准确率\n    loss = nn.CrossEntropyLoss()\n    is_training = net.training  # Bool net是否处于train模式\n    net.eval()\n    l_sum, acc_sum, n = 0, 0, 0\n    with torch.no_grad():\n        for X, y in data_iter:\n            X, y = X.to(device), y.to(device)\n            y_hat = net(X)\n            l = loss(y_hat, y)\n            l_sum += l.item() * y.shape[0]\n            acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n            n += y.shape[0]\n    net.train(is_training)  # 恢复net的train/eval状态\n    return l_sum / n, acc_sum / n","execution_count":null},{"metadata":{"id":"1152873805124699BECACD130D5D23BF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def train(net, train_iter, valid_iter, num_epochs, lr, wd, device, lr_period,\n          lr_decay):\n    loss = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.fc.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n    net = net.to(device)\n    for epoch in range(num_epochs):\n        train_l_sum, n, start = 0.0, 0, time.time()\n        if epoch > 0 and epoch % lr_period == 0:  # 每lr_period个epoch，学习率衰减一次\n            lr = lr * lr_decay\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr\n        for X, y in train_iter:\n            X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()\n            y_hat = net(X)\n            l = loss(y_hat, y)\n            l.backward()\n            optimizer.step()\n            train_l_sum += l.item() * y.shape[0]\n            n += y.shape[0]\n        time_s = \"time %.2f sec\" % (time.time() - start)\n        if valid_iter is not None:\n            valid_loss, valid_acc = evaluate_loss_acc(valid_iter, net, device)\n            epoch_s = (\"epoch %d, train loss %f, valid loss %f, valid acc %f, \"\n                       % (epoch + 1, train_l_sum / n, valid_loss, valid_acc))\n        else:\n            epoch_s = (\"epoch %d, train loss %f, \"\n                       % (epoch + 1, train_l_sum / n))\n        print(epoch_s + time_s + ', lr ' + str(lr))","execution_count":null},{"metadata":{"id":"90BEAA6C190449428BA369473C2A23A8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 调参"},{"metadata":{"id":"6E1908F5C1824A088D0CA866A628164D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"num_epochs, lr_period, lr_decay = 20, 10, 0.1\nlr, wd = 0.03, 1e-4\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null},{"metadata":{"id":"84604553CB9D4022884A4AE3EDA85341","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"net = get_net(device)\ntrain(net, train_iter, valid_iter, num_epochs, lr, wd, device, lr_period, lr_decay)","execution_count":null},{"metadata":{"id":"B1BAC6E359D14F869B08B3CE22554080","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 在完整数据集上训练模型"},{"metadata":{"id":"7F8B23E63B7642A8942D7898A50263A2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 使用上面的参数设置，在完整数据集上训练模型大致需要40-50分钟的时间\nnet = get_net(device)\ntrain(net, train_valid_iter, None, num_epochs, lr, wd, device, lr_period, lr_decay)","execution_count":null},{"metadata":{"id":"E1CFAEB7A7724F01883D1A4DC273ABC5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 对测试集分类并提交结果\n\n用训练好的模型对测试数据进行预测。比赛要求对测试集中的每张图片，都要预测其属于各个类别的概率。"},{"metadata":{"id":"1738508364424BC8883D031E38C8A462","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"preds = []\nfor X, _ in test_iter:\n    X = X.to(device)\n    output = net(X)\n    output = torch.softmax(output, dim=1)\n    preds += output.tolist()\nids = sorted(os.listdir(os.path.join(new_data_dir, 'test/unknown')))\nwith open('submission.csv', 'w') as f:\n    f.write('id,' + ','.join(train_valid_ds.classes) + '\\n')\n    for i, output in zip(ids, preds):\n        f.write(i.split('.')[0] + ',' + ','.join(\n            [str(num) for num in output]) + '\\n')","execution_count":null},{"metadata":{"id":"F94C8905E7CB4497B88FFEED607A15C3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# GAN\n> 介绍生成对抗网络的原理和实现"},{"metadata":{"id":"9CEB030D5F4C40098B9F49A37D04A92B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Generative Adversarial Networks\n\n\nThroughout most of this book, we have talked about how to make predictions. In some form or another, we used deep neural networks learned mappings from data points to labels. This kind of learning is called discriminative learning, as in, we'd like to be able to discriminate between photos cats and photos of dogs. Classifiers and regressors are both examples of discriminative learning. And neural networks trained by backpropagation have upended everything we thought we knew about discriminative learning on large complicated datasets. Classification accuracies on high-res images has gone from useless to human-level (with some caveats) in just 5-6 years. We will spare you another spiel about all the other discriminative tasks where deep neural networks do astoundingly well.\n\nBut there is more to machine learning than just solving discriminative tasks. For example, given a large dataset, without any labels, we might want to learn a model that concisely captures the characteristics of this data. Given such a model, we could sample synthetic data points that resemble the distribution of the training data. For example, given a large corpus of photographs of faces, we might want to be able to generate a new photorealistic image that looks like it might plausibly have come from the same dataset. This kind of learning is called generative modeling.\n\nUntil recently, we had no method that could synthesize novel photorealistic images. But the success of deep neural networks for discriminative learning opened up new possibilities. One big trend over the last three years has been the application of discriminative deep nets to overcome challenges in problems that we do not generally think of as supervised learning problems. The recurrent neural network language models are one example of using a discriminative network (trained to predict the next character) that once trained can act as a generative model.\n\nIn 2014, a breakthrough paper introduced Generative adversarial networks (GANs) `Goodfellow.Pouget-Abadie.Mirza.ea.2014`, a clever new way to leverage the power of discriminative models to get good generative models. At their heart, GANs rely on the idea that a data generator is good if we cannot tell fake data apart from real data. In statistics, this is called a two-sample test - a test to answer the question whether datasets $X=\\{x_1,\\ldots, x_n\\}$ and $X'=\\{x'_1,\\ldots, x'_n\\}$ were drawn from the same distribution. The main difference between most statistics papers and GANs is that the latter use this idea in a constructive way. In other words, rather than just training a model to say \"hey, these two datasets do not look like they came from the same distribution\", they use the [two-sample test](https://en.wikipedia.org/wiki/Two-sample_hypothesis_testing) to provide training signals to a generative model. This allows us to improve the data generator until it generates something that resembles the real data. At the very least, it needs to fool the classifier. Even if our classifier is a state of the art deep neural network.\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5tv0m8ro4.jpg?imageView2/0/w/320/h/320)\n\n\nThe GAN architecture is illustrated.As you can see, there are two pieces in GAN architecture - first off, we need a device (say, a deep network but it really could be anything, such as a game rendering engine) that might potentially be able to generate data that looks just like the real thing. If we are dealing with images, this needs to generate images. If we are dealing with speech, it needs to generate audio sequences, and so on. We call this the generator network. The second component is the discriminator network. It attempts to distinguish fake and real data from each other. Both networks are in competition with each other. The generator network attempts to fool the discriminator network. At that point, the discriminator network adapts to the new fake data. This information, in turn is used to improve the generator network, and so on.\n\n\nThe discriminator is a binary classifier to distinguish if the input $x$ is real (from real data) or fake (from the generator). Typically, the discriminator outputs a scalar prediction $o\\in\\mathbb R$ for input $\\mathbf x$, such as using a dense layer with hidden size 1, and then applies sigmoid function to obtain the predicted probability $D(\\mathbf x) = 1/(1+e^{-o})$. Assume the label $y$ for the true data is $1$ and $0$ for the fake data. We train the discriminator to minimize the cross-entropy loss, *i.e.*,\n\n\n$$\n \\min_D \\{ - y \\log D(\\mathbf x) - (1-y)\\log(1-D(\\mathbf x)) \\},\n$$\n\n\nFor the generator, it first draws some parameter $\\mathbf z\\in\\mathbb R^d$ from a source of randomness, *e.g.*, a normal distribution $\\mathbf z \\sim \\mathcal{N} (0, 1)$. We often call $\\mathbf z$ as the latent variable. It then applies a function to generate $\\mathbf x'=G(\\mathbf z)$. The goal of the generator is to fool the discriminator to classify $\\mathbf x'=G(\\mathbf z)$ as true data, *i.e.*, we want $D( G(\\mathbf z)) \\approx 1$. In other words, for a given discriminator $D$, we update the parameters of the generator $G$ to maximize the cross-entropy loss when $y=0$, *i.e.*,\n\n\n$$\n \\max_G \\{ - (1-y) \\log(1-D(G(\\mathbf z))) \\} = \\max_G \\{ - \\log(1-D(G(\\mathbf z))) \\}.\n$$\n\n\nIf the discriminator does a perfect job, then $D(\\mathbf x')\\approx 0$ so the above loss near 0, which results the gradients are too small to make a good progress for the generator. So commonly we minimize the following loss:\n\n\n$$\n \\min_G \\{ - y \\log(D(G(\\mathbf z))) \\} = \\min_G \\{ - \\log(D(G(\\mathbf z))) \\}, \n$$\n\n\nwhich is just feed $\\mathbf x'=G(\\mathbf z)$ into the discriminator but giving label $y=1$.\n\n\nTo sum up, $D$ and $G$ are playing a \"minimax\" game with the comprehensive objective function:\n\n\n$$\nmin_D max_G \\{ -E_{x \\sim \\text{Data}} log D(\\mathbf x) - E_{z \\sim \\text{Noise}} log(1 - D(G(\\mathbf z))) \\}.\n$$\n\n\n\n\nMany of the GANs applications are in the context of images. As a demonstration purpose, we are going to content ourselves with fitting a much simpler distribution first. We will illustrate what happens if we use GANs to build the world's most inefficient estimator of parameters for a Gaussian. Let's get started."},{"metadata":{"id":"EE8ABD7C954447919C86AD40B6D504B5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport numpy as np\nfrom torch.autograd import Variable\nimport torch","execution_count":null},{"metadata":{"id":"91DAB2565FA2430E8B530BBAADD3D0B3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Generate some \"real\" data\n\nSince this is going to be the world's lamest example, we simply generate data drawn from a Gaussian."},{"metadata":{"id":"8E4932A38FB243C280681E701B5DB424","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"X=np.random.normal(size=(1000,2))\nA=np.array([[1,2],[-0.1,0.5]])\nb=np.array([1,2])\ndata=X.dot(A)+b","execution_count":null},{"metadata":{"id":"2F72BCC179594E82B9B5EE76BAC83D38","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"Let's see what we got. This should be a Gaussian shifted in some rather arbitrary way with mean $b$ and covariance matrix $A^TA$."},{"metadata":{"id":"915798EDF2674B4084AF1B7513211BCC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"plt.figure(figsize=(3.5,2.5))\nplt.scatter(X[:100,0],X[:100,1],color='red')\nplt.show()\nplt.figure(figsize=(3.5,2.5))\nplt.scatter(data[:100,0],data[:100,1],color='blue')\nplt.show()\nprint(\"The covariance matrix is\\n%s\" % np.dot(A.T, A))","execution_count":null},{"metadata":{"id":"103D6E6775D14A2A8BFB510170C7BF27","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"batch_size=8\ndata_iter=DataLoader(data,batch_size=batch_size)","execution_count":null},{"metadata":{"id":"4CE2ACA6425A463D8ACA7B2EC11DE2C5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Generator\n\nOur generator network will be the simplest network possible - a single layer linear model. This is since we will be driving that linear network with a Gaussian data generator. Hence, it literally only needs to learn the parameters to fake things perfectly."},{"metadata":{"id":"553385BA107A4AC18197B4F4C2EF3B60","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class net_G(nn.Module):\n    def __init__(self):\n        super(net_G,self).__init__()\n        self.model=nn.Sequential(\n            nn.Linear(2,2),\n        )\n        self._initialize_weights()\n    def forward(self,x):\n        x=self.model(x)\n        return x\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m,nn.Linear):\n                m.weight.data.normal_(0,0.02)\n                m.bias.data.zero_()","execution_count":null},{"metadata":{"id":"2C3993EA46CA497195CCFE5B401FF237","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Discriminator\n\nFor the discriminator we will be a bit more discriminating: we will use an MLP with 3 layers to make things a bit more interesting."},{"metadata":{"id":"401B0B76A445465C82C19F5957F61D39","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class net_D(nn.Module):\n    def __init__(self):\n        super(net_D,self).__init__()\n        self.model=nn.Sequential(\n            nn.Linear(2,5),\n            nn.Tanh(),\n            nn.Linear(5,3),\n            nn.Tanh(),\n            nn.Linear(3,1),\n            nn.Sigmoid()\n        )\n        self._initialize_weights()\n    def forward(self,x):\n        x=self.model(x)\n        return x\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m,nn.Linear):\n                m.weight.data.normal_(0,0.02)\n                m.bias.data.zero_()","execution_count":null},{"metadata":{"id":"40746542386F4A63994BBE23DEBFF73A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Training\n\nFirst we define a function to update the discriminator."},{"metadata":{"id":"CD6B521E46DE4E8E80C4F008DD7D1460","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# Saved in the d2l package for later use\ndef update_D(X,Z,net_D,net_G,loss,trainer_D):\n    batch_size=X.shape[0]\n    Tensor=torch.FloatTensor\n    ones=Variable(Tensor(np.ones(batch_size))).view(batch_size,1)\n    zeros = Variable(Tensor(np.zeros(batch_size))).view(batch_size,1)\n    real_Y=net_D(X.float())\n    fake_X=net_G(Z)\n    fake_Y=net_D(fake_X)\n    loss_D=(loss(real_Y,ones)+loss(fake_Y,zeros))/2\n    loss_D.backward()\n    trainer_D.step()\n    return float(loss_D.sum())","execution_count":null},{"metadata":{"id":"AC34A67EF2254C9FAEAC6C4FDC4EAF63","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"The generator is updated similarly. Here we reuse the cross-entropy loss but change the label of the fake data from $0$ to $1$."},{"metadata":{"id":"7121AFA733A149AD82AF9A05AA913A8C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# Saved in the d2l package for later use\ndef update_G(Z,net_D,net_G,loss,trainer_G):\n    batch_size=Z.shape[0]\n    Tensor=torch.FloatTensor\n    ones=Variable(Tensor(np.ones((batch_size,)))).view(batch_size,1)\n    fake_X=net_G(Z)\n    fake_Y=net_D(fake_X)\n    loss_G=loss(fake_Y,ones)\n    loss_G.backward()\n    trainer_G.step()\n    return float(loss_G.sum())","execution_count":null},{"metadata":{"id":"E4989833A37444A2ADE9D74A6CDCAE74","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"Both the discriminator and the generator performs a binary logistic regression with the cross-entropy loss. We use Adam to smooth the training process. In each iteration, we first update the discriminator and then the generator. We visualize both losses and generated examples."},{"metadata":{"id":"694FB69E61CE416C990062F4711C4FF7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def train(net_D,net_G,data_iter,num_epochs,lr_D,lr_G,latent_dim,data):\n    loss=nn.BCELoss()\n    Tensor=torch.FloatTensor\n    trainer_D=torch.optim.Adam(net_D.parameters(),lr=lr_D)\n    trainer_G=torch.optim.Adam(net_G.parameters(),lr=lr_G)\n    plt.figure(figsize=(7,4))\n    d_loss_point=[]\n    g_loss_point=[]\n    d_loss=0\n    g_loss=0\n    for epoch in range(1,num_epochs+1):\n        d_loss_sum=0\n        g_loss_sum=0\n        batch=0\n        for X in data_iter:\n            batch+=1\n            X=Variable(X)\n            batch_size=X.shape[0]\n            Z=Variable(Tensor(np.random.normal(0,1,(batch_size,latent_dim))))\n            trainer_D.zero_grad()\n            d_loss = update_D(X, Z, net_D, net_G, loss, trainer_D)\n            d_loss_sum+=d_loss\n            trainer_G.zero_grad()\n            g_loss = update_G(Z, net_D, net_G, loss, trainer_G)\n            g_loss_sum+=g_loss\n        d_loss_point.append(d_loss_sum/batch)\n        g_loss_point.append(g_loss_sum/batch)\n    plt.ylabel('Loss', fontdict={'size': 14})\n    plt.xlabel('epoch', fontdict={'size': 14})\n    plt.xticks(range(0,num_epochs+1,3))\n    plt.plot(range(1,num_epochs+1),d_loss_point,color='orange',label='discriminator')\n    plt.plot(range(1,num_epochs+1),g_loss_point,color='blue',label='generator')\n    plt.legend()\n    plt.show()\n    print(d_loss,g_loss)\n    \n    Z =Variable(Tensor( np.random.normal(0, 1, size=(100, latent_dim))))\n    fake_X=net_G(Z).detach().numpy()\n    plt.figure(figsize=(3.5,2.5))\n    plt.scatter(data[:,0],data[:,1],color='blue',label='real')\n    plt.scatter(fake_X[:,0],fake_X[:,1],color='orange',label='generated')\n    plt.legend()\n    plt.show()","execution_count":null},{"metadata":{"id":"1BB29640625742F480BE41174338592E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"Now we specify the hyper-parameters to fit the Gaussian distribution."},{"metadata":{"id":"9885C360AE7C43068850C3245794FC67","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"if __name__ == '__main__':\n    lr_D,lr_G,latent_dim,num_epochs=0.05,0.005,2,20\n    generator=net_G()\n    discriminator=net_D()\n    train(discriminator,generator,data_iter,num_epochs,lr_D,lr_G,latent_dim,data)","execution_count":null},{"metadata":{"id":"8908BDC6D8D7472F8B8082023F4BDC32","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Summary\n\n* Generative adversarial networks (GANs) composes of two deep networks, the generator and the discriminator.\n* The generator generates the image as much closer to the true image as possible to fool the discriminator, via maximizing the cross-entropy loss, *i.e.*, $\\max \\log(D(\\mathbf{x'}))$.\n* The discriminator tries to distinguish the generated images from the true images, via minimizing the cross-entropy loss, *i.e.*, $\\min - y \\log D(\\mathbf{x}) - (1-y)\\log(1-D(\\mathbf{x}))$.\n\n## Exercises\n\n* Does an equilibrium exist where the generator wins, *i.e.* the discriminator ends up unable to distinguish the two distributions on finite samples?\n"},{"metadata":{"id":"F731242CAA6747F88958A8D8A2F2612E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# DCGAN\n> 介绍Deep Convolutional Generative Adversarial Networks的原理和实现"},{"metadata":{"id":"A87BFB726C964FCCA62643B3E580E0B5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Deep Convolutional Generative Adversarial Networks\n\nwe introduced the basic ideas behind how GANs work. We showed that they can draw samples from some simple, easy-to-sample distribution, like a uniform or normal distribution, and transform them into samples that appear to match the distribution of some dataset. And while our example of matching a 2D Gaussian distribution got the point across, it is not especially exciting.\n\nIn this section, we will demonstrate how you can use GANs to generate photorealistic images. We will be basing our models on the deep convolutional GANs (DCGAN) introduced in :cite:`Radford.Metz.Chintala.2015`. We will borrow the convolutional architecture that have proven so successful for discriminative computer vision problems and show how via GANs, they can be leveraged to generate photorealistic images."},{"metadata":{"id":"1AE7D36E7B0F43D0B9C460D530EC1F35","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport numpy as np\nfrom torch.autograd import Variable\nimport torch\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import transforms\nimport zipfile\ncuda = True if torch.cuda.is_available() else False\nprint(cuda)","execution_count":null},{"metadata":{"id":"A958972502874310906F3380B992F3B2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## The Pokemon Dataset\n\nThe dataset we will use is a collection of Pokemon sprites obtained from [pokemondb](https://pokemondb.net/sprites). First download, extract and load this dataset."},{"metadata":{"id":"03180EA5F19D4B9D8B17DC239FE8FD2C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"We resize each image into $64\\times 64$. The `ToTensor` transformation will project the pixel value into $[0, 1]$, while our generator will use the tanh function to obtain outputs in $[-1, 1]$. Therefore we normalize the data with $0.5$ mean and $0.5$ standard deviation to match the value range."},{"metadata":{"id":"AB77B676A49B48248ADA1A6BD4834B6E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"data_dir='/home/kesci/input/pokemon8600/'\nbatch_size=256\ntransform=transforms.Compose([\n    transforms.Resize((64,64)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n])\npokemon=ImageFolder(data_dir+'pokemon',transform)\ndata_iter=DataLoader(pokemon,batch_size=batch_size,shuffle=True)","execution_count":null},{"metadata":{"id":"8C2D290DA41D4A67886D5CAE21810E3E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# Let's visualize the first 20 images.\nfig=plt.figure(figsize=(4,4))\nimgs=data_iter.dataset.imgs\nfor i in range(20):\n    img = plt.imread(imgs[i*150][0])\n    plt.subplot(4,5,i+1)\n    plt.imshow(img)\n    plt.axis('off')\nplt.show()","execution_count":null},{"metadata":{"id":"93A7A6A5324E446F81A7F05FFD65862F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## The Generator\n\nThe generator needs to map the noise variable $\\mathbf z\\in\\mathbb R^d$, a length-$d$ vector, to a RGB image with width and height to be $64\\times 64$ . In :numref:`sec_fcn` we introduced the fully convolutional network that uses transposed convolution layer (refer to :numref:`sec_transposed_conv`) to enlarge input size. The basic block of the generator contains a transposed convolution layer followed by the batch normalization and ReLU activation."},{"metadata":{"id":"42AB54191456439A8D1F093E7DC1B510","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class G_block(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=4,strides=2, padding=1):\n        super(G_block,self).__init__()\n        self.conv2d_trans=nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size,\n                                             stride=strides, padding=padding, bias=False)\n        self.batch_norm=nn.BatchNorm2d(out_channels,0.8)\n        self.activation=nn.ReLU()\n    def forward(self,x):\n        return self.activation(self.batch_norm(self.conv2d_trans(x)))","execution_count":null},{"metadata":{"id":"77792DA6C32D44EEADAE821687BEB191","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"In default, the transposed convolution layer uses a $k_h = k_w = 4$ kernel, a $s_h = s_w = 2$ strides, and a $p_h = p_w = 1$ padding. With a input shape of $n_h^{'} \\times n_w^{'} = 16 \\times 16$, the generator block will double input's width and height.\n\n\n$$\n\\begin{aligned}\nn_h^{'} \\times n_w^{'} &= [(n_h k_h - (n_h-1)(k_h-s_h)- 2p_h] \\times [(n_w k_w - (n_w-1)(k_w-s_w)- 2p_w]\\\\\n  &= [(k_h + s_h (n_h-1)- 2p_h] \\times [(k_w + s_w (n_w-1)- 2p_w]\\\\\n  &= [(4 + 2 \\times (16-1)- 2 \\times 1] \\times [(4 + 2 \\times (16-1)- 2 \\times 1]\\\\\n  &= 32 \\times 32 .\\\\\n\\end{aligned}\n$$\n"},{"metadata":{"id":"3FEC079B51354340973D13C63403EF0F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"Tensor=torch.cuda.FloatTensor\nx=Variable(Tensor(np.zeros((2,3,16,16))))\ng_blk=G_block(3,20)\ng_blk.cuda()\nprint(g_blk(x).shape)","execution_count":null},{"metadata":{"id":"BB7456B5FF1B485A8EB6925571D647C2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"If changing the transposed convolution layer to a $4\\times 4$ kernel, $1\\times 1$ strides and zero padding. With a input size of $1 \\times 1$, the output will have its width and height increased by 3 respectively."},{"metadata":{"id":"B50C2BBE06F04A958944567C6EAAB54C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"x=Variable(Tensor(np.zeros((2,3,1,1))))\ng_blk=G_block(3,20,strides=1,padding=0)\ng_blk.cuda()\nprint(g_blk(x).shape)","execution_count":null},{"metadata":{"id":"59F3EDA99FE64EB4875ED9EAF44125C3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"The generator consists of four basic blocks that increase input's both width and height from 1 to 32. At the same time, it first projects the latent variable into $64\\times 8$ channels, and then halve the channels each time. At last, a transposed convolution layer is used to generate the output. It further doubles the width and height to match the desired $64\\times 64$ shape, and reduces the channel size to $3$. The tanh activation function is applied to project output values into the $(-1, 1)$ range."},{"metadata":{"id":"154CA26F711F471E911814D4B7CF70EF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class net_G(nn.Module):\n    def __init__(self,in_channels):\n        super(net_G,self).__init__()\n\n        n_G=64\n        self.model=nn.Sequential(\n            G_block(in_channels,n_G*8,strides=1,padding=0),\n            G_block(n_G*8,n_G*4),\n            G_block(n_G*4,n_G*2),\n            G_block(n_G*2,n_G),\n            nn.ConvTranspose2d(\n                n_G,3,kernel_size=4,stride=2,padding=1,bias=False\n            ),\n            nn.Tanh()\n        )\n    def forward(self,x):\n        x=self.model(x)\n        return x\n\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, mean=0, std=0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, mean=1.0, std=0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)","execution_count":null},{"metadata":{"id":"AA03F88EA12E4E18BFD844B8F20BCE32","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# Generate a 100 dimensional latent variable to verify the generator's output shape.\nx=Variable(Tensor(np.zeros((1,100,1,1))))\ngenerator=net_G(100)\ngenerator.cuda()\ngenerator.apply(weights_init_normal)\nprint(generator(x).shape)","execution_count":null},{"metadata":{"id":"F2AD7D4DED734BD485F9F9E811478C63","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Discriminator\n\nThe discriminator is a normal convolutional network network except that it uses a leaky ReLU as its activation function. Given $\\alpha \\in[0, 1]$, its definition is\n\n\n$$\n\\textrm{leaky ReLU}(x) = \\begin{cases}x & \\text{if}\\ x > 0\\\\ \\alpha x &\\text{otherwise}\\end{cases}.\n$$\n\n\nAs it can be seen, it is normal ReLU if $\\alpha=0$, and an identity function if $\\alpha=1$. For $\\alpha \\in (0, 1)$, leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It aims to fix the \"dying ReLU\" problem that a neuron might always output a negative value and therefore cannot make any progress since the gradient of ReLU is 0."},{"metadata":{"id":"06E29AA7517840788EF67C6AE8B688B8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"alphas = [0, 0.2, 0.4, .6]\nx = np.arange(-2, 1, 0.1)\nY = [nn.LeakyReLU(alpha)(Tensor(x)).cpu().numpy()for alpha in alphas]\nplt.figure(figsize=(4,4))\nfor y in Y:\n    plt.plot(x,y)\nplt.show()","execution_count":null},{"metadata":{"id":"3DE35279528B461783468FAF2DA5033D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"The basic block of the discriminator is a convolution layer followed by a batch normalization layer and a leaky ReLU activation. The hyper-parameters of the convolution layer are similar to the transpose convolution layer in the generator block."},{"metadata":{"id":"48E3E2D51714436FA6FA33A9B879BF74","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class D_block(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=4,strides=2,\n                 padding=1,alpha=0.2):\n        super(D_block,self).__init__()\n        self.conv2d=nn.Conv2d(in_channels,out_channels,kernel_size,strides,padding,bias=False)\n        self.batch_norm=nn.BatchNorm2d(out_channels,0.8)\n        self.activation=nn.LeakyReLU(alpha)\n    def forward(self,X):\n        return self.activation(self.batch_norm(self.conv2d(X)))","execution_count":null},{"metadata":{"id":"4B1934549F204F2689D69EB9F15B6465","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"A basic block with default settings will halve the width and height of the inputs, as we demonstrated in :numref:`sec_padding`. For example, given a input shape $n_h = n_w = 16 $, with a kernel shape $k_h = k_w = 4$, a stride shape $s_h = s_w = 2$, and a padding shape $p_h = p_w = 1$, the output shape will be:\n\n\n$$\n\n\\begin{aligned}\nn_h^{'} \\times n_w^{'} &= \\lfloor(n_h-k_h+2p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+2p_w+s_w)/s_w\\rfloor\\\\\n  &= \\lfloor(16-4+2\\times 1+2)/2\\rfloor \\times \\lfloor(16-4+2\\times 1+2)/2\\rfloor\\\\\n  &= 8 \\times 8 .\\\\\n\\end{aligned}\n\n$$\n"},{"metadata":{"id":"F9B8B8AFBFDD4097B018D6436D81E686","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"x = Variable(Tensor(np.zeros((2, 3, 16, 16))))\nd_blk = D_block(3,20)\nd_blk.cuda()\nprint(d_blk(x).shape)","execution_count":null},{"metadata":{"id":"C447CA8AD4CF405D894288D216D82F85","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# The discriminator is a mirror of the generator.\nclass net_D(nn.Module):\n    def __init__(self,in_channels):\n        super(net_D,self).__init__()\n        n_D=64\n        self.model=nn.Sequential(\n            D_block(in_channels,n_D),\n            D_block(n_D,n_D*2),\n            D_block(n_D*2,n_D*4),\n            D_block(n_D*4,n_D*8)\n        )\n        self.conv=nn.Conv2d(n_D*8,1,kernel_size=4,bias=False)\n        self.activation=nn.Sigmoid()\n        # self._initialize_weights()\n    def forward(self,x):\n        x=self.model(x)\n        x=self.conv(x)\n        x=self.activation(x)\n        return x","execution_count":null},{"metadata":{"id":"FCF273B52B3F4BF6873581231932664E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"It uses a convolution layer with output channel $1$ as the last layer to obtain a single prediction value."},{"metadata":{"id":"0188517711D041668D1C5D3A9F2F0DA5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"x = Variable(Tensor(np.zeros((1, 3, 64, 64))))\ndiscriminator=net_D(3)\ndiscriminator.cuda()\ndiscriminator.apply(weights_init_normal)\nprint(discriminator(x).shape)","execution_count":null},{"metadata":{"id":"84806BF5D93A4C5084A6A41981FEFB97","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Training\nCompared to the basic GAN in :numref:`sec_basic_gan`, we use the same learning rate for both generator and discriminator since they are similar to each other. In addition, we change $\\beta_1$ in Adam (:numref:`sec_adam`) from $0.9$ to $0.5$. It decreases the smoothness of the momentum, the exponentially weighted moving average of past gradients, to take care of the rapid changing gradients because the generator and the discriminator fight with each other. Besides, the random generated noise `Z`, is a 4-D tensor and we are using GPU to accelerate the computation."},{"metadata":{"id":"02085588627643678E68731EBF993555","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"def update_D(X,Z,net_D,net_G,loss,trainer_D):\n    batch_size=X.shape[0]\n    Tensor=torch.cuda.FloatTensor\n    ones=Variable(Tensor(np.ones(batch_size,)),requires_grad=False).view(batch_size,1)\n    zeros = Variable(Tensor(np.zeros(batch_size,)),requires_grad=False).view(batch_size,1)\n    real_Y=net_D(X).view(batch_size,-1)\n    fake_X=net_G(Z)\n    fake_Y=net_D(fake_X).view(batch_size,-1)\n    loss_D=(loss(real_Y,ones)+loss(fake_Y,zeros))/2\n    loss_D.backward()\n    trainer_D.step()\n    return float(loss_D.sum())\n\ndef update_G(Z,net_D,net_G,loss,trainer_G):\n    batch_size=Z.shape[0]\n    Tensor=torch.cuda.FloatTensor\n    ones=Variable(Tensor(np.ones((batch_size,))),requires_grad=False).view(batch_size,1)\n    fake_X=net_G(Z)\n    fake_Y=net_D(fake_X).view(batch_size,-1)\n    loss_G=loss(fake_Y,ones)\n    loss_G.backward()\n    trainer_G.step()\n    return float(loss_G.sum())\n\n\ndef train(net_D,net_G,data_iter,num_epochs,lr,latent_dim):\n    loss=nn.BCELoss()\n    Tensor=torch.cuda.FloatTensor\n    trainer_D=torch.optim.Adam(net_D.parameters(),lr=lr,betas=(0.5,0.999))\n    trainer_G=torch.optim.Adam(net_G.parameters(),lr=lr,betas=(0.5,0.999))\n    plt.figure(figsize=(7,4))\n    d_loss_point=[]\n    g_loss_point=[]\n    d_loss=0\n    g_loss=0\n    for epoch in range(1,num_epochs+1):\n        d_loss_sum=0\n        g_loss_sum=0\n        batch=0\n        for X in data_iter:\n            X=X[:][0]\n            batch+=1\n            X=Variable(X.type(Tensor))\n            batch_size=X.shape[0]\n            Z=Variable(Tensor(np.random.normal(0,1,(batch_size,latent_dim,1,1))))\n\n            trainer_D.zero_grad()\n            d_loss = update_D(X, Z, net_D, net_G, loss, trainer_D)\n            d_loss_sum+=d_loss\n            trainer_G.zero_grad()\n            g_loss = update_G(Z, net_D, net_G, loss, trainer_G)\n            g_loss_sum+=g_loss\n\n        d_loss_point.append(d_loss_sum/batch)\n        g_loss_point.append(g_loss_sum/batch)\n        print(\n            \"[Epoch %d/%d]  [D loss: %f] [G loss: %f]\"\n            % (epoch, num_epochs,  d_loss_sum/batch_size,  g_loss_sum/batch_size)\n        )\n\n\n    plt.ylabel('Loss', fontdict={ 'size': 14})\n    plt.xlabel('epoch', fontdict={ 'size': 14})\n    plt.xticks(range(0,num_epochs+1,3))\n    plt.plot(range(1,num_epochs+1),d_loss_point,color='orange',label='discriminator')\n    plt.plot(range(1,num_epochs+1),g_loss_point,color='blue',label='generator')\n    plt.legend()\n    plt.show()\n    print(d_loss,g_loss)\n\n    Z = Variable(Tensor(np.random.normal(0, 1, size=(21, latent_dim, 1, 1))),requires_grad=False)\n    fake_x = generator(Z)\n    fake_x=fake_x.cpu().detach().numpy()\n    plt.figure(figsize=(14,6))\n    for i in range(21):\n        im=np.transpose(fake_x[i])\n        plt.subplot(3,7,i+1)\n        plt.imshow(im)\n    plt.show()","execution_count":null},{"metadata":{"id":"113CA1D99A064600875E0637B76E72CF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 训练模型\nif __name__ == '__main__':\n    lr,latent_dim,num_epochs=0.005,100,50\n    train(discriminator,generator,data_iter,num_epochs,lr,latent_dim)","execution_count":null},{"metadata":{"id":"1FA2585ED4C44584BB4DC02EFB96185A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Summary\n\n* DCGAN architecture has four convolutional layers for the Discriminator and four \"fractionally-strided\" convolutional layers for the Generator.\n* The Discriminator is a 4-layer strided convolutions with batch normalization (except its input layer) and leaky ReLU activations. \n* Leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It aims to fix the “dying ReLU” problem and helps the gradients flow easier through the architecture.\n\n\n## Exercises\n\n* What will happen if we use standard ReLU activation rather than leaky ReLU?\n* Apply DCGAN on Fashion-MNIST and see which category works well and which does not.\n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}